{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pathways that work:\n",
    "\n",
    "- Malate\n",
    "\n",
    "- Citrate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CatPred Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating kinetic parameters for all enzymes in network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      substrate                                             SMILES  \\\n",
      "0       Acetate                                         CC(=O)[O-]   \n",
      "1  Acetaldehyde                                               CC=O   \n",
      "2          NADH  C1C=CN(C=C1C(=O)N)[C@H]2[C@@H]([C@@H]([C@H](O2...   \n",
      "3          NAD+  C1=CC(=C[N+](=C1)[C@H]2[C@@H]([C@@H]([C@H](O2)...   \n",
      "\n",
      "                                            sequence pdbpath  \n",
      "0  MAENHDYEREINRLFELQKKNVVRLRTSSIDERIAKLKKLKEYIWE...   seq10  \n",
      "1  MAENHDYEREINRLFELQKKNVVRLRTSSIDERIAKLKKLKEYIWE...   seq10  \n",
      "2  MAENHDYEREINRLFELQKKNVVRLRTSSIDERIAKLKKLKEYIWE...   seq10  \n",
      "3  MAENHDYEREINRLFELQKKNVVRLRTSSIDERIAKLKKLKEYIWE...   seq10  \n",
      "/home/xuvin/py_repos/wccm/src/catpred_pipeline/catpred\n",
      "Predicting.. This will take a while..\n",
      "Loading training args\n",
      "Loading models\n",
      "Setting molecule featurization parameters to default.\n",
      "Loading data\n",
      "Validating SMILES\n",
      "Test size = 4\n",
      "Creating protein model\n",
      "MoleculeModel(\n",
      "  (softplus): Softplus(beta=1.0, threshold=20.0)\n",
      "  (encoder): MPN(\n",
      "    (encoder): ModuleList(\n",
      "      (0): MPNEncoder(\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (act_func): ReLU()\n",
      "        (W_i): Linear(in_features=147, out_features=300, bias=False)\n",
      "        (W_h): Linear(in_features=300, out_features=300, bias=False)\n",
      "        (W_o): Linear(in_features=433, out_features=300, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (seq_embedder): Embedding(21, 36, padding_idx=20)\n",
      "  (rotary_embedder): RotaryEmbedding()\n",
      "  (multihead_attn): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=36, out_features=36, bias=True)\n",
      "  )\n",
      "  (attentive_pooler): AttentivePooling(\n",
      "    (linear1): Linear(in_features=1316, out_features=1316, bias=True)\n",
      "    (tanh): Tanh()\n",
      "    (linear2): Linear(in_features=1316, out_features=1, bias=True)\n",
      "    (softmax): Softmax(dim=1)\n",
      "  )\n",
      "  (readout): Sequential(\n",
      "    (0): Dropout(p=0.0, inplace=False)\n",
      "    (1): Linear(in_features=1616, out_features=300, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.0, inplace=False)\n",
      "    (4): Linear(in_features=300, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
      "Loading pretrained parameter \"seq_embedder.weight\".\n",
      "Loading pretrained parameter \"rotary_embedder.freqs\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_weight\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_bias\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.weight\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.bias\".\n",
      "Loading pretrained parameter \"readout.1.weight\".\n",
      "Loading pretrained parameter \"readout.1.bias\".\n",
      "Loading pretrained parameter \"readout.4.weight\".\n",
      "Loading pretrained parameter \"readout.4.bias\".\n",
      "Moving model to cuda\n",
      "Creating protein model\n",
      "MoleculeModel(\n",
      "  (softplus): Softplus(beta=1.0, threshold=20.0)\n",
      "  (encoder): MPN(\n",
      "    (encoder): ModuleList(\n",
      "      (0): MPNEncoder(\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (act_func): ReLU()\n",
      "        (W_i): Linear(in_features=147, out_features=300, bias=False)\n",
      "        (W_h): Linear(in_features=300, out_features=300, bias=False)\n",
      "        (W_o): Linear(in_features=433, out_features=300, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (seq_embedder): Embedding(21, 36, padding_idx=20)\n",
      "  (rotary_embedder): RotaryEmbedding()\n",
      "  (multihead_attn): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=36, out_features=36, bias=True)\n",
      "  )\n",
      "  (attentive_pooler): AttentivePooling(\n",
      "    (linear1): Linear(in_features=1316, out_features=1316, bias=True)\n",
      "    (tanh): Tanh()\n",
      "    (linear2): Linear(in_features=1316, out_features=1, bias=True)\n",
      "    (softmax): Softmax(dim=1)\n",
      "  )\n",
      "  (readout): Sequential(\n",
      "    (0): Dropout(p=0.0, inplace=False)\n",
      "    (1): Linear(in_features=1616, out_features=300, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.0, inplace=False)\n",
      "    (4): Linear(in_features=300, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
      "Loading pretrained parameter \"seq_embedder.weight\".\n",
      "Loading pretrained parameter \"rotary_embedder.freqs\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_weight\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_bias\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.weight\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.bias\".\n",
      "Loading pretrained parameter \"readout.1.weight\".\n",
      "Loading pretrained parameter \"readout.1.bias\".\n",
      "Loading pretrained parameter \"readout.4.weight\".\n",
      "Loading pretrained parameter \"readout.4.bias\".\n",
      "Moving model to cuda\n",
      "Creating protein model\n",
      "MoleculeModel(\n",
      "  (softplus): Softplus(beta=1.0, threshold=20.0)\n",
      "  (encoder): MPN(\n",
      "    (encoder): ModuleList(\n",
      "      (0): MPNEncoder(\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (act_func): ReLU()\n",
      "        (W_i): Linear(in_features=147, out_features=300, bias=False)\n",
      "        (W_h): Linear(in_features=300, out_features=300, bias=False)\n",
      "        (W_o): Linear(in_features=433, out_features=300, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (seq_embedder): Embedding(21, 36, padding_idx=20)\n",
      "  (rotary_embedder): RotaryEmbedding()\n",
      "  (multihead_attn): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=36, out_features=36, bias=True)\n",
      "  )\n",
      "  (attentive_pooler): AttentivePooling(\n",
      "    (linear1): Linear(in_features=1316, out_features=1316, bias=True)\n",
      "    (tanh): Tanh()\n",
      "    (linear2): Linear(in_features=1316, out_features=1, bias=True)\n",
      "    (softmax): Softmax(dim=1)\n",
      "  )\n",
      "  (readout): Sequential(\n",
      "    (0): Dropout(p=0.0, inplace=False)\n",
      "    (1): Linear(in_features=1616, out_features=300, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.0, inplace=False)\n",
      "    (4): Linear(in_features=300, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
      "Loading pretrained parameter \"seq_embedder.weight\".\n",
      "Loading pretrained parameter \"rotary_embedder.freqs\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_weight\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_bias\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.weight\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.bias\".\n",
      "Loading pretrained parameter \"readout.1.weight\".\n",
      "Loading pretrained parameter \"readout.1.bias\".\n",
      "Loading pretrained parameter \"readout.4.weight\".\n",
      "Loading pretrained parameter \"readout.4.bias\".\n",
      "Moving model to cuda\n",
      "Creating protein model\n",
      "MoleculeModel(\n",
      "  (softplus): Softplus(beta=1.0, threshold=20.0)\n",
      "  (encoder): MPN(\n",
      "    (encoder): ModuleList(\n",
      "      (0): MPNEncoder(\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (act_func): ReLU()\n",
      "        (W_i): Linear(in_features=147, out_features=300, bias=False)\n",
      "        (W_h): Linear(in_features=300, out_features=300, bias=False)\n",
      "        (W_o): Linear(in_features=433, out_features=300, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (seq_embedder): Embedding(21, 36, padding_idx=20)\n",
      "  (rotary_embedder): RotaryEmbedding()\n",
      "  (multihead_attn): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=36, out_features=36, bias=True)\n",
      "  )\n",
      "  (attentive_pooler): AttentivePooling(\n",
      "    (linear1): Linear(in_features=1316, out_features=1316, bias=True)\n",
      "    (tanh): Tanh()\n",
      "    (linear2): Linear(in_features=1316, out_features=1, bias=True)\n",
      "    (softmax): Softmax(dim=1)\n",
      "  )\n",
      "  (readout): Sequential(\n",
      "    (0): Dropout(p=0.0, inplace=False)\n",
      "    (1): Linear(in_features=1616, out_features=300, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.0, inplace=False)\n",
      "    (4): Linear(in_features=300, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
      "Loading pretrained parameter \"seq_embedder.weight\".\n",
      "Loading pretrained parameter \"rotary_embedder.freqs\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_weight\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_bias\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.weight\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.bias\".\n",
      "Loading pretrained parameter \"readout.1.weight\".\n",
      "Loading pretrained parameter \"readout.1.bias\".\n",
      "Loading pretrained parameter \"readout.4.weight\".\n",
      "Loading pretrained parameter \"readout.4.bias\".\n",
      "Moving model to cuda\n",
      "Creating protein model\n",
      "MoleculeModel(\n",
      "  (softplus): Softplus(beta=1.0, threshold=20.0)\n",
      "  (encoder): MPN(\n",
      "    (encoder): ModuleList(\n",
      "      (0): MPNEncoder(\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (act_func): ReLU()\n",
      "        (W_i): Linear(in_features=147, out_features=300, bias=False)\n",
      "        (W_h): Linear(in_features=300, out_features=300, bias=False)\n",
      "        (W_o): Linear(in_features=433, out_features=300, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (seq_embedder): Embedding(21, 36, padding_idx=20)\n",
      "  (rotary_embedder): RotaryEmbedding()\n",
      "  (multihead_attn): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=36, out_features=36, bias=True)\n",
      "  )\n",
      "  (attentive_pooler): AttentivePooling(\n",
      "    (linear1): Linear(in_features=1316, out_features=1316, bias=True)\n",
      "    (tanh): Tanh()\n",
      "    (linear2): Linear(in_features=1316, out_features=1, bias=True)\n",
      "    (softmax): Softmax(dim=1)\n",
      "  )\n",
      "  (readout): Sequential(\n",
      "    (0): Dropout(p=0.0, inplace=False)\n",
      "    (1): Linear(in_features=1616, out_features=300, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.0, inplace=False)\n",
      "    (4): Linear(in_features=300, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
      "Loading pretrained parameter \"seq_embedder.weight\".\n",
      "Loading pretrained parameter \"rotary_embedder.freqs\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_weight\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_bias\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.weight\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.bias\".\n",
      "Loading pretrained parameter \"readout.1.weight\".\n",
      "Loading pretrained parameter \"readout.1.bias\".\n",
      "Loading pretrained parameter \"readout.4.weight\".\n",
      "Loading pretrained parameter \"readout.4.bias\".\n",
      "Moving model to cuda\n",
      "Creating protein model\n",
      "MoleculeModel(\n",
      "  (softplus): Softplus(beta=1.0, threshold=20.0)\n",
      "  (encoder): MPN(\n",
      "    (encoder): ModuleList(\n",
      "      (0): MPNEncoder(\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (act_func): ReLU()\n",
      "        (W_i): Linear(in_features=147, out_features=300, bias=False)\n",
      "        (W_h): Linear(in_features=300, out_features=300, bias=False)\n",
      "        (W_o): Linear(in_features=433, out_features=300, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (seq_embedder): Embedding(21, 36, padding_idx=20)\n",
      "  (rotary_embedder): RotaryEmbedding()\n",
      "  (multihead_attn): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=36, out_features=36, bias=True)\n",
      "  )\n",
      "  (attentive_pooler): AttentivePooling(\n",
      "    (linear1): Linear(in_features=1316, out_features=1316, bias=True)\n",
      "    (tanh): Tanh()\n",
      "    (linear2): Linear(in_features=1316, out_features=1, bias=True)\n",
      "    (softmax): Softmax(dim=1)\n",
      "  )\n",
      "  (readout): Sequential(\n",
      "    (0): Dropout(p=0.0, inplace=False)\n",
      "    (1): Linear(in_features=1616, out_features=300, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.0, inplace=False)\n",
      "    (4): Linear(in_features=300, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
      "Loading pretrained parameter \"seq_embedder.weight\".\n",
      "Loading pretrained parameter \"rotary_embedder.freqs\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_weight\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_bias\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.weight\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.bias\".\n",
      "Loading pretrained parameter \"readout.1.weight\".\n",
      "Loading pretrained parameter \"readout.1.bias\".\n",
      "Loading pretrained parameter \"readout.4.weight\".\n",
      "Loading pretrained parameter \"readout.4.bias\".\n",
      "Moving model to cuda\n",
      "Creating protein model\n",
      "MoleculeModel(\n",
      "  (softplus): Softplus(beta=1.0, threshold=20.0)\n",
      "  (encoder): MPN(\n",
      "    (encoder): ModuleList(\n",
      "      (0): MPNEncoder(\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (act_func): ReLU()\n",
      "        (W_i): Linear(in_features=147, out_features=300, bias=False)\n",
      "        (W_h): Linear(in_features=300, out_features=300, bias=False)\n",
      "        (W_o): Linear(in_features=433, out_features=300, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (seq_embedder): Embedding(21, 36, padding_idx=20)\n",
      "  (rotary_embedder): RotaryEmbedding()\n",
      "  (multihead_attn): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=36, out_features=36, bias=True)\n",
      "  )\n",
      "  (attentive_pooler): AttentivePooling(\n",
      "    (linear1): Linear(in_features=1316, out_features=1316, bias=True)\n",
      "    (tanh): Tanh()\n",
      "    (linear2): Linear(in_features=1316, out_features=1, bias=True)\n",
      "    (softmax): Softmax(dim=1)\n",
      "  )\n",
      "  (readout): Sequential(\n",
      "    (0): Dropout(p=0.0, inplace=False)\n",
      "    (1): Linear(in_features=1616, out_features=300, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.0, inplace=False)\n",
      "    (4): Linear(in_features=300, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
      "Loading pretrained parameter \"seq_embedder.weight\".\n",
      "Loading pretrained parameter \"rotary_embedder.freqs\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_weight\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_bias\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.weight\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.bias\".\n",
      "Loading pretrained parameter \"readout.1.weight\".\n",
      "Loading pretrained parameter \"readout.1.bias\".\n",
      "Loading pretrained parameter \"readout.4.weight\".\n",
      "Loading pretrained parameter \"readout.4.bias\".\n",
      "Moving model to cuda\n",
      "Creating protein model\n",
      "MoleculeModel(\n",
      "  (softplus): Softplus(beta=1.0, threshold=20.0)\n",
      "  (encoder): MPN(\n",
      "    (encoder): ModuleList(\n",
      "      (0): MPNEncoder(\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (act_func): ReLU()\n",
      "        (W_i): Linear(in_features=147, out_features=300, bias=False)\n",
      "        (W_h): Linear(in_features=300, out_features=300, bias=False)\n",
      "        (W_o): Linear(in_features=433, out_features=300, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (seq_embedder): Embedding(21, 36, padding_idx=20)\n",
      "  (rotary_embedder): RotaryEmbedding()\n",
      "  (multihead_attn): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=36, out_features=36, bias=True)\n",
      "  )\n",
      "  (attentive_pooler): AttentivePooling(\n",
      "    (linear1): Linear(in_features=1316, out_features=1316, bias=True)\n",
      "    (tanh): Tanh()\n",
      "    (linear2): Linear(in_features=1316, out_features=1, bias=True)\n",
      "    (softmax): Softmax(dim=1)\n",
      "  )\n",
      "  (readout): Sequential(\n",
      "    (0): Dropout(p=0.0, inplace=False)\n",
      "    (1): Linear(in_features=1616, out_features=300, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.0, inplace=False)\n",
      "    (4): Linear(in_features=300, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
      "Loading pretrained parameter \"seq_embedder.weight\".\n",
      "Loading pretrained parameter \"rotary_embedder.freqs\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_weight\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_bias\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.weight\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.bias\".\n",
      "Loading pretrained parameter \"readout.1.weight\".\n",
      "Loading pretrained parameter \"readout.1.bias\".\n",
      "Loading pretrained parameter \"readout.4.weight\".\n",
      "Loading pretrained parameter \"readout.4.bias\".\n",
      "Moving model to cuda\n",
      "Creating protein model\n",
      "MoleculeModel(\n",
      "  (softplus): Softplus(beta=1.0, threshold=20.0)\n",
      "  (encoder): MPN(\n",
      "    (encoder): ModuleList(\n",
      "      (0): MPNEncoder(\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (act_func): ReLU()\n",
      "        (W_i): Linear(in_features=147, out_features=300, bias=False)\n",
      "        (W_h): Linear(in_features=300, out_features=300, bias=False)\n",
      "        (W_o): Linear(in_features=433, out_features=300, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (seq_embedder): Embedding(21, 36, padding_idx=20)\n",
      "  (rotary_embedder): RotaryEmbedding()\n",
      "  (multihead_attn): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=36, out_features=36, bias=True)\n",
      "  )\n",
      "  (attentive_pooler): AttentivePooling(\n",
      "    (linear1): Linear(in_features=1316, out_features=1316, bias=True)\n",
      "    (tanh): Tanh()\n",
      "    (linear2): Linear(in_features=1316, out_features=1, bias=True)\n",
      "    (softmax): Softmax(dim=1)\n",
      "  )\n",
      "  (readout): Sequential(\n",
      "    (0): Dropout(p=0.0, inplace=False)\n",
      "    (1): Linear(in_features=1616, out_features=300, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.0, inplace=False)\n",
      "    (4): Linear(in_features=300, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
      "Loading pretrained parameter \"seq_embedder.weight\".\n",
      "Loading pretrained parameter \"rotary_embedder.freqs\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_weight\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_bias\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.weight\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.bias\".\n",
      "Loading pretrained parameter \"readout.1.weight\".\n",
      "Loading pretrained parameter \"readout.1.bias\".\n",
      "Loading pretrained parameter \"readout.4.weight\".\n",
      "Loading pretrained parameter \"readout.4.bias\".\n",
      "Moving model to cuda\n",
      "Creating protein model\n",
      "MoleculeModel(\n",
      "  (softplus): Softplus(beta=1.0, threshold=20.0)\n",
      "  (encoder): MPN(\n",
      "    (encoder): ModuleList(\n",
      "      (0): MPNEncoder(\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (act_func): ReLU()\n",
      "        (W_i): Linear(in_features=147, out_features=300, bias=False)\n",
      "        (W_h): Linear(in_features=300, out_features=300, bias=False)\n",
      "        (W_o): Linear(in_features=433, out_features=300, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (seq_embedder): Embedding(21, 36, padding_idx=20)\n",
      "  (rotary_embedder): RotaryEmbedding()\n",
      "  (multihead_attn): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=36, out_features=36, bias=True)\n",
      "  )\n",
      "  (attentive_pooler): AttentivePooling(\n",
      "    (linear1): Linear(in_features=1316, out_features=1316, bias=True)\n",
      "    (tanh): Tanh()\n",
      "    (linear2): Linear(in_features=1316, out_features=1, bias=True)\n",
      "    (softmax): Softmax(dim=1)\n",
      "  )\n",
      "  (readout): Sequential(\n",
      "    (0): Dropout(p=0.0, inplace=False)\n",
      "    (1): Linear(in_features=1616, out_features=300, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.0, inplace=False)\n",
      "    (4): Linear(in_features=300, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
      "Loading pretrained parameter \"seq_embedder.weight\".\n",
      "Loading pretrained parameter \"rotary_embedder.freqs\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_weight\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_bias\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.weight\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.bias\".\n",
      "Loading pretrained parameter \"readout.1.weight\".\n",
      "Loading pretrained parameter \"readout.1.bias\".\n",
      "Loading pretrained parameter \"readout.4.weight\".\n",
      "Loading pretrained parameter \"readout.4.bias\".\n",
      "Moving model to cuda\n",
      "Saving predictions to ../pipeline_entry_kcat_input_output.csv\n",
      "Elapsed time = 0:00:06\n",
      "Prediction completed.\n",
      "Output saved to results/ pipeline_entry_kcat_input_output.csv\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sh: 1: PROTEIN_EMBED_USE_CPU=1: not found\n",
      "/home/xuvin/.conda/envs/catpred/lib/python3.9/site-packages/rotary_embedding_torch/rotary_embedding_torch.py:35: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  @autocast(enabled = False)\n",
      "/home/xuvin/.conda/envs/catpred/lib/python3.9/site-packages/rotary_embedding_torch/rotary_embedding_torch.py:268: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  @autocast(enabled = False)\n",
      "\n",
      "0it [00:00, ?it/s]/home/xuvin/py_repos/wccm/src/catpred_pipeline/catpred/catpred/data/cache_utils.py:95: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(str(entry_path))\n",
      "\n",
      "4it [00:00, 71.55it/s]\n",
      "\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 24528.09it/s]\n",
      "\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 1565.18it/s]\n",
      "\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]/home/xuvin/py_repos/wccm/src/catpred_pipeline/catpred/catpred/utils.py:113: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(path, map_location=lambda storage, loc: storage)\n",
      "/home/xuvin/py_repos/wccm/src/catpred_pipeline/catpred/catpred/utils.py:446: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(path, map_location=lambda storage, loc: storage)\n",
      "\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.45s/it]\u001b[A\n",
      "\n",
      "                                             \u001b[A\n",
      " 10%|█         | 1/10 [00:02<00:26,  3.00s/it]/home/xuvin/py_repos/wccm/src/catpred_pipeline/catpred/catpred/utils.py:113: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(path, map_location=lambda storage, loc: storage)\n",
      "/home/xuvin/py_repos/wccm/src/catpred_pipeline/catpred/catpred/utils.py:446: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(path, map_location=lambda storage, loc: storage)\n",
      "\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "                                     \u001b[A\n",
      " 20%|██        | 2/10 [00:03<00:11,  1.38s/it]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "                                     \u001b[A\n",
      " 30%|███       | 3/10 [00:03<00:06,  1.06it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "                                     \u001b[A\n",
      " 40%|████      | 4/10 [00:03<00:04,  1.49it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "                                     \u001b[A\n",
      " 50%|█████     | 5/10 [00:04<00:02,  1.94it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "                                     \u001b[A\n",
      " 60%|██████    | 6/10 [00:04<00:01,  2.39it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "                                     \u001b[A\n",
      " 70%|███████   | 7/10 [00:04<00:01,  2.28it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "                                     \u001b[A\n",
      " 80%|████████  | 8/10 [00:05<00:00,  2.64it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "                                     \u001b[A\n",
      " 90%|█████████ | 9/10 [00:05<00:00,  3.02it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "                                     \u001b[A\n",
      "100%|██████████| 10/10 [00:05<00:00,  3.24it/s]\n",
      "100%|██████████| 10/10 [00:05<00:00,  1.78it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/xuvin/py_repos/wccm/src/catpred_pipeline/catpred\n",
      "Predicting.. This will take a while..\n",
      "Loading training args\n",
      "Loading models\n",
      "Setting molecule featurization parameters to default.\n",
      "Loading data\n",
      "Validating SMILES\n",
      "Test size = 4\n",
      "Creating protein model\n",
      "MoleculeModel(\n",
      "  (softplus): Softplus(beta=1.0, threshold=20.0)\n",
      "  (encoder): MPN(\n",
      "    (encoder): ModuleList(\n",
      "      (0): MPNEncoder(\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (act_func): ReLU()\n",
      "        (W_i): Linear(in_features=147, out_features=300, bias=False)\n",
      "        (W_h): Linear(in_features=300, out_features=300, bias=False)\n",
      "        (W_o): Linear(in_features=433, out_features=300, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (seq_embedder): Embedding(21, 36, padding_idx=20)\n",
      "  (rotary_embedder): RotaryEmbedding()\n",
      "  (multihead_attn): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=36, out_features=36, bias=True)\n",
      "  )\n",
      "  (attentive_pooler): AttentivePooling(\n",
      "    (linear1): Linear(in_features=1316, out_features=1316, bias=True)\n",
      "    (tanh): Tanh()\n",
      "    (linear2): Linear(in_features=1316, out_features=1, bias=True)\n",
      "    (softmax): Softmax(dim=1)\n",
      "  )\n",
      "  (readout): Sequential(\n",
      "    (0): Dropout(p=0.0, inplace=False)\n",
      "    (1): Linear(in_features=1616, out_features=300, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.0, inplace=False)\n",
      "    (4): Linear(in_features=300, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
      "Loading pretrained parameter \"seq_embedder.weight\".\n",
      "Loading pretrained parameter \"rotary_embedder.freqs\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_weight\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_bias\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.weight\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.bias\".\n",
      "Loading pretrained parameter \"readout.1.weight\".\n",
      "Loading pretrained parameter \"readout.1.bias\".\n",
      "Loading pretrained parameter \"readout.4.weight\".\n",
      "Loading pretrained parameter \"readout.4.bias\".\n",
      "Moving model to cuda\n",
      "Creating protein model\n",
      "MoleculeModel(\n",
      "  (softplus): Softplus(beta=1.0, threshold=20.0)\n",
      "  (encoder): MPN(\n",
      "    (encoder): ModuleList(\n",
      "      (0): MPNEncoder(\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (act_func): ReLU()\n",
      "        (W_i): Linear(in_features=147, out_features=300, bias=False)\n",
      "        (W_h): Linear(in_features=300, out_features=300, bias=False)\n",
      "        (W_o): Linear(in_features=433, out_features=300, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (seq_embedder): Embedding(21, 36, padding_idx=20)\n",
      "  (rotary_embedder): RotaryEmbedding()\n",
      "  (multihead_attn): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=36, out_features=36, bias=True)\n",
      "  )\n",
      "  (attentive_pooler): AttentivePooling(\n",
      "    (linear1): Linear(in_features=1316, out_features=1316, bias=True)\n",
      "    (tanh): Tanh()\n",
      "    (linear2): Linear(in_features=1316, out_features=1, bias=True)\n",
      "    (softmax): Softmax(dim=1)\n",
      "  )\n",
      "  (readout): Sequential(\n",
      "    (0): Dropout(p=0.0, inplace=False)\n",
      "    (1): Linear(in_features=1616, out_features=300, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.0, inplace=False)\n",
      "    (4): Linear(in_features=300, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
      "Loading pretrained parameter \"seq_embedder.weight\".\n",
      "Loading pretrained parameter \"rotary_embedder.freqs\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_weight\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_bias\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.weight\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.bias\".\n",
      "Loading pretrained parameter \"readout.1.weight\".\n",
      "Loading pretrained parameter \"readout.1.bias\".\n",
      "Loading pretrained parameter \"readout.4.weight\".\n",
      "Loading pretrained parameter \"readout.4.bias\".\n",
      "Moving model to cuda\n",
      "Creating protein model\n",
      "MoleculeModel(\n",
      "  (softplus): Softplus(beta=1.0, threshold=20.0)\n",
      "  (encoder): MPN(\n",
      "    (encoder): ModuleList(\n",
      "      (0): MPNEncoder(\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (act_func): ReLU()\n",
      "        (W_i): Linear(in_features=147, out_features=300, bias=False)\n",
      "        (W_h): Linear(in_features=300, out_features=300, bias=False)\n",
      "        (W_o): Linear(in_features=433, out_features=300, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (seq_embedder): Embedding(21, 36, padding_idx=20)\n",
      "  (rotary_embedder): RotaryEmbedding()\n",
      "  (multihead_attn): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=36, out_features=36, bias=True)\n",
      "  )\n",
      "  (attentive_pooler): AttentivePooling(\n",
      "    (linear1): Linear(in_features=1316, out_features=1316, bias=True)\n",
      "    (tanh): Tanh()\n",
      "    (linear2): Linear(in_features=1316, out_features=1, bias=True)\n",
      "    (softmax): Softmax(dim=1)\n",
      "  )\n",
      "  (readout): Sequential(\n",
      "    (0): Dropout(p=0.0, inplace=False)\n",
      "    (1): Linear(in_features=1616, out_features=300, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.0, inplace=False)\n",
      "    (4): Linear(in_features=300, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
      "Loading pretrained parameter \"seq_embedder.weight\".\n",
      "Loading pretrained parameter \"rotary_embedder.freqs\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_weight\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_bias\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.weight\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.bias\".\n",
      "Loading pretrained parameter \"readout.1.weight\".\n",
      "Loading pretrained parameter \"readout.1.bias\".\n",
      "Loading pretrained parameter \"readout.4.weight\".\n",
      "Loading pretrained parameter \"readout.4.bias\".\n",
      "Moving model to cuda\n",
      "Creating protein model\n",
      "MoleculeModel(\n",
      "  (softplus): Softplus(beta=1.0, threshold=20.0)\n",
      "  (encoder): MPN(\n",
      "    (encoder): ModuleList(\n",
      "      (0): MPNEncoder(\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (act_func): ReLU()\n",
      "        (W_i): Linear(in_features=147, out_features=300, bias=False)\n",
      "        (W_h): Linear(in_features=300, out_features=300, bias=False)\n",
      "        (W_o): Linear(in_features=433, out_features=300, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (seq_embedder): Embedding(21, 36, padding_idx=20)\n",
      "  (rotary_embedder): RotaryEmbedding()\n",
      "  (multihead_attn): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=36, out_features=36, bias=True)\n",
      "  )\n",
      "  (attentive_pooler): AttentivePooling(\n",
      "    (linear1): Linear(in_features=1316, out_features=1316, bias=True)\n",
      "    (tanh): Tanh()\n",
      "    (linear2): Linear(in_features=1316, out_features=1, bias=True)\n",
      "    (softmax): Softmax(dim=1)\n",
      "  )\n",
      "  (readout): Sequential(\n",
      "    (0): Dropout(p=0.0, inplace=False)\n",
      "    (1): Linear(in_features=1616, out_features=300, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.0, inplace=False)\n",
      "    (4): Linear(in_features=300, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
      "Loading pretrained parameter \"seq_embedder.weight\".\n",
      "Loading pretrained parameter \"rotary_embedder.freqs\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_weight\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_bias\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.weight\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.bias\".\n",
      "Loading pretrained parameter \"readout.1.weight\".\n",
      "Loading pretrained parameter \"readout.1.bias\".\n",
      "Loading pretrained parameter \"readout.4.weight\".\n",
      "Loading pretrained parameter \"readout.4.bias\".\n",
      "Moving model to cuda\n",
      "Creating protein model\n",
      "MoleculeModel(\n",
      "  (softplus): Softplus(beta=1.0, threshold=20.0)\n",
      "  (encoder): MPN(\n",
      "    (encoder): ModuleList(\n",
      "      (0): MPNEncoder(\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (act_func): ReLU()\n",
      "        (W_i): Linear(in_features=147, out_features=300, bias=False)\n",
      "        (W_h): Linear(in_features=300, out_features=300, bias=False)\n",
      "        (W_o): Linear(in_features=433, out_features=300, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (seq_embedder): Embedding(21, 36, padding_idx=20)\n",
      "  (rotary_embedder): RotaryEmbedding()\n",
      "  (multihead_attn): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=36, out_features=36, bias=True)\n",
      "  )\n",
      "  (attentive_pooler): AttentivePooling(\n",
      "    (linear1): Linear(in_features=1316, out_features=1316, bias=True)\n",
      "    (tanh): Tanh()\n",
      "    (linear2): Linear(in_features=1316, out_features=1, bias=True)\n",
      "    (softmax): Softmax(dim=1)\n",
      "  )\n",
      "  (readout): Sequential(\n",
      "    (0): Dropout(p=0.0, inplace=False)\n",
      "    (1): Linear(in_features=1616, out_features=300, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.0, inplace=False)\n",
      "    (4): Linear(in_features=300, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
      "Loading pretrained parameter \"seq_embedder.weight\".\n",
      "Loading pretrained parameter \"rotary_embedder.freqs\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_weight\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_bias\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.weight\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.bias\".\n",
      "Loading pretrained parameter \"readout.1.weight\".\n",
      "Loading pretrained parameter \"readout.1.bias\".\n",
      "Loading pretrained parameter \"readout.4.weight\".\n",
      "Loading pretrained parameter \"readout.4.bias\".\n",
      "Moving model to cuda\n",
      "Creating protein model\n",
      "MoleculeModel(\n",
      "  (softplus): Softplus(beta=1.0, threshold=20.0)\n",
      "  (encoder): MPN(\n",
      "    (encoder): ModuleList(\n",
      "      (0): MPNEncoder(\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (act_func): ReLU()\n",
      "        (W_i): Linear(in_features=147, out_features=300, bias=False)\n",
      "        (W_h): Linear(in_features=300, out_features=300, bias=False)\n",
      "        (W_o): Linear(in_features=433, out_features=300, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (seq_embedder): Embedding(21, 36, padding_idx=20)\n",
      "  (rotary_embedder): RotaryEmbedding()\n",
      "  (multihead_attn): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=36, out_features=36, bias=True)\n",
      "  )\n",
      "  (attentive_pooler): AttentivePooling(\n",
      "    (linear1): Linear(in_features=1316, out_features=1316, bias=True)\n",
      "    (tanh): Tanh()\n",
      "    (linear2): Linear(in_features=1316, out_features=1, bias=True)\n",
      "    (softmax): Softmax(dim=1)\n",
      "  )\n",
      "  (readout): Sequential(\n",
      "    (0): Dropout(p=0.0, inplace=False)\n",
      "    (1): Linear(in_features=1616, out_features=300, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.0, inplace=False)\n",
      "    (4): Linear(in_features=300, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
      "Loading pretrained parameter \"seq_embedder.weight\".\n",
      "Loading pretrained parameter \"rotary_embedder.freqs\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_weight\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_bias\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.weight\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.bias\".\n",
      "Loading pretrained parameter \"readout.1.weight\".\n",
      "Loading pretrained parameter \"readout.1.bias\".\n",
      "Loading pretrained parameter \"readout.4.weight\".\n",
      "Loading pretrained parameter \"readout.4.bias\".\n",
      "Moving model to cuda\n",
      "Creating protein model\n",
      "MoleculeModel(\n",
      "  (softplus): Softplus(beta=1.0, threshold=20.0)\n",
      "  (encoder): MPN(\n",
      "    (encoder): ModuleList(\n",
      "      (0): MPNEncoder(\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (act_func): ReLU()\n",
      "        (W_i): Linear(in_features=147, out_features=300, bias=False)\n",
      "        (W_h): Linear(in_features=300, out_features=300, bias=False)\n",
      "        (W_o): Linear(in_features=433, out_features=300, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (seq_embedder): Embedding(21, 36, padding_idx=20)\n",
      "  (rotary_embedder): RotaryEmbedding()\n",
      "  (multihead_attn): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=36, out_features=36, bias=True)\n",
      "  )\n",
      "  (attentive_pooler): AttentivePooling(\n",
      "    (linear1): Linear(in_features=1316, out_features=1316, bias=True)\n",
      "    (tanh): Tanh()\n",
      "    (linear2): Linear(in_features=1316, out_features=1, bias=True)\n",
      "    (softmax): Softmax(dim=1)\n",
      "  )\n",
      "  (readout): Sequential(\n",
      "    (0): Dropout(p=0.0, inplace=False)\n",
      "    (1): Linear(in_features=1616, out_features=300, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.0, inplace=False)\n",
      "    (4): Linear(in_features=300, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
      "Loading pretrained parameter \"seq_embedder.weight\".\n",
      "Loading pretrained parameter \"rotary_embedder.freqs\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_weight\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_bias\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.weight\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.bias\".\n",
      "Loading pretrained parameter \"readout.1.weight\".\n",
      "Loading pretrained parameter \"readout.1.bias\".\n",
      "Loading pretrained parameter \"readout.4.weight\".\n",
      "Loading pretrained parameter \"readout.4.bias\".\n",
      "Moving model to cuda\n",
      "Creating protein model\n",
      "MoleculeModel(\n",
      "  (softplus): Softplus(beta=1.0, threshold=20.0)\n",
      "  (encoder): MPN(\n",
      "    (encoder): ModuleList(\n",
      "      (0): MPNEncoder(\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (act_func): ReLU()\n",
      "        (W_i): Linear(in_features=147, out_features=300, bias=False)\n",
      "        (W_h): Linear(in_features=300, out_features=300, bias=False)\n",
      "        (W_o): Linear(in_features=433, out_features=300, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (seq_embedder): Embedding(21, 36, padding_idx=20)\n",
      "  (rotary_embedder): RotaryEmbedding()\n",
      "  (multihead_attn): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=36, out_features=36, bias=True)\n",
      "  )\n",
      "  (attentive_pooler): AttentivePooling(\n",
      "    (linear1): Linear(in_features=1316, out_features=1316, bias=True)\n",
      "    (tanh): Tanh()\n",
      "    (linear2): Linear(in_features=1316, out_features=1, bias=True)\n",
      "    (softmax): Softmax(dim=1)\n",
      "  )\n",
      "  (readout): Sequential(\n",
      "    (0): Dropout(p=0.0, inplace=False)\n",
      "    (1): Linear(in_features=1616, out_features=300, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.0, inplace=False)\n",
      "    (4): Linear(in_features=300, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
      "Loading pretrained parameter \"seq_embedder.weight\".\n",
      "Loading pretrained parameter \"rotary_embedder.freqs\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_weight\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_bias\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.weight\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.bias\".\n",
      "Loading pretrained parameter \"readout.1.weight\".\n",
      "Loading pretrained parameter \"readout.1.bias\".\n",
      "Loading pretrained parameter \"readout.4.weight\".\n",
      "Loading pretrained parameter \"readout.4.bias\".\n",
      "Moving model to cuda\n",
      "Creating protein model\n",
      "MoleculeModel(\n",
      "  (softplus): Softplus(beta=1.0, threshold=20.0)\n",
      "  (encoder): MPN(\n",
      "    (encoder): ModuleList(\n",
      "      (0): MPNEncoder(\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (act_func): ReLU()\n",
      "        (W_i): Linear(in_features=147, out_features=300, bias=False)\n",
      "        (W_h): Linear(in_features=300, out_features=300, bias=False)\n",
      "        (W_o): Linear(in_features=433, out_features=300, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (seq_embedder): Embedding(21, 36, padding_idx=20)\n",
      "  (rotary_embedder): RotaryEmbedding()\n",
      "  (multihead_attn): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=36, out_features=36, bias=True)\n",
      "  )\n",
      "  (attentive_pooler): AttentivePooling(\n",
      "    (linear1): Linear(in_features=1316, out_features=1316, bias=True)\n",
      "    (tanh): Tanh()\n",
      "    (linear2): Linear(in_features=1316, out_features=1, bias=True)\n",
      "    (softmax): Softmax(dim=1)\n",
      "  )\n",
      "  (readout): Sequential(\n",
      "    (0): Dropout(p=0.0, inplace=False)\n",
      "    (1): Linear(in_features=1616, out_features=300, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.0, inplace=False)\n",
      "    (4): Linear(in_features=300, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
      "Loading pretrained parameter \"seq_embedder.weight\".\n",
      "Loading pretrained parameter \"rotary_embedder.freqs\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_weight\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_bias\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.weight\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.bias\".\n",
      "Loading pretrained parameter \"readout.1.weight\".\n",
      "Loading pretrained parameter \"readout.1.bias\".\n",
      "Loading pretrained parameter \"readout.4.weight\".\n",
      "Loading pretrained parameter \"readout.4.bias\".\n",
      "Moving model to cuda\n",
      "Creating protein model\n",
      "MoleculeModel(\n",
      "  (softplus): Softplus(beta=1.0, threshold=20.0)\n",
      "  (encoder): MPN(\n",
      "    (encoder): ModuleList(\n",
      "      (0): MPNEncoder(\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (act_func): ReLU()\n",
      "        (W_i): Linear(in_features=147, out_features=300, bias=False)\n",
      "        (W_h): Linear(in_features=300, out_features=300, bias=False)\n",
      "        (W_o): Linear(in_features=433, out_features=300, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (seq_embedder): Embedding(21, 36, padding_idx=20)\n",
      "  (rotary_embedder): RotaryEmbedding()\n",
      "  (multihead_attn): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=36, out_features=36, bias=True)\n",
      "  )\n",
      "  (attentive_pooler): AttentivePooling(\n",
      "    (linear1): Linear(in_features=1316, out_features=1316, bias=True)\n",
      "    (tanh): Tanh()\n",
      "    (linear2): Linear(in_features=1316, out_features=1, bias=True)\n",
      "    (softmax): Softmax(dim=1)\n",
      "  )\n",
      "  (readout): Sequential(\n",
      "    (0): Dropout(p=0.0, inplace=False)\n",
      "    (1): Linear(in_features=1616, out_features=300, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.0, inplace=False)\n",
      "    (4): Linear(in_features=300, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
      "Loading pretrained parameter \"seq_embedder.weight\".\n",
      "Loading pretrained parameter \"rotary_embedder.freqs\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_weight\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_bias\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.weight\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.bias\".\n",
      "Loading pretrained parameter \"readout.1.weight\".\n",
      "Loading pretrained parameter \"readout.1.bias\".\n",
      "Loading pretrained parameter \"readout.4.weight\".\n",
      "Loading pretrained parameter \"readout.4.bias\".\n",
      "Moving model to cuda\n",
      "Saving predictions to ../pipeline_entry_km_input_output.csv\n",
      "Elapsed time = 0:00:06\n",
      "Prediction completed.\n",
      "Output saved to results/ pipeline_entry_km_input_output.csv\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sh: 1: PROTEIN_EMBED_USE_CPU=1: not found\n",
      "/home/xuvin/.conda/envs/catpred/lib/python3.9/site-packages/rotary_embedding_torch/rotary_embedding_torch.py:35: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  @autocast(enabled = False)\n",
      "/home/xuvin/.conda/envs/catpred/lib/python3.9/site-packages/rotary_embedding_torch/rotary_embedding_torch.py:268: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  @autocast(enabled = False)\n",
      "\n",
      "0it [00:00, ?it/s]/home/xuvin/py_repos/wccm/src/catpred_pipeline/catpred/catpred/data/cache_utils.py:95: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(str(entry_path))\n",
      "\n",
      "4it [00:00, 59.42it/s]\n",
      "\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 24492.29it/s]\n",
      "\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 1545.29it/s]\n",
      "\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]/home/xuvin/py_repos/wccm/src/catpred_pipeline/catpred/catpred/utils.py:113: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(path, map_location=lambda storage, loc: storage)\n",
      "/home/xuvin/py_repos/wccm/src/catpred_pipeline/catpred/catpred/utils.py:446: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(path, map_location=lambda storage, loc: storage)\n",
      "\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.29s/it]\u001b[A\n",
      "\n",
      "                                             \u001b[A\n",
      " 10%|█         | 1/10 [00:02<00:24,  2.73s/it]/home/xuvin/py_repos/wccm/src/catpred_pipeline/catpred/catpred/utils.py:113: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(path, map_location=lambda storage, loc: storage)\n",
      "/home/xuvin/py_repos/wccm/src/catpred_pipeline/catpred/catpred/utils.py:446: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(path, map_location=lambda storage, loc: storage)\n",
      "\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "                                     \u001b[A\n",
      " 20%|██        | 2/10 [00:03<00:10,  1.30s/it]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "                                     \u001b[A\n",
      " 30%|███       | 3/10 [00:03<00:06,  1.09it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "                                     \u001b[A\n",
      " 40%|████      | 4/10 [00:03<00:04,  1.49it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "                                     \u001b[A\n",
      " 50%|█████     | 5/10 [00:04<00:02,  1.92it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "                                     \u001b[A\n",
      " 60%|██████    | 6/10 [00:04<00:01,  2.32it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "                                     \u001b[A\n",
      " 70%|███████   | 7/10 [00:04<00:01,  2.24it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "                                     \u001b[A\n",
      " 80%|████████  | 8/10 [00:05<00:00,  2.47it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "                                     \u001b[A\n",
      " 90%|█████████ | 9/10 [00:05<00:00,  2.80it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "                                     \u001b[A\n",
      "100%|██████████| 10/10 [00:05<00:00,  3.06it/s]\n",
      "100%|██████████| 10/10 [00:05<00:00,  1.78it/s]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# \n",
    "substrates = ['Acetate','Acetaldehyde','NADH','NAD+']\n",
    "SMILES = [\n",
    "    'CC(=O)[O-]',\n",
    "    'CC=O',\n",
    "    'C1C=CN(C=C1C(=O)N)[C@H]2[C@@H]([C@@H]([C@H](O2)COP(=O)(O)OP(=O)(O)OC[C@@H]3[C@H]([C@H]([C@@H](O3)N4C=NC5=C(N=CN=C54)N)O)O)O)O',\n",
    "    'C1=CC(=C[N+](=C1)[C@H]2[C@@H]([C@@H]([C@H](O2)COP(=O)([O-])OP(=O)(O)OC[C@@H]3[C@H]([C@H]([C@@H](O3)N4C=NC5=C(N=CN=C54)N)O)O)O)O)C(=O)N'\n",
    "    ]\n",
    "sequences = [\n",
    "    'maenhdyereinrlfelqkknvvrlrtssideriaklkklkeyiwenkekiqeavyndlrkppeevllteiypvvseirhviknlkkwtkpkkvrtpislfgaksyyrfeakgvvliispwnypfelsigplitaiaagnavvlkpselsphtsgyikklvadifdesevavvegdavvaqkllemgfnhifftgstkvakavlkkasetlssvtlelggkspviidgkfdieeaakkitwgkylnagqtciapdyvfvkkellgdfvshlkhyikkyyysdgsgrcsnycgiinerhfnrlknvfevtvkegakvcegglfvenecyisptvltdvgrdsyimeeeifgpilpvltyekiddvieyinskpaplvlyvfsrdrkfyrhvinnvisgdclindviahfanprlpfgghnasgigkshgyygfrefshlrsimiqpkrtmlqllyppygefvkkliewstkyf',\n",
    "    'maenhdyereinrlfelqkknvvrlrtssideriaklkklkeyiwenkekiqeavyndlrkppeevllteiypvvseirhviknlkkwtkpkkvrtpislfgaksyyrfeakgvvliispwnypfelsigplitaiaagnavvlkpselsphtsgyikklvadifdesevavvegdavvaqkllemgfnhifftgstkvakavlkkasetlssvtlelggkspviidgkfdieeaakkitwgkylnagqtciapdyvfvkkellgdfvshlkhyikkyyysdgsgrcsnycgiinerhfnrlknvfevtvkegakvcegglfvenecyisptvltdvgrdsyimeeeifgpilpvltyekiddvieyinskpaplvlyvfsrdrkfyrhvinnvisgdclindviahfanprlpfgghnasgigkshgyygfrefshlrsimiqpkrtmlqllyppygefvkkliewstkyf',\n",
    "    'maenhdyereinrlfelqkknvvrlrtssideriaklkklkeyiwenkekiqeavyndlrkppeevllteiypvvseirhviknlkkwtkpkkvrtpislfgaksyyrfeakgvvliispwnypfelsigplitaiaagnavvlkpselsphtsgyikklvadifdesevavvegdavvaqkllemgfnhifftgstkvakavlkkasetlssvtlelggkspviidgkfdieeaakkitwgkylnagqtciapdyvfvkkellgdfvshlkhyikkyyysdgsgrcsnycgiinerhfnrlknvfevtvkegakvcegglfvenecyisptvltdvgrdsyimeeeifgpilpvltyekiddvieyinskpaplvlyvfsrdrkfyrhvinnvisgdclindviahfanprlpfgghnasgigkshgyygfrefshlrsimiqpkrtmlqllyppygefvkkliewstkyf',\n",
    "    'maenhdyereinrlfelqkknvvrlrtssideriaklkklkeyiwenkekiqeavyndlrkppeevllteiypvvseirhviknlkkwtkpkkvrtpislfgaksyyrfeakgvvliispwnypfelsigplitaiaagnavvlkpselsphtsgyikklvadifdesevavvegdavvaqkllemgfnhifftgstkvakavlkkasetlssvtlelggkspviidgkfdieeaakkitwgkylnagqtciapdyvfvkkellgdfvshlkhyikkyyysdgsgrcsnycgiinerhfnrlknvfevtvkegakvcegglfvenecyisptvltdvgrdsyimeeeifgpilpvltyekiddvieyinskpaplvlyvfsrdrkfyrhvinnvisgdclindviahfanprlpfgghnasgigkshgyygfrefshlrsimiqpkrtmlqllyppygefvkkliewstkyf'\n",
    "    ]\n",
    "sequences = list(map(lambda x: x.upper(),sequences))\n",
    "pdb_paths = ['seq10','seq10','seq10','seq10']\n",
    "\n",
    "test = pd.DataFrame({'substrate':substrates,'SMILES':SMILES,'sequence':sequences,'pdbpath':pdb_paths})\n",
    "\n",
    "print(test)\n",
    "\n",
    "test.to_csv('./src/catpred_pipeline/pipeline_entry_kcat.csv')\n",
    "test.to_csv('./src/catpred_pipeline/pipeline_entry_km.csv')\n",
    "\n",
    "cmd = 'cd src/catpred_pipeline/catpred; ' \\\n",
    "'conda run -n catpred python3 ./demo_run.py --parameter kcat --input_file ../pipeline_entry_kcat.csv --checkpoint_dir ../data/pretrained/production/kcat/; ' \\\n",
    "'conda run -n catpred python3 ./demo_run.py --parameter km --input_file ../pipeline_entry_km.csv --checkpoint_dir ../data/pretrained/production/km/'\n",
    "subprocess.call(cmd,shell=True,executable='/bin/bash')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performing kinetic modeling of enzyme system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2757452496.py, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[25], line 6\u001b[0;36m\u001b[0m\n\u001b[0;31m    for\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import scipy\n",
    "import numpy as np\n",
    "\n",
    "# model currently doesn't consider change of enzyme concentrations\n",
    "def ode_func(kin_param_bundles,metas,enz):\n",
    "    dydt = np.zeros((len(metas)+len(enz),))\n",
    "    for idx,item in enumerate(metas):\n",
    "        dydt[idx] = item['']\n",
    "    for idx,item in enumerate(enz):\n",
    "        dydt[len(metas)+idx] = item['']\n",
    "    return dydt\n",
    "\n",
    "func = ode_func(bundle,metas,enz)\n",
    "\n",
    "print(func)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, np.float64(0.0))\n",
      "(1, np.float64(5.0))\n",
      "(2, np.float64(0.0))\n",
      "[0. 5. 0.]\n",
      "<enumerate object at 0x7f2c87310310>\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "test = np.zeros((3,))\n",
    "test[1] = 5\n",
    "\n",
    "for idx in enumerate(test):\n",
    "    print(idx)\n",
    "\n",
    "print(test)\n",
    "\n",
    "test2 = enumerate(test)\n",
    "\n",
    "print(test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.classes.WholeCellConsortiumModel import WholeCellConsortiumModel\n",
    "\n",
    "wccm = WholeCellConsortiumModel()\n",
    "wccm.generate_whole_network('test',invalid_enz=[],invalid_rxn=[])\n",
    "wccm.set_reaction_mass_balance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.classes.WholeCellConsortiumModel import WholeCellConsortiumModel\n",
    "\n",
    "wccm = WholeCellConsortiumModel({})\n",
    "wccm.generate_whole_network('test',invalid_enz=[],invalid_rxn=[])\n",
    "wccm.match_KEGG_compounds_to_BRENDA()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.classes.WholeCellConsortiumModel import WholeCellConsortiumModel\n",
    "import os\n",
    "\n",
    "os.system('which python')\n",
    "\n",
    "wccm = WholeCellConsortiumModel({})\n",
    "wccm.generate_whole_network('test',invalid_enz=[],invalid_rxn=[])\n",
    "wccm.set_reaction_reversibility()\n",
    "\n",
    "### STARTHERE: how to deal with generic reactions and generic compounds?\n",
    "# how to deal with protonated/non-protonated versions?\n",
    "# KEGG only lists reactions with neutral molecules; BRENDA may list them with non-neutrals, how to handle this? - based on molecular formula:\n",
    "#   if there is a difference of just one hydrogen between each of the potential acid/base states, then consolidate them into a single entity.\n",
    "# need to go through each reaction, find reaction reversibility for each of the enzymes that are dashed\n",
    "# need to incorporate thermodynamic information to also account for reversibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "data = ['C00020','C00044','C00144','C00125','C00126','C00138','C00139','C01326']\n",
    "checkboxes = [widgets.Checkbox(value=False, description=label) for label in data]\n",
    "output = widgets.VBox(children=checkboxes)\n",
    "display(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.classes.WholeCellConsortiumModel import WholeCellConsortiumModel\n",
    "\n",
    "valid_cofactors = {}\n",
    "for idx,x in enumerate(data):\n",
    "    valid_cofactors[x] = checkboxes[idx].value\n",
    "\n",
    "print(valid_cofactors)\n",
    "\n",
    "invalid_enz = ['4.1.2.36']\n",
    "invalid_rxn = ['R00326','R00710','R00711','R01019']\n",
    "\n",
    "wccm = WholeCellConsortiumModel(valid_cofactors=valid_cofactors)\n",
    "wccm.generate_whole_network('test',invalid_enz=invalid_enz,invalid_rxn=invalid_rxn)\n",
    "test_graph_2 = wccm.seek_optimal_network('test',['C00186'],['C00033','C00058','C00469'],2,2)\n",
    "# test_graph_2 = wccm.seek_optimal_network('test',['C00186'],['C00058','C00084'],2,2)\n",
    "wccm.visualize_graph('test')\n",
    "\n",
    "# IMPROVEMENTS:\n",
    "# how can we ensure that we use EtOH+acetate mixture? - calculate overall redox, set EtOH/acetate input ratio before optimization\n",
    "\n",
    "# set EtOH/acetate input ratio based on overall mass/redox balance\n",
    "# restrict valid cofactors\n",
    "# restrict small gas input into reactions\n",
    "# how to get reactions from BRENDA as well?\n",
    "# how to decide on best overall mass/redox balance when there are multiple options? Maybe run an optimization for all of them? - constrain by substrate ratios?\n",
    "# how to determine optimal overall redox balance via optimization with COBRA? Effectively determining best substrate ratios for a given product"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wccm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
