{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pathways that work:\n",
    "\n",
    "- Malate\n",
    "\n",
    "- Citrate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CatPred Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating kinetic parameters for all enzymes in network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "item {'substrates': ['Formate', 'Acetyl-CoA', 'Pyruvate', 'CoA'], 'SMILES': ['C(=O)[O-]', 'CC(=O)SCCNC(=O)CCNC(=O)[C@@H](C(C)(C)COP(=O)(O)OP(=O)(O)OC[C@@H]1[C@H]([C@H]([C@@H](O1)N2C=NC3=C(N=CN=C32)N)O)OP(=O)(O)O)O', 'CC(=O)C(=O)[O-]', 'CC(C)(COP(=O)(O)OP(=O)(O)OC[C@@H]1[C@H]([C@H]([C@@H](O1)N2C=NC3=C(N=CN=C32)N)O)OP(=O)(O)O)[C@H](C(=O)NCCC(=O)NCCS)O'], 'sequences': ['MSELNEKLATAWEGFTKGDWQNEVNVRDFIQKNYTPYEGDESFLAGATEATTTLWDKVMEGVKLENRTHAPVDFDTAVASTITSHDAGYINKQLEKIVGLQTEAPLKRALIPFGGIKMIEGSCKAYNRELDPMIKKIFTEYRKTHNQGVFDVYTPDILRCRKSGVLTGLPDAYGRGRIIGDYRRVALYGIDYLMKDKLAQFTSLQADLENGVNLEQTIRLREEIAEQHRALGQMKEMAAKYGYDISGPATNAQEAIQWTYFGYLAAVKSQNGAAMSFGRTSTFLDVYIERDLKAGKITEQEAQEMVDHLVMKLRMVRFLRTPEYDELFSGDPIWATESIGGMGLDGRTLVTKNSFRFLNTLYTMGPSPEPNMTILWSEKLPLNFKKFAAKVSIDTSSLQYENDDLMRPDFNNDDYAIACCVSPMIVGKQMQFFGARANLAKTMLYAINGGVDEKLKMQVGPKSEPIKGDVLNYDEVMERMDHFMDWLAKQYITALNIIHYMHDKYSYEASLMALHDRDVIRTMACGIAGLSVAADSLSAIKYAKVKPIRDEDGLAIDFEIEGEYPQFGNNDPRVDDLAVDLVERFMKKIQKLHTYRDAIPTQSVLTITSNVVYGKKTGNTPDGRRAGAPFGPGANPMHGRDQKGAVASLTSVAKLPFAYAKDGISYTFSIVPNALGKDDEVRKTNLAGLMDGYFHHEASIEGGQHLNVNVMNREMLLDAMENPEKYPQLTIRVSGYAVRFNSLTKEQQQDVITRTFTQSM', 'MSELNEKLATAWEGFTKGDWQNEVNVRDFIQKNYTPYEGDESFLAGATEATTTLWDKVMEGVKLENRTHAPVDFDTAVASTITSHDAGYINKQLEKIVGLQTEAPLKRALIPFGGIKMIEGSCKAYNRELDPMIKKIFTEYRKTHNQGVFDVYTPDILRCRKSGVLTGLPDAYGRGRIIGDYRRVALYGIDYLMKDKLAQFTSLQADLENGVNLEQTIRLREEIAEQHRALGQMKEMAAKYGYDISGPATNAQEAIQWTYFGYLAAVKSQNGAAMSFGRTSTFLDVYIERDLKAGKITEQEAQEMVDHLVMKLRMVRFLRTPEYDELFSGDPIWATESIGGMGLDGRTLVTKNSFRFLNTLYTMGPSPEPNMTILWSEKLPLNFKKFAAKVSIDTSSLQYENDDLMRPDFNNDDYAIACCVSPMIVGKQMQFFGARANLAKTMLYAINGGVDEKLKMQVGPKSEPIKGDVLNYDEVMERMDHFMDWLAKQYITALNIIHYMHDKYSYEASLMALHDRDVIRTMACGIAGLSVAADSLSAIKYAKVKPIRDEDGLAIDFEIEGEYPQFGNNDPRVDDLAVDLVERFMKKIQKLHTYRDAIPTQSVLTITSNVVYGKKTGNTPDGRRAGAPFGPGANPMHGRDQKGAVASLTSVAKLPFAYAKDGISYTFSIVPNALGKDDEVRKTNLAGLMDGYFHHEASIEGGQHLNVNVMNREMLLDAMENPEKYPQLTIRVSGYAVRFNSLTKEQQQDVITRTFTQSM', 'MSELNEKLATAWEGFTKGDWQNEVNVRDFIQKNYTPYEGDESFLAGATEATTTLWDKVMEGVKLENRTHAPVDFDTAVASTITSHDAGYINKQLEKIVGLQTEAPLKRALIPFGGIKMIEGSCKAYNRELDPMIKKIFTEYRKTHNQGVFDVYTPDILRCRKSGVLTGLPDAYGRGRIIGDYRRVALYGIDYLMKDKLAQFTSLQADLENGVNLEQTIRLREEIAEQHRALGQMKEMAAKYGYDISGPATNAQEAIQWTYFGYLAAVKSQNGAAMSFGRTSTFLDVYIERDLKAGKITEQEAQEMVDHLVMKLRMVRFLRTPEYDELFSGDPIWATESIGGMGLDGRTLVTKNSFRFLNTLYTMGPSPEPNMTILWSEKLPLNFKKFAAKVSIDTSSLQYENDDLMRPDFNNDDYAIACCVSPMIVGKQMQFFGARANLAKTMLYAINGGVDEKLKMQVGPKSEPIKGDVLNYDEVMERMDHFMDWLAKQYITALNIIHYMHDKYSYEASLMALHDRDVIRTMACGIAGLSVAADSLSAIKYAKVKPIRDEDGLAIDFEIEGEYPQFGNNDPRVDDLAVDLVERFMKKIQKLHTYRDAIPTQSVLTITSNVVYGKKTGNTPDGRRAGAPFGPGANPMHGRDQKGAVASLTSVAKLPFAYAKDGISYTFSIVPNALGKDDEVRKTNLAGLMDGYFHHEASIEGGQHLNVNVMNREMLLDAMENPEKYPQLTIRVSGYAVRFNSLTKEQQQDVITRTFTQSM', 'MSELNEKLATAWEGFTKGDWQNEVNVRDFIQKNYTPYEGDESFLAGATEATTTLWDKVMEGVKLENRTHAPVDFDTAVASTITSHDAGYINKQLEKIVGLQTEAPLKRALIPFGGIKMIEGSCKAYNRELDPMIKKIFTEYRKTHNQGVFDVYTPDILRCRKSGVLTGLPDAYGRGRIIGDYRRVALYGIDYLMKDKLAQFTSLQADLENGVNLEQTIRLREEIAEQHRALGQMKEMAAKYGYDISGPATNAQEAIQWTYFGYLAAVKSQNGAAMSFGRTSTFLDVYIERDLKAGKITEQEAQEMVDHLVMKLRMVRFLRTPEYDELFSGDPIWATESIGGMGLDGRTLVTKNSFRFLNTLYTMGPSPEPNMTILWSEKLPLNFKKFAAKVSIDTSSLQYENDDLMRPDFNNDDYAIACCVSPMIVGKQMQFFGARANLAKTMLYAINGGVDEKLKMQVGPKSEPIKGDVLNYDEVMERMDHFMDWLAKQYITALNIIHYMHDKYSYEASLMALHDRDVIRTMACGIAGLSVAADSLSAIKYAKVKPIRDEDGLAIDFEIEGEYPQFGNNDPRVDDLAVDLVERFMKKIQKLHTYRDAIPTQSVLTITSNVVYGKKTGNTPDGRRAGAPFGPGANPMHGRDQKGAVASLTSVAKLPFAYAKDGISYTFSIVPNALGKDDEVRKTNLAGLMDGYFHHEASIEGGQHLNVNVMNREMLLDAMENPEKYPQLTIRVSGYAVRFNSLTKEQQQDVITRTFTQSM'], 'pdb_paths': ['seq1.seq', 'seq1.seq', 'seq1.seq', 'seq1.seq'], 'subs': ['C(=O)[O-]', 'CC(=O)SCCNC(=O)CCNC(=O)[C@@H](C(C)(C)COP(=O)(O)OP(=O)(O)OC[C@@H]1[C@H]([C@H]([C@@H](O1)N2C=NC3=C(N=CN=C32)N)O)OP(=O)(O)O)O'], 'stoich': [1, 1, 1, 1], 'EC': '2.3.1.54'}\n",
      "starting R00212\n",
      "input |    | substrate   | SMILES                                                                                                                     | sequence                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 | pdbpath   |\n",
      "|---:|:------------|:---------------------------------------------------------------------------------------------------------------------------|:-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:----------|\n",
      "|  0 | Formate     | C(=O)[O-]                                                                                                                  | MSELNEKLATAWEGFTKGDWQNEVNVRDFIQKNYTPYEGDESFLAGATEATTTLWDKVMEGVKLENRTHAPVDFDTAVASTITSHDAGYINKQLEKIVGLQTEAPLKRALIPFGGIKMIEGSCKAYNRELDPMIKKIFTEYRKTHNQGVFDVYTPDILRCRKSGVLTGLPDAYGRGRIIGDYRRVALYGIDYLMKDKLAQFTSLQADLENGVNLEQTIRLREEIAEQHRALGQMKEMAAKYGYDISGPATNAQEAIQWTYFGYLAAVKSQNGAAMSFGRTSTFLDVYIERDLKAGKITEQEAQEMVDHLVMKLRMVRFLRTPEYDELFSGDPIWATESIGGMGLDGRTLVTKNSFRFLNTLYTMGPSPEPNMTILWSEKLPLNFKKFAAKVSIDTSSLQYENDDLMRPDFNNDDYAIACCVSPMIVGKQMQFFGARANLAKTMLYAINGGVDEKLKMQVGPKSEPIKGDVLNYDEVMERMDHFMDWLAKQYITALNIIHYMHDKYSYEASLMALHDRDVIRTMACGIAGLSVAADSLSAIKYAKVKPIRDEDGLAIDFEIEGEYPQFGNNDPRVDDLAVDLVERFMKKIQKLHTYRDAIPTQSVLTITSNVVYGKKTGNTPDGRRAGAPFGPGANPMHGRDQKGAVASLTSVAKLPFAYAKDGISYTFSIVPNALGKDDEVRKTNLAGLMDGYFHHEASIEGGQHLNVNVMNREMLLDAMENPEKYPQLTIRVSGYAVRFNSLTKEQQQDVITRTFTQSM | seq1.seq  |\n",
      "|  1 | Acetyl-CoA  | CC(=O)SCCNC(=O)CCNC(=O)[C@@H](C(C)(C)COP(=O)(O)OP(=O)(O)OC[C@@H]1[C@H]([C@H]([C@@H](O1)N2C=NC3=C(N=CN=C32)N)O)OP(=O)(O)O)O | MSELNEKLATAWEGFTKGDWQNEVNVRDFIQKNYTPYEGDESFLAGATEATTTLWDKVMEGVKLENRTHAPVDFDTAVASTITSHDAGYINKQLEKIVGLQTEAPLKRALIPFGGIKMIEGSCKAYNRELDPMIKKIFTEYRKTHNQGVFDVYTPDILRCRKSGVLTGLPDAYGRGRIIGDYRRVALYGIDYLMKDKLAQFTSLQADLENGVNLEQTIRLREEIAEQHRALGQMKEMAAKYGYDISGPATNAQEAIQWTYFGYLAAVKSQNGAAMSFGRTSTFLDVYIERDLKAGKITEQEAQEMVDHLVMKLRMVRFLRTPEYDELFSGDPIWATESIGGMGLDGRTLVTKNSFRFLNTLYTMGPSPEPNMTILWSEKLPLNFKKFAAKVSIDTSSLQYENDDLMRPDFNNDDYAIACCVSPMIVGKQMQFFGARANLAKTMLYAINGGVDEKLKMQVGPKSEPIKGDVLNYDEVMERMDHFMDWLAKQYITALNIIHYMHDKYSYEASLMALHDRDVIRTMACGIAGLSVAADSLSAIKYAKVKPIRDEDGLAIDFEIEGEYPQFGNNDPRVDDLAVDLVERFMKKIQKLHTYRDAIPTQSVLTITSNVVYGKKTGNTPDGRRAGAPFGPGANPMHGRDQKGAVASLTSVAKLPFAYAKDGISYTFSIVPNALGKDDEVRKTNLAGLMDGYFHHEASIEGGQHLNVNVMNREMLLDAMENPEKYPQLTIRVSGYAVRFNSLTKEQQQDVITRTFTQSM | seq1.seq  |\n",
      "|  2 | Pyruvate    | CC(=O)C(=O)[O-]                                                                                                            | MSELNEKLATAWEGFTKGDWQNEVNVRDFIQKNYTPYEGDESFLAGATEATTTLWDKVMEGVKLENRTHAPVDFDTAVASTITSHDAGYINKQLEKIVGLQTEAPLKRALIPFGGIKMIEGSCKAYNRELDPMIKKIFTEYRKTHNQGVFDVYTPDILRCRKSGVLTGLPDAYGRGRIIGDYRRVALYGIDYLMKDKLAQFTSLQADLENGVNLEQTIRLREEIAEQHRALGQMKEMAAKYGYDISGPATNAQEAIQWTYFGYLAAVKSQNGAAMSFGRTSTFLDVYIERDLKAGKITEQEAQEMVDHLVMKLRMVRFLRTPEYDELFSGDPIWATESIGGMGLDGRTLVTKNSFRFLNTLYTMGPSPEPNMTILWSEKLPLNFKKFAAKVSIDTSSLQYENDDLMRPDFNNDDYAIACCVSPMIVGKQMQFFGARANLAKTMLYAINGGVDEKLKMQVGPKSEPIKGDVLNYDEVMERMDHFMDWLAKQYITALNIIHYMHDKYSYEASLMALHDRDVIRTMACGIAGLSVAADSLSAIKYAKVKPIRDEDGLAIDFEIEGEYPQFGNNDPRVDDLAVDLVERFMKKIQKLHTYRDAIPTQSVLTITSNVVYGKKTGNTPDGRRAGAPFGPGANPMHGRDQKGAVASLTSVAKLPFAYAKDGISYTFSIVPNALGKDDEVRKTNLAGLMDGYFHHEASIEGGQHLNVNVMNREMLLDAMENPEKYPQLTIRVSGYAVRFNSLTKEQQQDVITRTFTQSM | seq1.seq  |\n",
      "|  3 | CoA         | CC(C)(COP(=O)(O)OP(=O)(O)OC[C@@H]1[C@H]([C@H]([C@@H](O1)N2C=NC3=C(N=CN=C32)N)O)OP(=O)(O)O)[C@H](C(=O)NCCC(=O)NCCS)O        | MSELNEKLATAWEGFTKGDWQNEVNVRDFIQKNYTPYEGDESFLAGATEATTTLWDKVMEGVKLENRTHAPVDFDTAVASTITSHDAGYINKQLEKIVGLQTEAPLKRALIPFGGIKMIEGSCKAYNRELDPMIKKIFTEYRKTHNQGVFDVYTPDILRCRKSGVLTGLPDAYGRGRIIGDYRRVALYGIDYLMKDKLAQFTSLQADLENGVNLEQTIRLREEIAEQHRALGQMKEMAAKYGYDISGPATNAQEAIQWTYFGYLAAVKSQNGAAMSFGRTSTFLDVYIERDLKAGKITEQEAQEMVDHLVMKLRMVRFLRTPEYDELFSGDPIWATESIGGMGLDGRTLVTKNSFRFLNTLYTMGPSPEPNMTILWSEKLPLNFKKFAAKVSIDTSSLQYENDDLMRPDFNNDDYAIACCVSPMIVGKQMQFFGARANLAKTMLYAINGGVDEKLKMQVGPKSEPIKGDVLNYDEVMERMDHFMDWLAKQYITALNIIHYMHDKYSYEASLMALHDRDVIRTMACGIAGLSVAADSLSAIKYAKVKPIRDEDGLAIDFEIEGEYPQFGNNDPRVDDLAVDLVERFMKKIQKLHTYRDAIPTQSVLTITSNVVYGKKTGNTPDGRRAGAPFGPGANPMHGRDQKGAVASLTSVAKLPFAYAKDGISYTFSIVPNALGKDDEVRKTNLAGLMDGYFHHEASIEGGQHLNVNVMNREMLLDAMENPEKYPQLTIRVSGYAVRFNSLTKEQQQDVITRTFTQSM | seq1.seq  |\n",
      "input2 ['C(=O)[O-]', 'CC(=O)SCCNC(=O)CCNC(=O)[C@@H](C(C)(C)COP(=O)(O)OP(=O)(O)OC[C@@H]1[C@H]([C@H]([C@@H](O1)N2C=NC3=C(N=CN=C32)N)O)OP(=O)(O)O)O', 'CC(=O)C(=O)[O-]', 'CC(C)(COP(=O)(O)OP(=O)(O)OC[C@@H]1[C@H]([C@H]([C@@H](O1)N2C=NC3=C(N=CN=C32)N)O)OP(=O)(O)O)[C@H](C(=O)NCCC(=O)NCCS)O']\n",
      "/home/xuvin/py_repos/wccm/src/catpred_pipeline/catpred\n",
      "Predicting.. This will take a while..\n",
      "calculating protein embed only on cpu\n",
      "Loading training args\n",
      "Loading models\n",
      "Setting molecule featurization parameters to default.\n",
      "Loading data\n",
      "Validating SMILES\n",
      "Test size = 4\n",
      "Creating protein model\n",
      "MoleculeModel(\n",
      "  (softplus): Softplus(beta=1.0, threshold=20.0)\n",
      "  (encoder): MPN(\n",
      "    (encoder): ModuleList(\n",
      "      (0): MPNEncoder(\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (act_func): ReLU()\n",
      "        (W_i): Linear(in_features=147, out_features=300, bias=False)\n",
      "        (W_h): Linear(in_features=300, out_features=300, bias=False)\n",
      "        (W_o): Linear(in_features=433, out_features=300, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (seq_embedder): Embedding(21, 36, padding_idx=20)\n",
      "  (rotary_embedder): RotaryEmbedding()\n",
      "  (multihead_attn): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=36, out_features=36, bias=True)\n",
      "  )\n",
      "  (attentive_pooler): AttentivePooling(\n",
      "    (linear1): Linear(in_features=1316, out_features=1316, bias=True)\n",
      "    (tanh): Tanh()\n",
      "    (linear2): Linear(in_features=1316, out_features=1, bias=True)\n",
      "    (softmax): Softmax(dim=1)\n",
      "  )\n",
      "  (readout): Sequential(\n",
      "    (0): Dropout(p=0.0, inplace=False)\n",
      "    (1): Linear(in_features=1616, out_features=300, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.0, inplace=False)\n",
      "    (4): Linear(in_features=300, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
      "Loading pretrained parameter \"seq_embedder.weight\".\n",
      "Loading pretrained parameter \"rotary_embedder.freqs\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_weight\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_bias\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.weight\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.bias\".\n",
      "Loading pretrained parameter \"readout.1.weight\".\n",
      "Loading pretrained parameter \"readout.1.bias\".\n",
      "Loading pretrained parameter \"readout.4.weight\".\n",
      "Loading pretrained parameter \"readout.4.bias\".\n",
      "Moving model to cuda\n",
      "Creating protein model\n",
      "MoleculeModel(\n",
      "  (softplus): Softplus(beta=1.0, threshold=20.0)\n",
      "  (encoder): MPN(\n",
      "    (encoder): ModuleList(\n",
      "      (0): MPNEncoder(\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (act_func): ReLU()\n",
      "        (W_i): Linear(in_features=147, out_features=300, bias=False)\n",
      "        (W_h): Linear(in_features=300, out_features=300, bias=False)\n",
      "        (W_o): Linear(in_features=433, out_features=300, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (seq_embedder): Embedding(21, 36, padding_idx=20)\n",
      "  (rotary_embedder): RotaryEmbedding()\n",
      "  (multihead_attn): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=36, out_features=36, bias=True)\n",
      "  )\n",
      "  (attentive_pooler): AttentivePooling(\n",
      "    (linear1): Linear(in_features=1316, out_features=1316, bias=True)\n",
      "    (tanh): Tanh()\n",
      "    (linear2): Linear(in_features=1316, out_features=1, bias=True)\n",
      "    (softmax): Softmax(dim=1)\n",
      "  )\n",
      "  (readout): Sequential(\n",
      "    (0): Dropout(p=0.0, inplace=False)\n",
      "    (1): Linear(in_features=1616, out_features=300, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.0, inplace=False)\n",
      "    (4): Linear(in_features=300, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
      "Loading pretrained parameter \"seq_embedder.weight\".\n",
      "Loading pretrained parameter \"rotary_embedder.freqs\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_weight\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_bias\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.weight\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.bias\".\n",
      "Loading pretrained parameter \"readout.1.weight\".\n",
      "Loading pretrained parameter \"readout.1.bias\".\n",
      "Loading pretrained parameter \"readout.4.weight\".\n",
      "Loading pretrained parameter \"readout.4.bias\".\n",
      "Moving model to cuda\n",
      "Creating protein model\n",
      "MoleculeModel(\n",
      "  (softplus): Softplus(beta=1.0, threshold=20.0)\n",
      "  (encoder): MPN(\n",
      "    (encoder): ModuleList(\n",
      "      (0): MPNEncoder(\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (act_func): ReLU()\n",
      "        (W_i): Linear(in_features=147, out_features=300, bias=False)\n",
      "        (W_h): Linear(in_features=300, out_features=300, bias=False)\n",
      "        (W_o): Linear(in_features=433, out_features=300, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (seq_embedder): Embedding(21, 36, padding_idx=20)\n",
      "  (rotary_embedder): RotaryEmbedding()\n",
      "  (multihead_attn): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=36, out_features=36, bias=True)\n",
      "  )\n",
      "  (attentive_pooler): AttentivePooling(\n",
      "    (linear1): Linear(in_features=1316, out_features=1316, bias=True)\n",
      "    (tanh): Tanh()\n",
      "    (linear2): Linear(in_features=1316, out_features=1, bias=True)\n",
      "    (softmax): Softmax(dim=1)\n",
      "  )\n",
      "  (readout): Sequential(\n",
      "    (0): Dropout(p=0.0, inplace=False)\n",
      "    (1): Linear(in_features=1616, out_features=300, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.0, inplace=False)\n",
      "    (4): Linear(in_features=300, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
      "Loading pretrained parameter \"seq_embedder.weight\".\n",
      "Loading pretrained parameter \"rotary_embedder.freqs\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_weight\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_bias\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.weight\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.bias\".\n",
      "Loading pretrained parameter \"readout.1.weight\".\n",
      "Loading pretrained parameter \"readout.1.bias\".\n",
      "Loading pretrained parameter \"readout.4.weight\".\n",
      "Loading pretrained parameter \"readout.4.bias\".\n",
      "Moving model to cuda\n",
      "Creating protein model\n",
      "MoleculeModel(\n",
      "  (softplus): Softplus(beta=1.0, threshold=20.0)\n",
      "  (encoder): MPN(\n",
      "    (encoder): ModuleList(\n",
      "      (0): MPNEncoder(\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (act_func): ReLU()\n",
      "        (W_i): Linear(in_features=147, out_features=300, bias=False)\n",
      "        (W_h): Linear(in_features=300, out_features=300, bias=False)\n",
      "        (W_o): Linear(in_features=433, out_features=300, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (seq_embedder): Embedding(21, 36, padding_idx=20)\n",
      "  (rotary_embedder): RotaryEmbedding()\n",
      "  (multihead_attn): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=36, out_features=36, bias=True)\n",
      "  )\n",
      "  (attentive_pooler): AttentivePooling(\n",
      "    (linear1): Linear(in_features=1316, out_features=1316, bias=True)\n",
      "    (tanh): Tanh()\n",
      "    (linear2): Linear(in_features=1316, out_features=1, bias=True)\n",
      "    (softmax): Softmax(dim=1)\n",
      "  )\n",
      "  (readout): Sequential(\n",
      "    (0): Dropout(p=0.0, inplace=False)\n",
      "    (1): Linear(in_features=1616, out_features=300, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.0, inplace=False)\n",
      "    (4): Linear(in_features=300, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
      "Loading pretrained parameter \"seq_embedder.weight\".\n",
      "Loading pretrained parameter \"rotary_embedder.freqs\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_weight\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_bias\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.weight\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.bias\".\n",
      "Loading pretrained parameter \"readout.1.weight\".\n",
      "Loading pretrained parameter \"readout.1.bias\".\n",
      "Loading pretrained parameter \"readout.4.weight\".\n",
      "Loading pretrained parameter \"readout.4.bias\".\n",
      "Moving model to cuda\n",
      "Creating protein model\n",
      "MoleculeModel(\n",
      "  (softplus): Softplus(beta=1.0, threshold=20.0)\n",
      "  (encoder): MPN(\n",
      "    (encoder): ModuleList(\n",
      "      (0): MPNEncoder(\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (act_func): ReLU()\n",
      "        (W_i): Linear(in_features=147, out_features=300, bias=False)\n",
      "        (W_h): Linear(in_features=300, out_features=300, bias=False)\n",
      "        (W_o): Linear(in_features=433, out_features=300, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (seq_embedder): Embedding(21, 36, padding_idx=20)\n",
      "  (rotary_embedder): RotaryEmbedding()\n",
      "  (multihead_attn): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=36, out_features=36, bias=True)\n",
      "  )\n",
      "  (attentive_pooler): AttentivePooling(\n",
      "    (linear1): Linear(in_features=1316, out_features=1316, bias=True)\n",
      "    (tanh): Tanh()\n",
      "    (linear2): Linear(in_features=1316, out_features=1, bias=True)\n",
      "    (softmax): Softmax(dim=1)\n",
      "  )\n",
      "  (readout): Sequential(\n",
      "    (0): Dropout(p=0.0, inplace=False)\n",
      "    (1): Linear(in_features=1616, out_features=300, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.0, inplace=False)\n",
      "    (4): Linear(in_features=300, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
      "Loading pretrained parameter \"seq_embedder.weight\".\n",
      "Loading pretrained parameter \"rotary_embedder.freqs\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_weight\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_bias\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.weight\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.bias\".\n",
      "Loading pretrained parameter \"readout.1.weight\".\n",
      "Loading pretrained parameter \"readout.1.bias\".\n",
      "Loading pretrained parameter \"readout.4.weight\".\n",
      "Loading pretrained parameter \"readout.4.bias\".\n",
      "Moving model to cuda\n",
      "Creating protein model\n",
      "MoleculeModel(\n",
      "  (softplus): Softplus(beta=1.0, threshold=20.0)\n",
      "  (encoder): MPN(\n",
      "    (encoder): ModuleList(\n",
      "      (0): MPNEncoder(\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (act_func): ReLU()\n",
      "        (W_i): Linear(in_features=147, out_features=300, bias=False)\n",
      "        (W_h): Linear(in_features=300, out_features=300, bias=False)\n",
      "        (W_o): Linear(in_features=433, out_features=300, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (seq_embedder): Embedding(21, 36, padding_idx=20)\n",
      "  (rotary_embedder): RotaryEmbedding()\n",
      "  (multihead_attn): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=36, out_features=36, bias=True)\n",
      "  )\n",
      "  (attentive_pooler): AttentivePooling(\n",
      "    (linear1): Linear(in_features=1316, out_features=1316, bias=True)\n",
      "    (tanh): Tanh()\n",
      "    (linear2): Linear(in_features=1316, out_features=1, bias=True)\n",
      "    (softmax): Softmax(dim=1)\n",
      "  )\n",
      "  (readout): Sequential(\n",
      "    (0): Dropout(p=0.0, inplace=False)\n",
      "    (1): Linear(in_features=1616, out_features=300, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.0, inplace=False)\n",
      "    (4): Linear(in_features=300, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
      "Loading pretrained parameter \"seq_embedder.weight\".\n",
      "Loading pretrained parameter \"rotary_embedder.freqs\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_weight\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_bias\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.weight\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.bias\".\n",
      "Loading pretrained parameter \"readout.1.weight\".\n",
      "Loading pretrained parameter \"readout.1.bias\".\n",
      "Loading pretrained parameter \"readout.4.weight\".\n",
      "Loading pretrained parameter \"readout.4.bias\".\n",
      "Moving model to cuda\n",
      "Creating protein model\n",
      "MoleculeModel(\n",
      "  (softplus): Softplus(beta=1.0, threshold=20.0)\n",
      "  (encoder): MPN(\n",
      "    (encoder): ModuleList(\n",
      "      (0): MPNEncoder(\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (act_func): ReLU()\n",
      "        (W_i): Linear(in_features=147, out_features=300, bias=False)\n",
      "        (W_h): Linear(in_features=300, out_features=300, bias=False)\n",
      "        (W_o): Linear(in_features=433, out_features=300, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (seq_embedder): Embedding(21, 36, padding_idx=20)\n",
      "  (rotary_embedder): RotaryEmbedding()\n",
      "  (multihead_attn): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=36, out_features=36, bias=True)\n",
      "  )\n",
      "  (attentive_pooler): AttentivePooling(\n",
      "    (linear1): Linear(in_features=1316, out_features=1316, bias=True)\n",
      "    (tanh): Tanh()\n",
      "    (linear2): Linear(in_features=1316, out_features=1, bias=True)\n",
      "    (softmax): Softmax(dim=1)\n",
      "  )\n",
      "  (readout): Sequential(\n",
      "    (0): Dropout(p=0.0, inplace=False)\n",
      "    (1): Linear(in_features=1616, out_features=300, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.0, inplace=False)\n",
      "    (4): Linear(in_features=300, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
      "Loading pretrained parameter \"seq_embedder.weight\".\n",
      "Loading pretrained parameter \"rotary_embedder.freqs\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_weight\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_bias\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.weight\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.bias\".\n",
      "Loading pretrained parameter \"readout.1.weight\".\n",
      "Loading pretrained parameter \"readout.1.bias\".\n",
      "Loading pretrained parameter \"readout.4.weight\".\n",
      "Loading pretrained parameter \"readout.4.bias\".\n",
      "Moving model to cuda\n",
      "Creating protein model\n",
      "MoleculeModel(\n",
      "  (softplus): Softplus(beta=1.0, threshold=20.0)\n",
      "  (encoder): MPN(\n",
      "    (encoder): ModuleList(\n",
      "      (0): MPNEncoder(\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (act_func): ReLU()\n",
      "        (W_i): Linear(in_features=147, out_features=300, bias=False)\n",
      "        (W_h): Linear(in_features=300, out_features=300, bias=False)\n",
      "        (W_o): Linear(in_features=433, out_features=300, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (seq_embedder): Embedding(21, 36, padding_idx=20)\n",
      "  (rotary_embedder): RotaryEmbedding()\n",
      "  (multihead_attn): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=36, out_features=36, bias=True)\n",
      "  )\n",
      "  (attentive_pooler): AttentivePooling(\n",
      "    (linear1): Linear(in_features=1316, out_features=1316, bias=True)\n",
      "    (tanh): Tanh()\n",
      "    (linear2): Linear(in_features=1316, out_features=1, bias=True)\n",
      "    (softmax): Softmax(dim=1)\n",
      "  )\n",
      "  (readout): Sequential(\n",
      "    (0): Dropout(p=0.0, inplace=False)\n",
      "    (1): Linear(in_features=1616, out_features=300, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.0, inplace=False)\n",
      "    (4): Linear(in_features=300, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
      "Loading pretrained parameter \"seq_embedder.weight\".\n",
      "Loading pretrained parameter \"rotary_embedder.freqs\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_weight\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_bias\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.weight\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.bias\".\n",
      "Loading pretrained parameter \"readout.1.weight\".\n",
      "Loading pretrained parameter \"readout.1.bias\".\n",
      "Loading pretrained parameter \"readout.4.weight\".\n",
      "Loading pretrained parameter \"readout.4.bias\".\n",
      "Moving model to cuda\n",
      "Creating protein model\n",
      "MoleculeModel(\n",
      "  (softplus): Softplus(beta=1.0, threshold=20.0)\n",
      "  (encoder): MPN(\n",
      "    (encoder): ModuleList(\n",
      "      (0): MPNEncoder(\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (act_func): ReLU()\n",
      "        (W_i): Linear(in_features=147, out_features=300, bias=False)\n",
      "        (W_h): Linear(in_features=300, out_features=300, bias=False)\n",
      "        (W_o): Linear(in_features=433, out_features=300, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (seq_embedder): Embedding(21, 36, padding_idx=20)\n",
      "  (rotary_embedder): RotaryEmbedding()\n",
      "  (multihead_attn): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=36, out_features=36, bias=True)\n",
      "  )\n",
      "  (attentive_pooler): AttentivePooling(\n",
      "    (linear1): Linear(in_features=1316, out_features=1316, bias=True)\n",
      "    (tanh): Tanh()\n",
      "    (linear2): Linear(in_features=1316, out_features=1, bias=True)\n",
      "    (softmax): Softmax(dim=1)\n",
      "  )\n",
      "  (readout): Sequential(\n",
      "    (0): Dropout(p=0.0, inplace=False)\n",
      "    (1): Linear(in_features=1616, out_features=300, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.0, inplace=False)\n",
      "    (4): Linear(in_features=300, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
      "Loading pretrained parameter \"seq_embedder.weight\".\n",
      "Loading pretrained parameter \"rotary_embedder.freqs\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_weight\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_bias\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.weight\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.bias\".\n",
      "Loading pretrained parameter \"readout.1.weight\".\n",
      "Loading pretrained parameter \"readout.1.bias\".\n",
      "Loading pretrained parameter \"readout.4.weight\".\n",
      "Loading pretrained parameter \"readout.4.bias\".\n",
      "Moving model to cuda\n",
      "Creating protein model\n",
      "MoleculeModel(\n",
      "  (softplus): Softplus(beta=1.0, threshold=20.0)\n",
      "  (encoder): MPN(\n",
      "    (encoder): ModuleList(\n",
      "      (0): MPNEncoder(\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (act_func): ReLU()\n",
      "        (W_i): Linear(in_features=147, out_features=300, bias=False)\n",
      "        (W_h): Linear(in_features=300, out_features=300, bias=False)\n",
      "        (W_o): Linear(in_features=433, out_features=300, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (seq_embedder): Embedding(21, 36, padding_idx=20)\n",
      "  (rotary_embedder): RotaryEmbedding()\n",
      "  (multihead_attn): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=36, out_features=36, bias=True)\n",
      "  )\n",
      "  (attentive_pooler): AttentivePooling(\n",
      "    (linear1): Linear(in_features=1316, out_features=1316, bias=True)\n",
      "    (tanh): Tanh()\n",
      "    (linear2): Linear(in_features=1316, out_features=1, bias=True)\n",
      "    (softmax): Softmax(dim=1)\n",
      "  )\n",
      "  (readout): Sequential(\n",
      "    (0): Dropout(p=0.0, inplace=False)\n",
      "    (1): Linear(in_features=1616, out_features=300, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.0, inplace=False)\n",
      "    (4): Linear(in_features=300, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
      "Loading pretrained parameter \"seq_embedder.weight\".\n",
      "Loading pretrained parameter \"rotary_embedder.freqs\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_weight\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_bias\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.weight\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.bias\".\n",
      "Loading pretrained parameter \"readout.1.weight\".\n",
      "Loading pretrained parameter \"readout.1.bias\".\n",
      "Loading pretrained parameter \"readout.4.weight\".\n",
      "Loading pretrained parameter \"readout.4.bias\".\n",
      "Moving model to cuda\n",
      "Saving predictions to ../pipeline_entry_kcat_input_output.csv\n",
      "Elapsed time = 0:00:08\n",
      "Prediction completed.\n",
      "Output saved to results/ pipeline_entry_kcat_input_output.csv\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xuvin/.conda/envs/catpred/lib/python3.9/site-packages/rotary_embedding_torch/rotary_embedding_torch.py:35: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  @autocast(enabled = False)\n",
      "/home/xuvin/.conda/envs/catpred/lib/python3.9/site-packages/rotary_embedding_torch/rotary_embedding_torch.py:268: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  @autocast(enabled = False)\n",
      "\n",
      "0it [00:00, ?it/s]/home/xuvin/py_repos/wccm/src/catpred_pipeline/catpred/catpred/data/cache_utils.py:95: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(str(entry_path))\n",
      "\n",
      "1it [00:00,  4.99it/s]\n",
      "4it [00:00, 18.82it/s]\n",
      "\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 30897.27it/s]\n",
      "\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 1634.73it/s]\n",
      "\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]/home/xuvin/py_repos/wccm/src/catpred_pipeline/catpred/catpred/utils.py:113: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(path, map_location=lambda storage, loc: storage)\n",
      "/home/xuvin/py_repos/wccm/src/catpred_pipeline/catpred/catpred/utils.py:446: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(path, map_location=lambda storage, loc: storage)\n",
      "\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "100%|██████████| 1/1 [00:02<00:00,  2.47s/it]\u001b[A\n",
      "\n",
      "                                             \u001b[A\n",
      " 10%|█         | 1/10 [00:04<00:37,  4.17s/it]/home/xuvin/py_repos/wccm/src/catpred_pipeline/catpred/catpred/utils.py:113: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(path, map_location=lambda storage, loc: storage)\n",
      "/home/xuvin/py_repos/wccm/src/catpred_pipeline/catpred/catpred/utils.py:446: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(path, map_location=lambda storage, loc: storage)\n",
      "\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "                                     \u001b[A\n",
      " 20%|██        | 2/10 [00:04<00:14,  1.87s/it]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "                                     \u001b[A\n",
      " 30%|███       | 3/10 [00:04<00:08,  1.15s/it]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "                                     \u001b[A\n",
      " 40%|████      | 4/10 [00:05<00:05,  1.12it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "                                     \u001b[A\n",
      " 50%|█████     | 5/10 [00:05<00:03,  1.49it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "                                     \u001b[A\n",
      " 60%|██████    | 6/10 [00:05<00:02,  1.86it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "                                     \u001b[A\n",
      " 70%|███████   | 7/10 [00:06<00:01,  2.17it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "                                     \u001b[A\n",
      " 80%|████████  | 8/10 [00:06<00:00,  2.11it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "                                     \u001b[A\n",
      " 90%|█████████ | 9/10 [00:06<00:00,  2.42it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "                                     \u001b[A\n",
      "100%|██████████| 10/10 [00:07<00:00,  2.65it/s]\n",
      "100%|██████████| 10/10 [00:07<00:00,  1.40it/s]\n",
      "\n",
      "/home/xuvin/.conda/envs/catpred/lib/python3.9/site-packages/rotary_embedding_torch/rotary_embedding_torch.py:35: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  @autocast(enabled = False)\n",
      "/home/xuvin/.conda/envs/catpred/lib/python3.9/site-packages/rotary_embedding_torch/rotary_embedding_torch.py:268: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  @autocast(enabled = False)\n",
      "\n",
      "0it [00:00, ?it/s]/home/xuvin/py_repos/wccm/src/catpred_pipeline/catpred/catpred/data/cache_utils.py:95: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(str(entry_path))\n",
      "\n",
      "4it [00:00, 88.34it/s]\n",
      "\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 34239.22it/s]\n",
      "\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 1090.85it/s]\n",
      "\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]/home/xuvin/py_repos/wccm/src/catpred_pipeline/catpred/catpred/utils.py:113: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(path, map_location=lambda storage, loc: storage)\n",
      "/home/xuvin/py_repos/wccm/src/catpred_pipeline/catpred/catpred/utils.py:446: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(path, map_location=lambda storage, loc: storage)\n",
      "\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.40s/it]\u001b[A\n",
      "\n",
      "                                             \u001b[A\n",
      " 10%|█         | 1/10 [00:03<00:35,  3.92s/it]/home/xuvin/py_repos/wccm/src/catpred_pipeline/catpred/catpred/utils.py:113: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(path, map_location=lambda storage, loc: storage)\n",
      "/home/xuvin/py_repos/wccm/src/catpred_pipeline/catpred/catpred/utils.py:446: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(path, map_location=lambda storage, loc: storage)\n",
      "\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "                                     \u001b[A\n",
      " 20%|██        | 2/10 [00:04<00:14,  1.78s/it]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "                                     \u001b[A\n",
      " 30%|███       | 3/10 [00:04<00:07,  1.10s/it]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "                                     \u001b[A\n",
      " 40%|████      | 4/10 [00:05<00:05,  1.15it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "                                     \u001b[A\n",
      " 50%|█████     | 5/10 [00:05<00:03,  1.53it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "                                     \u001b[A\n",
      " 60%|██████    | 6/10 [00:05<00:02,  1.95it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "                                     \u001b[A\n",
      " 70%|███████   | 7/10 [00:05<00:01,  2.36it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "                                     \u001b[A\n",
      " 80%|████████  | 8/10 [00:06<00:00,  2.24it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "                                     \u001b[A\n",
      " 90%|█████████ | 9/10 [00:06<00:00,  2.55it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "                                     \u001b[A\n",
      "100%|██████████| 10/10 [00:06<00:00,  2.84it/s]\n",
      "100%|██████████| 10/10 [00:06<00:00,  1.47it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/xuvin/py_repos/wccm/src/catpred_pipeline/catpred\n",
      "Predicting.. This will take a while..\n",
      "calculating protein embed only on cpu\n",
      "Loading training args\n",
      "Loading models\n",
      "Setting molecule featurization parameters to default.\n",
      "Loading data\n",
      "Validating SMILES\n",
      "Test size = 4\n",
      "Creating protein model\n",
      "MoleculeModel(\n",
      "  (softplus): Softplus(beta=1.0, threshold=20.0)\n",
      "  (encoder): MPN(\n",
      "    (encoder): ModuleList(\n",
      "      (0): MPNEncoder(\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (act_func): ReLU()\n",
      "        (W_i): Linear(in_features=147, out_features=300, bias=False)\n",
      "        (W_h): Linear(in_features=300, out_features=300, bias=False)\n",
      "        (W_o): Linear(in_features=433, out_features=300, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (seq_embedder): Embedding(21, 36, padding_idx=20)\n",
      "  (rotary_embedder): RotaryEmbedding()\n",
      "  (multihead_attn): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=36, out_features=36, bias=True)\n",
      "  )\n",
      "  (attentive_pooler): AttentivePooling(\n",
      "    (linear1): Linear(in_features=1316, out_features=1316, bias=True)\n",
      "    (tanh): Tanh()\n",
      "    (linear2): Linear(in_features=1316, out_features=1, bias=True)\n",
      "    (softmax): Softmax(dim=1)\n",
      "  )\n",
      "  (readout): Sequential(\n",
      "    (0): Dropout(p=0.0, inplace=False)\n",
      "    (1): Linear(in_features=1616, out_features=300, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.0, inplace=False)\n",
      "    (4): Linear(in_features=300, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
      "Loading pretrained parameter \"seq_embedder.weight\".\n",
      "Loading pretrained parameter \"rotary_embedder.freqs\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_weight\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_bias\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.weight\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.bias\".\n",
      "Loading pretrained parameter \"readout.1.weight\".\n",
      "Loading pretrained parameter \"readout.1.bias\".\n",
      "Loading pretrained parameter \"readout.4.weight\".\n",
      "Loading pretrained parameter \"readout.4.bias\".\n",
      "Moving model to cuda\n",
      "Creating protein model\n",
      "MoleculeModel(\n",
      "  (softplus): Softplus(beta=1.0, threshold=20.0)\n",
      "  (encoder): MPN(\n",
      "    (encoder): ModuleList(\n",
      "      (0): MPNEncoder(\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (act_func): ReLU()\n",
      "        (W_i): Linear(in_features=147, out_features=300, bias=False)\n",
      "        (W_h): Linear(in_features=300, out_features=300, bias=False)\n",
      "        (W_o): Linear(in_features=433, out_features=300, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (seq_embedder): Embedding(21, 36, padding_idx=20)\n",
      "  (rotary_embedder): RotaryEmbedding()\n",
      "  (multihead_attn): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=36, out_features=36, bias=True)\n",
      "  )\n",
      "  (attentive_pooler): AttentivePooling(\n",
      "    (linear1): Linear(in_features=1316, out_features=1316, bias=True)\n",
      "    (tanh): Tanh()\n",
      "    (linear2): Linear(in_features=1316, out_features=1, bias=True)\n",
      "    (softmax): Softmax(dim=1)\n",
      "  )\n",
      "  (readout): Sequential(\n",
      "    (0): Dropout(p=0.0, inplace=False)\n",
      "    (1): Linear(in_features=1616, out_features=300, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.0, inplace=False)\n",
      "    (4): Linear(in_features=300, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
      "Loading pretrained parameter \"seq_embedder.weight\".\n",
      "Loading pretrained parameter \"rotary_embedder.freqs\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_weight\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_bias\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.weight\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.bias\".\n",
      "Loading pretrained parameter \"readout.1.weight\".\n",
      "Loading pretrained parameter \"readout.1.bias\".\n",
      "Loading pretrained parameter \"readout.4.weight\".\n",
      "Loading pretrained parameter \"readout.4.bias\".\n",
      "Moving model to cuda\n",
      "Creating protein model\n",
      "MoleculeModel(\n",
      "  (softplus): Softplus(beta=1.0, threshold=20.0)\n",
      "  (encoder): MPN(\n",
      "    (encoder): ModuleList(\n",
      "      (0): MPNEncoder(\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (act_func): ReLU()\n",
      "        (W_i): Linear(in_features=147, out_features=300, bias=False)\n",
      "        (W_h): Linear(in_features=300, out_features=300, bias=False)\n",
      "        (W_o): Linear(in_features=433, out_features=300, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (seq_embedder): Embedding(21, 36, padding_idx=20)\n",
      "  (rotary_embedder): RotaryEmbedding()\n",
      "  (multihead_attn): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=36, out_features=36, bias=True)\n",
      "  )\n",
      "  (attentive_pooler): AttentivePooling(\n",
      "    (linear1): Linear(in_features=1316, out_features=1316, bias=True)\n",
      "    (tanh): Tanh()\n",
      "    (linear2): Linear(in_features=1316, out_features=1, bias=True)\n",
      "    (softmax): Softmax(dim=1)\n",
      "  )\n",
      "  (readout): Sequential(\n",
      "    (0): Dropout(p=0.0, inplace=False)\n",
      "    (1): Linear(in_features=1616, out_features=300, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.0, inplace=False)\n",
      "    (4): Linear(in_features=300, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
      "Loading pretrained parameter \"seq_embedder.weight\".\n",
      "Loading pretrained parameter \"rotary_embedder.freqs\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_weight\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_bias\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.weight\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.bias\".\n",
      "Loading pretrained parameter \"readout.1.weight\".\n",
      "Loading pretrained parameter \"readout.1.bias\".\n",
      "Loading pretrained parameter \"readout.4.weight\".\n",
      "Loading pretrained parameter \"readout.4.bias\".\n",
      "Moving model to cuda\n",
      "Creating protein model\n",
      "MoleculeModel(\n",
      "  (softplus): Softplus(beta=1.0, threshold=20.0)\n",
      "  (encoder): MPN(\n",
      "    (encoder): ModuleList(\n",
      "      (0): MPNEncoder(\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (act_func): ReLU()\n",
      "        (W_i): Linear(in_features=147, out_features=300, bias=False)\n",
      "        (W_h): Linear(in_features=300, out_features=300, bias=False)\n",
      "        (W_o): Linear(in_features=433, out_features=300, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (seq_embedder): Embedding(21, 36, padding_idx=20)\n",
      "  (rotary_embedder): RotaryEmbedding()\n",
      "  (multihead_attn): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=36, out_features=36, bias=True)\n",
      "  )\n",
      "  (attentive_pooler): AttentivePooling(\n",
      "    (linear1): Linear(in_features=1316, out_features=1316, bias=True)\n",
      "    (tanh): Tanh()\n",
      "    (linear2): Linear(in_features=1316, out_features=1, bias=True)\n",
      "    (softmax): Softmax(dim=1)\n",
      "  )\n",
      "  (readout): Sequential(\n",
      "    (0): Dropout(p=0.0, inplace=False)\n",
      "    (1): Linear(in_features=1616, out_features=300, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.0, inplace=False)\n",
      "    (4): Linear(in_features=300, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
      "Loading pretrained parameter \"seq_embedder.weight\".\n",
      "Loading pretrained parameter \"rotary_embedder.freqs\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_weight\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_bias\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.weight\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.bias\".\n",
      "Loading pretrained parameter \"readout.1.weight\".\n",
      "Loading pretrained parameter \"readout.1.bias\".\n",
      "Loading pretrained parameter \"readout.4.weight\".\n",
      "Loading pretrained parameter \"readout.4.bias\".\n",
      "Moving model to cuda\n",
      "Creating protein model\n",
      "MoleculeModel(\n",
      "  (softplus): Softplus(beta=1.0, threshold=20.0)\n",
      "  (encoder): MPN(\n",
      "    (encoder): ModuleList(\n",
      "      (0): MPNEncoder(\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (act_func): ReLU()\n",
      "        (W_i): Linear(in_features=147, out_features=300, bias=False)\n",
      "        (W_h): Linear(in_features=300, out_features=300, bias=False)\n",
      "        (W_o): Linear(in_features=433, out_features=300, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (seq_embedder): Embedding(21, 36, padding_idx=20)\n",
      "  (rotary_embedder): RotaryEmbedding()\n",
      "  (multihead_attn): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=36, out_features=36, bias=True)\n",
      "  )\n",
      "  (attentive_pooler): AttentivePooling(\n",
      "    (linear1): Linear(in_features=1316, out_features=1316, bias=True)\n",
      "    (tanh): Tanh()\n",
      "    (linear2): Linear(in_features=1316, out_features=1, bias=True)\n",
      "    (softmax): Softmax(dim=1)\n",
      "  )\n",
      "  (readout): Sequential(\n",
      "    (0): Dropout(p=0.0, inplace=False)\n",
      "    (1): Linear(in_features=1616, out_features=300, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.0, inplace=False)\n",
      "    (4): Linear(in_features=300, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
      "Loading pretrained parameter \"seq_embedder.weight\".\n",
      "Loading pretrained parameter \"rotary_embedder.freqs\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_weight\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_bias\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.weight\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.bias\".\n",
      "Loading pretrained parameter \"readout.1.weight\".\n",
      "Loading pretrained parameter \"readout.1.bias\".\n",
      "Loading pretrained parameter \"readout.4.weight\".\n",
      "Loading pretrained parameter \"readout.4.bias\".\n",
      "Moving model to cuda\n",
      "Creating protein model\n",
      "MoleculeModel(\n",
      "  (softplus): Softplus(beta=1.0, threshold=20.0)\n",
      "  (encoder): MPN(\n",
      "    (encoder): ModuleList(\n",
      "      (0): MPNEncoder(\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (act_func): ReLU()\n",
      "        (W_i): Linear(in_features=147, out_features=300, bias=False)\n",
      "        (W_h): Linear(in_features=300, out_features=300, bias=False)\n",
      "        (W_o): Linear(in_features=433, out_features=300, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (seq_embedder): Embedding(21, 36, padding_idx=20)\n",
      "  (rotary_embedder): RotaryEmbedding()\n",
      "  (multihead_attn): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=36, out_features=36, bias=True)\n",
      "  )\n",
      "  (attentive_pooler): AttentivePooling(\n",
      "    (linear1): Linear(in_features=1316, out_features=1316, bias=True)\n",
      "    (tanh): Tanh()\n",
      "    (linear2): Linear(in_features=1316, out_features=1, bias=True)\n",
      "    (softmax): Softmax(dim=1)\n",
      "  )\n",
      "  (readout): Sequential(\n",
      "    (0): Dropout(p=0.0, inplace=False)\n",
      "    (1): Linear(in_features=1616, out_features=300, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.0, inplace=False)\n",
      "    (4): Linear(in_features=300, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
      "Loading pretrained parameter \"seq_embedder.weight\".\n",
      "Loading pretrained parameter \"rotary_embedder.freqs\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_weight\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_bias\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.weight\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.bias\".\n",
      "Loading pretrained parameter \"readout.1.weight\".\n",
      "Loading pretrained parameter \"readout.1.bias\".\n",
      "Loading pretrained parameter \"readout.4.weight\".\n",
      "Loading pretrained parameter \"readout.4.bias\".\n",
      "Moving model to cuda\n",
      "Creating protein model\n",
      "MoleculeModel(\n",
      "  (softplus): Softplus(beta=1.0, threshold=20.0)\n",
      "  (encoder): MPN(\n",
      "    (encoder): ModuleList(\n",
      "      (0): MPNEncoder(\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (act_func): ReLU()\n",
      "        (W_i): Linear(in_features=147, out_features=300, bias=False)\n",
      "        (W_h): Linear(in_features=300, out_features=300, bias=False)\n",
      "        (W_o): Linear(in_features=433, out_features=300, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (seq_embedder): Embedding(21, 36, padding_idx=20)\n",
      "  (rotary_embedder): RotaryEmbedding()\n",
      "  (multihead_attn): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=36, out_features=36, bias=True)\n",
      "  )\n",
      "  (attentive_pooler): AttentivePooling(\n",
      "    (linear1): Linear(in_features=1316, out_features=1316, bias=True)\n",
      "    (tanh): Tanh()\n",
      "    (linear2): Linear(in_features=1316, out_features=1, bias=True)\n",
      "    (softmax): Softmax(dim=1)\n",
      "  )\n",
      "  (readout): Sequential(\n",
      "    (0): Dropout(p=0.0, inplace=False)\n",
      "    (1): Linear(in_features=1616, out_features=300, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.0, inplace=False)\n",
      "    (4): Linear(in_features=300, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
      "Loading pretrained parameter \"seq_embedder.weight\".\n",
      "Loading pretrained parameter \"rotary_embedder.freqs\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_weight\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_bias\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.weight\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.bias\".\n",
      "Loading pretrained parameter \"readout.1.weight\".\n",
      "Loading pretrained parameter \"readout.1.bias\".\n",
      "Loading pretrained parameter \"readout.4.weight\".\n",
      "Loading pretrained parameter \"readout.4.bias\".\n",
      "Moving model to cuda\n",
      "Creating protein model\n",
      "MoleculeModel(\n",
      "  (softplus): Softplus(beta=1.0, threshold=20.0)\n",
      "  (encoder): MPN(\n",
      "    (encoder): ModuleList(\n",
      "      (0): MPNEncoder(\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (act_func): ReLU()\n",
      "        (W_i): Linear(in_features=147, out_features=300, bias=False)\n",
      "        (W_h): Linear(in_features=300, out_features=300, bias=False)\n",
      "        (W_o): Linear(in_features=433, out_features=300, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (seq_embedder): Embedding(21, 36, padding_idx=20)\n",
      "  (rotary_embedder): RotaryEmbedding()\n",
      "  (multihead_attn): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=36, out_features=36, bias=True)\n",
      "  )\n",
      "  (attentive_pooler): AttentivePooling(\n",
      "    (linear1): Linear(in_features=1316, out_features=1316, bias=True)\n",
      "    (tanh): Tanh()\n",
      "    (linear2): Linear(in_features=1316, out_features=1, bias=True)\n",
      "    (softmax): Softmax(dim=1)\n",
      "  )\n",
      "  (readout): Sequential(\n",
      "    (0): Dropout(p=0.0, inplace=False)\n",
      "    (1): Linear(in_features=1616, out_features=300, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.0, inplace=False)\n",
      "    (4): Linear(in_features=300, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
      "Loading pretrained parameter \"seq_embedder.weight\".\n",
      "Loading pretrained parameter \"rotary_embedder.freqs\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_weight\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_bias\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.weight\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.bias\".\n",
      "Loading pretrained parameter \"readout.1.weight\".\n",
      "Loading pretrained parameter \"readout.1.bias\".\n",
      "Loading pretrained parameter \"readout.4.weight\".\n",
      "Loading pretrained parameter \"readout.4.bias\".\n",
      "Moving model to cuda\n",
      "Creating protein model\n",
      "MoleculeModel(\n",
      "  (softplus): Softplus(beta=1.0, threshold=20.0)\n",
      "  (encoder): MPN(\n",
      "    (encoder): ModuleList(\n",
      "      (0): MPNEncoder(\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (act_func): ReLU()\n",
      "        (W_i): Linear(in_features=147, out_features=300, bias=False)\n",
      "        (W_h): Linear(in_features=300, out_features=300, bias=False)\n",
      "        (W_o): Linear(in_features=433, out_features=300, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (seq_embedder): Embedding(21, 36, padding_idx=20)\n",
      "  (rotary_embedder): RotaryEmbedding()\n",
      "  (multihead_attn): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=36, out_features=36, bias=True)\n",
      "  )\n",
      "  (attentive_pooler): AttentivePooling(\n",
      "    (linear1): Linear(in_features=1316, out_features=1316, bias=True)\n",
      "    (tanh): Tanh()\n",
      "    (linear2): Linear(in_features=1316, out_features=1, bias=True)\n",
      "    (softmax): Softmax(dim=1)\n",
      "  )\n",
      "  (readout): Sequential(\n",
      "    (0): Dropout(p=0.0, inplace=False)\n",
      "    (1): Linear(in_features=1616, out_features=300, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.0, inplace=False)\n",
      "    (4): Linear(in_features=300, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
      "Loading pretrained parameter \"seq_embedder.weight\".\n",
      "Loading pretrained parameter \"rotary_embedder.freqs\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_weight\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_bias\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.weight\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.bias\".\n",
      "Loading pretrained parameter \"readout.1.weight\".\n",
      "Loading pretrained parameter \"readout.1.bias\".\n",
      "Loading pretrained parameter \"readout.4.weight\".\n",
      "Loading pretrained parameter \"readout.4.bias\".\n",
      "Moving model to cuda\n",
      "Creating protein model\n",
      "MoleculeModel(\n",
      "  (softplus): Softplus(beta=1.0, threshold=20.0)\n",
      "  (encoder): MPN(\n",
      "    (encoder): ModuleList(\n",
      "      (0): MPNEncoder(\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (act_func): ReLU()\n",
      "        (W_i): Linear(in_features=147, out_features=300, bias=False)\n",
      "        (W_h): Linear(in_features=300, out_features=300, bias=False)\n",
      "        (W_o): Linear(in_features=433, out_features=300, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (seq_embedder): Embedding(21, 36, padding_idx=20)\n",
      "  (rotary_embedder): RotaryEmbedding()\n",
      "  (multihead_attn): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=36, out_features=36, bias=True)\n",
      "  )\n",
      "  (attentive_pooler): AttentivePooling(\n",
      "    (linear1): Linear(in_features=1316, out_features=1316, bias=True)\n",
      "    (tanh): Tanh()\n",
      "    (linear2): Linear(in_features=1316, out_features=1, bias=True)\n",
      "    (softmax): Softmax(dim=1)\n",
      "  )\n",
      "  (readout): Sequential(\n",
      "    (0): Dropout(p=0.0, inplace=False)\n",
      "    (1): Linear(in_features=1616, out_features=300, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.0, inplace=False)\n",
      "    (4): Linear(in_features=300, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
      "Loading pretrained parameter \"seq_embedder.weight\".\n",
      "Loading pretrained parameter \"rotary_embedder.freqs\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_weight\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_bias\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.weight\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.bias\".\n",
      "Loading pretrained parameter \"readout.1.weight\".\n",
      "Loading pretrained parameter \"readout.1.bias\".\n",
      "Loading pretrained parameter \"readout.4.weight\".\n",
      "Loading pretrained parameter \"readout.4.bias\".\n",
      "Moving model to cuda\n",
      "Saving predictions to ../pipeline_entry_km_input_output.csv\n",
      "Elapsed time = 0:00:07\n",
      "Prediction completed.\n",
      "Output saved to results/ pipeline_entry_km_input_output.csv\n",
      "\n",
      "output |    |   Unnamed: 0.2 |   Unnamed: 0.1 |   Unnamed: 0 | substrate   | SMILES                                                                                                              | sequence                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 | pdbpath   |   log10kcat_max |   log10kcat_max_mve_uncal_var |   log10kcat_max_model_0 |   log10kcat_max_model_1 |   log10kcat_max_model_2 |   log10kcat_max_model_3 |   log10kcat_max_model_4 |   log10kcat_max_model_5 |   log10kcat_max_model_6 |   log10kcat_max_model_7 |   log10kcat_max_model_8 |   log10kcat_max_model_9 |   Prediction_(s^(-1)) |   Prediction_log10 |   SD_total |   SD_aleatoric |   SD_epistemic |\n",
      "|---:|---------------:|---------------:|-------------:|:------------|:--------------------------------------------------------------------------------------------------------------------|:-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:----------|----------------:|------------------------------:|------------------------:|------------------------:|------------------------:|------------------------:|------------------------:|------------------------:|------------------------:|------------------------:|------------------------:|------------------------:|----------------------:|-------------------:|-----------:|---------------:|---------------:|\n",
      "|  0 |              0 |              0 |            0 | Formate     | O=C[O-]                                                                                                             | MSELNEKLATAWEGFTKGDWQNEVNVRDFIQKNYTPYEGDESFLAGATEATTTLWDKVMEGVKLENRTHAPVDFDTAVASTITSHDAGYINKQLEKIVGLQTEAPLKRALIPFGGIKMIEGSCKAYNRELDPMIKKIFTEYRKTHNQGVFDVYTPDILRCRKSGVLTGLPDAYGRGRIIGDYRRVALYGIDYLMKDKLAQFTSLQADLENGVNLEQTIRLREEIAEQHRALGQMKEMAAKYGYDISGPATNAQEAIQWTYFGYLAAVKSQNGAAMSFGRTSTFLDVYIERDLKAGKITEQEAQEMVDHLVMKLRMVRFLRTPEYDELFSGDPIWATESIGGMGLDGRTLVTKNSFRFLNTLYTMGPSPEPNMTILWSEKLPLNFKKFAAKVSIDTSSLQYENDDLMRPDFNNDDYAIACCVSPMIVGKQMQFFGARANLAKTMLYAINGGVDEKLKMQVGPKSEPIKGDVLNYDEVMERMDHFMDWLAKQYITALNIIHYMHDKYSYEASLMALHDRDVIRTMACGIAGLSVAADSLSAIKYAKVKPIRDEDGLAIDFEIEGEYPQFGNNDPRVDDLAVDLVERFMKKIQKLHTYRDAIPTQSVLTITSNVVYGKKTGNTPDGRRAGAPFGPGANPMHGRDQKGAVASLTSVAKLPFAYAKDGISYTFSIVPNALGKDDEVRKTNLAGLMDGYFHHEASIEGGQHLNVNVMNREMLLDAMENPEKYPQLTIRVSGYAVRFNSLTKEQQQDVITRTFTQSM | seq1.seq  |        1.00241  |                      0.585397 |                1.85089  |                1.69428  |                1.34684  |                1.14037  |                0.955118 |                1.14302  |                1.0984   |                0.93708  |               0.776002  |               -0.917928 |             10.0556   |           1.00241  |   0.765112 |       0.272627 |       0.714892 |\n",
      "|  1 |              1 |              1 |            1 | Acetyl-CoA  | CC(=O)SCCNC(=O)CCNC(=O)[C@H](O)C(C)(C)COP(=O)(O)OP(=O)(O)OC[C@H]1O[C@@H](n2cnc3c(N)ncnc32)[C@H](O)[C@@H]1OP(=O)(O)O | MSELNEKLATAWEGFTKGDWQNEVNVRDFIQKNYTPYEGDESFLAGATEATTTLWDKVMEGVKLENRTHAPVDFDTAVASTITSHDAGYINKQLEKIVGLQTEAPLKRALIPFGGIKMIEGSCKAYNRELDPMIKKIFTEYRKTHNQGVFDVYTPDILRCRKSGVLTGLPDAYGRGRIIGDYRRVALYGIDYLMKDKLAQFTSLQADLENGVNLEQTIRLREEIAEQHRALGQMKEMAAKYGYDISGPATNAQEAIQWTYFGYLAAVKSQNGAAMSFGRTSTFLDVYIERDLKAGKITEQEAQEMVDHLVMKLRMVRFLRTPEYDELFSGDPIWATESIGGMGLDGRTLVTKNSFRFLNTLYTMGPSPEPNMTILWSEKLPLNFKKFAAKVSIDTSSLQYENDDLMRPDFNNDDYAIACCVSPMIVGKQMQFFGARANLAKTMLYAINGGVDEKLKMQVGPKSEPIKGDVLNYDEVMERMDHFMDWLAKQYITALNIIHYMHDKYSYEASLMALHDRDVIRTMACGIAGLSVAADSLSAIKYAKVKPIRDEDGLAIDFEIEGEYPQFGNNDPRVDDLAVDLVERFMKKIQKLHTYRDAIPTQSVLTITSNVVYGKKTGNTPDGRRAGAPFGPGANPMHGRDQKGAVASLTSVAKLPFAYAKDGISYTFSIVPNALGKDDEVRKTNLAGLMDGYFHHEASIEGGQHLNVNVMNREMLLDAMENPEKYPQLTIRVSGYAVRFNSLTKEQQQDVITRTFTQSM | seq1.seq  |        0.903243 |                      0.409418 |                1.72547  |                0.941468 |                0.770073 |                1.56705  |                0.672864 |                1.35592  |                0.729054 |                0.613962 |               0.734963  |               -0.078392 |              8.00282  |           0.903243 |   0.639858 |       0.399292 |       0.499984 |\n",
      "|  2 |              2 |              2 |            2 | Pyruvate    | CC(=O)C(=O)[O-]                                                                                                     | MSELNEKLATAWEGFTKGDWQNEVNVRDFIQKNYTPYEGDESFLAGATEATTTLWDKVMEGVKLENRTHAPVDFDTAVASTITSHDAGYINKQLEKIVGLQTEAPLKRALIPFGGIKMIEGSCKAYNRELDPMIKKIFTEYRKTHNQGVFDVYTPDILRCRKSGVLTGLPDAYGRGRIIGDYRRVALYGIDYLMKDKLAQFTSLQADLENGVNLEQTIRLREEIAEQHRALGQMKEMAAKYGYDISGPATNAQEAIQWTYFGYLAAVKSQNGAAMSFGRTSTFLDVYIERDLKAGKITEQEAQEMVDHLVMKLRMVRFLRTPEYDELFSGDPIWATESIGGMGLDGRTLVTKNSFRFLNTLYTMGPSPEPNMTILWSEKLPLNFKKFAAKVSIDTSSLQYENDDLMRPDFNNDDYAIACCVSPMIVGKQMQFFGARANLAKTMLYAINGGVDEKLKMQVGPKSEPIKGDVLNYDEVMERMDHFMDWLAKQYITALNIIHYMHDKYSYEASLMALHDRDVIRTMACGIAGLSVAADSLSAIKYAKVKPIRDEDGLAIDFEIEGEYPQFGNNDPRVDDLAVDLVERFMKKIQKLHTYRDAIPTQSVLTITSNVVYGKKTGNTPDGRRAGAPFGPGANPMHGRDQKGAVASLTSVAKLPFAYAKDGISYTFSIVPNALGKDDEVRKTNLAGLMDGYFHHEASIEGGQHLNVNVMNREMLLDAMENPEKYPQLTIRVSGYAVRFNSLTKEQQQDVITRTFTQSM | seq1.seq  |       -0.154988 |                      0.399596 |                0.809044 |                0.378633 |                0.198293 |                0.483808 |               -1.04224  |               -0.389791 |               -0.282906 |               -0.664143 |              -0.0800318 |               -0.960541 |              0.699861 |          -0.154988 |   0.632136 |       0.218286 |       0.593251 |\n",
      "|  3 |              3 |              3 |            3 | CoA         | CC(C)(COP(=O)(O)OP(=O)(O)OC[C@H]1O[C@@H](n2cnc3c(N)ncnc32)[C@H](O)[C@@H]1OP(=O)(O)O)[C@@H](O)C(=O)NCCC(=O)NCCS      | MSELNEKLATAWEGFTKGDWQNEVNVRDFIQKNYTPYEGDESFLAGATEATTTLWDKVMEGVKLENRTHAPVDFDTAVASTITSHDAGYINKQLEKIVGLQTEAPLKRALIPFGGIKMIEGSCKAYNRELDPMIKKIFTEYRKTHNQGVFDVYTPDILRCRKSGVLTGLPDAYGRGRIIGDYRRVALYGIDYLMKDKLAQFTSLQADLENGVNLEQTIRLREEIAEQHRALGQMKEMAAKYGYDISGPATNAQEAIQWTYFGYLAAVKSQNGAAMSFGRTSTFLDVYIERDLKAGKITEQEAQEMVDHLVMKLRMVRFLRTPEYDELFSGDPIWATESIGGMGLDGRTLVTKNSFRFLNTLYTMGPSPEPNMTILWSEKLPLNFKKFAAKVSIDTSSLQYENDDLMRPDFNNDDYAIACCVSPMIVGKQMQFFGARANLAKTMLYAINGGVDEKLKMQVGPKSEPIKGDVLNYDEVMERMDHFMDWLAKQYITALNIIHYMHDKYSYEASLMALHDRDVIRTMACGIAGLSVAADSLSAIKYAKVKPIRDEDGLAIDFEIEGEYPQFGNNDPRVDDLAVDLVERFMKKIQKLHTYRDAIPTQSVLTITSNVVYGKKTGNTPDGRRAGAPFGPGANPMHGRDQKGAVASLTSVAKLPFAYAKDGISYTFSIVPNALGKDDEVRKTNLAGLMDGYFHHEASIEGGQHLNVNVMNREMLLDAMENPEKYPQLTIRVSGYAVRFNSLTKEQQQDVITRTFTQSM | seq1.seq  |        0.910972 |                      0.404573 |                1.66919  |                0.97432  |                0.716412 |                1.59846  |                0.663085 |                1.42962  |                0.722047 |                0.700514 |               0.747979  |               -0.111914 |              8.14651  |           0.910972 |   0.63606  |       0.383801 |       0.507217 |\n",
      "SMILES ['C(=O)[O-]', 'CC(=O)SCCNC(=O)CCNC(=O)[C@@H](C(C)(C)COP(=O)(O)OP(=O)(O)OC[C@@H]1[C@H]([C@H]([C@@H](O1)N2C=NC3=C(N=CN=C32)N)O)OP(=O)(O)O)O', 'CC(=O)C(=O)[O-]', 'CC(C)(COP(=O)(O)OP(=O)(O)OC[C@@H]1[C@H]([C@H]([C@@H](O1)N2C=NC3=C(N=CN=C32)N)O)OP(=O)(O)O)[C@H](C(=O)NCCC(=O)NCCS)O']\n",
      "smiles_list C(=O)[O-] ['C(=O)[O-]']\n",
      "smiles_list CC(=O)SCCNC(=O)CCNC(=O)[C@@H](C(C)(C)COP(=O)(O)OP(=O)(O)OC[C@@H]1[C@H]([C@H]([C@@H](O1)N2C=NC3=C(N=CN=C32)N)O)OP(=O)(O)O)O ['C(=O)[O-]', 'CC(=O)SCCNC(=O)CCNC(=O)[C@@H](C(C)(C)COP(=O)(O)OP(=O)(O)OC[C@@H]1[C@H]([C@H]([C@@H](O1)N2C=NC3=C(N=CN=C32)N)O)OP(=O)(O)O)O']\n",
      "smiles_list CC(=O)C(=O)[O-] ['C(=O)[O-]', 'CC(=O)SCCNC(=O)CCNC(=O)[C@@H](C(C)(C)COP(=O)(O)OP(=O)(O)OC[C@@H]1[C@H]([C@H]([C@@H](O1)N2C=NC3=C(N=CN=C32)N)O)OP(=O)(O)O)O', 'CC(=O)C(=O)[O-]']\n",
      "smiles_list CC(C)(COP(=O)(O)OP(=O)(O)OC[C@@H]1[C@H]([C@H]([C@@H](O1)N2C=NC3=C(N=CN=C32)N)O)OP(=O)(O)O)[C@H](C(=O)NCCC(=O)NCCS)O ['C(=O)[O-]', 'CC(=O)SCCNC(=O)CCNC(=O)[C@@H](C(C)(C)COP(=O)(O)OP(=O)(O)OC[C@@H]1[C@H]([C@H]([C@@H](O1)N2C=NC3=C(N=CN=C32)N)O)OP(=O)(O)O)O', 'CC(=O)C(=O)[O-]', 'CC(C)(COP(=O)(O)OP(=O)(O)OC[C@@H]1[C@H]([C@H]([C@@H](O1)N2C=NC3=C(N=CN=C32)N)O)OP(=O)(O)O)[C@H](C(=O)NCCC(=O)NCCS)O']\n",
      "item {'substrates': ['Acetate', 'NADH', 'Acetaldehyde', 'NAD+'], 'SMILES': ['CC(=O)[O-]', 'C1C=CN(C=C1C(=O)N)[C@H]2[C@@H]([C@@H]([C@H](O2)COP(=O)(O)OP(=O)(O)OC[C@@H]3[C@H]([C@H]([C@@H](O3)N4C=NC5=C(N=CN=C54)N)O)O)O)O', 'CC=O', 'C1=CC(=C[N+](=C1)[C@H]2[C@@H]([C@@H]([C@H](O2)COP(=O)([O-])OP(=O)(O)OC[C@@H]3[C@H]([C@H]([C@@H](O3)N4C=NC5=C(N=CN=C54)N)O)O)O)O)C(=O)N'], 'sequences': ['MAENHDYEREINRLFELQKKNVVRLRTSSIDERIAKLKKLKEYIWENKEKIQEAVYNDLRKPPEEVLLTEIYPVVSEIRHVIKNLKKWTKPKKVRTPISLFGAKSYYRFEAKGVVLIISPWNYPFELSIGPLITAIAAGNAVVLKPSELSPHTSGYIKKLVADIFDESEVAVVEGDAVVAQKLLEMGFNHIFFTGSTKVAKAVLKKASETLSSVTLELGGKSPVIIDGKFDIEEAAKKITWGKYLNAGQTCIAPDYVFVKKELLGDFVSHLKHYIKKYYYSDGSGRCSNYCGIINERHFNRLKNVFEVTVKEGAKVCEGGLFVENECYISPTVLTDVGRDSYIMEEEIFGPILPVLTYEKIDDVIEYINSKPAPLVLYVFSRDRKFYRHVINNVISGDCLINDVIAHFANPRLPFGGHNASGIGKSHGYYGFREFSHLRSIMIQPKRTMLQLLYPPYGEFVKKLIEWSTKYF', 'MAENHDYEREINRLFELQKKNVVRLRTSSIDERIAKLKKLKEYIWENKEKIQEAVYNDLRKPPEEVLLTEIYPVVSEIRHVIKNLKKWTKPKKVRTPISLFGAKSYYRFEAKGVVLIISPWNYPFELSIGPLITAIAAGNAVVLKPSELSPHTSGYIKKLVADIFDESEVAVVEGDAVVAQKLLEMGFNHIFFTGSTKVAKAVLKKASETLSSVTLELGGKSPVIIDGKFDIEEAAKKITWGKYLNAGQTCIAPDYVFVKKELLGDFVSHLKHYIKKYYYSDGSGRCSNYCGIINERHFNRLKNVFEVTVKEGAKVCEGGLFVENECYISPTVLTDVGRDSYIMEEEIFGPILPVLTYEKIDDVIEYINSKPAPLVLYVFSRDRKFYRHVINNVISGDCLINDVIAHFANPRLPFGGHNASGIGKSHGYYGFREFSHLRSIMIQPKRTMLQLLYPPYGEFVKKLIEWSTKYF', 'MAENHDYEREINRLFELQKKNVVRLRTSSIDERIAKLKKLKEYIWENKEKIQEAVYNDLRKPPEEVLLTEIYPVVSEIRHVIKNLKKWTKPKKVRTPISLFGAKSYYRFEAKGVVLIISPWNYPFELSIGPLITAIAAGNAVVLKPSELSPHTSGYIKKLVADIFDESEVAVVEGDAVVAQKLLEMGFNHIFFTGSTKVAKAVLKKASETLSSVTLELGGKSPVIIDGKFDIEEAAKKITWGKYLNAGQTCIAPDYVFVKKELLGDFVSHLKHYIKKYYYSDGSGRCSNYCGIINERHFNRLKNVFEVTVKEGAKVCEGGLFVENECYISPTVLTDVGRDSYIMEEEIFGPILPVLTYEKIDDVIEYINSKPAPLVLYVFSRDRKFYRHVINNVISGDCLINDVIAHFANPRLPFGGHNASGIGKSHGYYGFREFSHLRSIMIQPKRTMLQLLYPPYGEFVKKLIEWSTKYF', 'MAENHDYEREINRLFELQKKNVVRLRTSSIDERIAKLKKLKEYIWENKEKIQEAVYNDLRKPPEEVLLTEIYPVVSEIRHVIKNLKKWTKPKKVRTPISLFGAKSYYRFEAKGVVLIISPWNYPFELSIGPLITAIAAGNAVVLKPSELSPHTSGYIKKLVADIFDESEVAVVEGDAVVAQKLLEMGFNHIFFTGSTKVAKAVLKKASETLSSVTLELGGKSPVIIDGKFDIEEAAKKITWGKYLNAGQTCIAPDYVFVKKELLGDFVSHLKHYIKKYYYSDGSGRCSNYCGIINERHFNRLKNVFEVTVKEGAKVCEGGLFVENECYISPTVLTDVGRDSYIMEEEIFGPILPVLTYEKIDDVIEYINSKPAPLVLYVFSRDRKFYRHVINNVISGDCLINDVIAHFANPRLPFGGHNASGIGKSHGYYGFREFSHLRSIMIQPKRTMLQLLYPPYGEFVKKLIEWSTKYF'], 'pdb_paths': ['seq2.seq', 'seq2.seq', 'seq2.seq', 'seq2.seq'], 'subs': ['CC(=O)[O-]', 'C1C=CN(C=C1C(=O)N)[C@H]2[C@@H]([C@@H]([C@H](O2)COP(=O)(O)OP(=O)(O)OC[C@@H]3[C@H]([C@H]([C@@H](O3)N4C=NC5=C(N=CN=C54)N)O)O)O)O'], 'stoich': [1, 1, 1, 1], 'EC': '1.2.1.3'}\n",
      "starting R00711\n",
      "input |    | substrate    | SMILES                                                                                                                                 | sequence                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 | pdbpath   |\n",
      "|---:|:-------------|:---------------------------------------------------------------------------------------------------------------------------------------|:-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:----------|\n",
      "|  0 | Acetate      | CC(=O)[O-]                                                                                                                             | MAENHDYEREINRLFELQKKNVVRLRTSSIDERIAKLKKLKEYIWENKEKIQEAVYNDLRKPPEEVLLTEIYPVVSEIRHVIKNLKKWTKPKKVRTPISLFGAKSYYRFEAKGVVLIISPWNYPFELSIGPLITAIAAGNAVVLKPSELSPHTSGYIKKLVADIFDESEVAVVEGDAVVAQKLLEMGFNHIFFTGSTKVAKAVLKKASETLSSVTLELGGKSPVIIDGKFDIEEAAKKITWGKYLNAGQTCIAPDYVFVKKELLGDFVSHLKHYIKKYYYSDGSGRCSNYCGIINERHFNRLKNVFEVTVKEGAKVCEGGLFVENECYISPTVLTDVGRDSYIMEEEIFGPILPVLTYEKIDDVIEYINSKPAPLVLYVFSRDRKFYRHVINNVISGDCLINDVIAHFANPRLPFGGHNASGIGKSHGYYGFREFSHLRSIMIQPKRTMLQLLYPPYGEFVKKLIEWSTKYF | seq2.seq  |\n",
      "|  1 | NADH         | C1C=CN(C=C1C(=O)N)[C@H]2[C@@H]([C@@H]([C@H](O2)COP(=O)(O)OP(=O)(O)OC[C@@H]3[C@H]([C@H]([C@@H](O3)N4C=NC5=C(N=CN=C54)N)O)O)O)O          | MAENHDYEREINRLFELQKKNVVRLRTSSIDERIAKLKKLKEYIWENKEKIQEAVYNDLRKPPEEVLLTEIYPVVSEIRHVIKNLKKWTKPKKVRTPISLFGAKSYYRFEAKGVVLIISPWNYPFELSIGPLITAIAAGNAVVLKPSELSPHTSGYIKKLVADIFDESEVAVVEGDAVVAQKLLEMGFNHIFFTGSTKVAKAVLKKASETLSSVTLELGGKSPVIIDGKFDIEEAAKKITWGKYLNAGQTCIAPDYVFVKKELLGDFVSHLKHYIKKYYYSDGSGRCSNYCGIINERHFNRLKNVFEVTVKEGAKVCEGGLFVENECYISPTVLTDVGRDSYIMEEEIFGPILPVLTYEKIDDVIEYINSKPAPLVLYVFSRDRKFYRHVINNVISGDCLINDVIAHFANPRLPFGGHNASGIGKSHGYYGFREFSHLRSIMIQPKRTMLQLLYPPYGEFVKKLIEWSTKYF | seq2.seq  |\n",
      "|  2 | Acetaldehyde | CC=O                                                                                                                                   | MAENHDYEREINRLFELQKKNVVRLRTSSIDERIAKLKKLKEYIWENKEKIQEAVYNDLRKPPEEVLLTEIYPVVSEIRHVIKNLKKWTKPKKVRTPISLFGAKSYYRFEAKGVVLIISPWNYPFELSIGPLITAIAAGNAVVLKPSELSPHTSGYIKKLVADIFDESEVAVVEGDAVVAQKLLEMGFNHIFFTGSTKVAKAVLKKASETLSSVTLELGGKSPVIIDGKFDIEEAAKKITWGKYLNAGQTCIAPDYVFVKKELLGDFVSHLKHYIKKYYYSDGSGRCSNYCGIINERHFNRLKNVFEVTVKEGAKVCEGGLFVENECYISPTVLTDVGRDSYIMEEEIFGPILPVLTYEKIDDVIEYINSKPAPLVLYVFSRDRKFYRHVINNVISGDCLINDVIAHFANPRLPFGGHNASGIGKSHGYYGFREFSHLRSIMIQPKRTMLQLLYPPYGEFVKKLIEWSTKYF | seq2.seq  |\n",
      "|  3 | NAD+         | C1=CC(=C[N+](=C1)[C@H]2[C@@H]([C@@H]([C@H](O2)COP(=O)([O-])OP(=O)(O)OC[C@@H]3[C@H]([C@H]([C@@H](O3)N4C=NC5=C(N=CN=C54)N)O)O)O)O)C(=O)N | MAENHDYEREINRLFELQKKNVVRLRTSSIDERIAKLKKLKEYIWENKEKIQEAVYNDLRKPPEEVLLTEIYPVVSEIRHVIKNLKKWTKPKKVRTPISLFGAKSYYRFEAKGVVLIISPWNYPFELSIGPLITAIAAGNAVVLKPSELSPHTSGYIKKLVADIFDESEVAVVEGDAVVAQKLLEMGFNHIFFTGSTKVAKAVLKKASETLSSVTLELGGKSPVIIDGKFDIEEAAKKITWGKYLNAGQTCIAPDYVFVKKELLGDFVSHLKHYIKKYYYSDGSGRCSNYCGIINERHFNRLKNVFEVTVKEGAKVCEGGLFVENECYISPTVLTDVGRDSYIMEEEIFGPILPVLTYEKIDDVIEYINSKPAPLVLYVFSRDRKFYRHVINNVISGDCLINDVIAHFANPRLPFGGHNASGIGKSHGYYGFREFSHLRSIMIQPKRTMLQLLYPPYGEFVKKLIEWSTKYF | seq2.seq  |\n",
      "input2 ['CC(=O)[O-]', 'C1C=CN(C=C1C(=O)N)[C@H]2[C@@H]([C@@H]([C@H](O2)COP(=O)(O)OP(=O)(O)OC[C@@H]3[C@H]([C@H]([C@@H](O3)N4C=NC5=C(N=CN=C54)N)O)O)O)O', 'CC=O', 'C1=CC(=C[N+](=C1)[C@H]2[C@@H]([C@@H]([C@H](O2)COP(=O)([O-])OP(=O)(O)OC[C@@H]3[C@H]([C@H]([C@@H](O3)N4C=NC5=C(N=CN=C54)N)O)O)O)O)C(=O)N']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xuvin/.conda/envs/catpred/lib/python3.9/site-packages/rotary_embedding_torch/rotary_embedding_torch.py:35: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  @autocast(enabled = False)\n",
      "/home/xuvin/.conda/envs/catpred/lib/python3.9/site-packages/rotary_embedding_torch/rotary_embedding_torch.py:268: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  @autocast(enabled = False)\n",
      "\n",
      "0it [00:00, ?it/s]/home/xuvin/py_repos/wccm/src/catpred_pipeline/catpred/catpred/data/cache_utils.py:95: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(str(entry_path))\n",
      "\n",
      "4it [00:00, 37.26it/s]\n",
      "4it [00:00, 36.54it/s]\n",
      "\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 30559.59it/s]\n",
      "\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 2019.40it/s]\n",
      "\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]/home/xuvin/py_repos/wccm/src/catpred_pipeline/catpred/catpred/utils.py:113: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(path, map_location=lambda storage, loc: storage)\n",
      "/home/xuvin/py_repos/wccm/src/catpred_pipeline/catpred/catpred/utils.py:446: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(path, map_location=lambda storage, loc: storage)\n",
      "\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.43s/it]\u001b[A\n",
      "\n",
      "                                             \u001b[A\n",
      " 10%|█         | 1/10 [00:04<00:40,  4.46s/it]/home/xuvin/py_repos/wccm/src/catpred_pipeline/catpred/catpred/utils.py:113: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(path, map_location=lambda storage, loc: storage)\n",
      "/home/xuvin/py_repos/wccm/src/catpred_pipeline/catpred/catpred/utils.py:446: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(path, map_location=lambda storage, loc: storage)\n",
      "\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "                                     \u001b[A\n",
      " 20%|██        | 2/10 [00:04<00:15,  1.98s/it]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "                                     \u001b[A\n",
      " 30%|███       | 3/10 [00:05<00:09,  1.29s/it]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "                                     \u001b[A\n",
      " 40%|████      | 4/10 [00:05<00:05,  1.14it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "                                     \u001b[A\n",
      " 50%|█████     | 5/10 [00:05<00:03,  1.56it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "                                     \u001b[A\n",
      " 60%|██████    | 6/10 [00:05<00:02,  1.96it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "                                     \u001b[A\n",
      " 70%|███████   | 7/10 [00:06<00:01,  2.04it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "                                     \u001b[A\n",
      " 80%|████████  | 8/10 [00:06<00:00,  2.41it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "                                     \u001b[A\n",
      " 90%|█████████ | 9/10 [00:06<00:00,  2.77it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "                                     \u001b[A\n",
      "100%|██████████| 10/10 [00:07<00:00,  3.21it/s]\n",
      "100%|██████████| 10/10 [00:07<00:00,  1.42it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/xuvin/py_repos/wccm/src/catpred_pipeline/catpred\n",
      "Predicting.. This will take a while..\n",
      "calculating protein embed only on cpu\n",
      "Loading training args\n",
      "Loading models\n",
      "Setting molecule featurization parameters to default.\n",
      "Loading data\n",
      "Validating SMILES\n",
      "Test size = 4\n",
      "Creating protein model\n",
      "MoleculeModel(\n",
      "  (softplus): Softplus(beta=1.0, threshold=20.0)\n",
      "  (encoder): MPN(\n",
      "    (encoder): ModuleList(\n",
      "      (0): MPNEncoder(\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (act_func): ReLU()\n",
      "        (W_i): Linear(in_features=147, out_features=300, bias=False)\n",
      "        (W_h): Linear(in_features=300, out_features=300, bias=False)\n",
      "        (W_o): Linear(in_features=433, out_features=300, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (seq_embedder): Embedding(21, 36, padding_idx=20)\n",
      "  (rotary_embedder): RotaryEmbedding()\n",
      "  (multihead_attn): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=36, out_features=36, bias=True)\n",
      "  )\n",
      "  (attentive_pooler): AttentivePooling(\n",
      "    (linear1): Linear(in_features=1316, out_features=1316, bias=True)\n",
      "    (tanh): Tanh()\n",
      "    (linear2): Linear(in_features=1316, out_features=1, bias=True)\n",
      "    (softmax): Softmax(dim=1)\n",
      "  )\n",
      "  (readout): Sequential(\n",
      "    (0): Dropout(p=0.0, inplace=False)\n",
      "    (1): Linear(in_features=1616, out_features=300, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.0, inplace=False)\n",
      "    (4): Linear(in_features=300, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
      "Loading pretrained parameter \"seq_embedder.weight\".\n",
      "Loading pretrained parameter \"rotary_embedder.freqs\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_weight\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_bias\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.weight\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.bias\".\n",
      "Loading pretrained parameter \"readout.1.weight\".\n",
      "Loading pretrained parameter \"readout.1.bias\".\n",
      "Loading pretrained parameter \"readout.4.weight\".\n",
      "Loading pretrained parameter \"readout.4.bias\".\n",
      "Moving model to cuda\n",
      "Creating protein model\n",
      "MoleculeModel(\n",
      "  (softplus): Softplus(beta=1.0, threshold=20.0)\n",
      "  (encoder): MPN(\n",
      "    (encoder): ModuleList(\n",
      "      (0): MPNEncoder(\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (act_func): ReLU()\n",
      "        (W_i): Linear(in_features=147, out_features=300, bias=False)\n",
      "        (W_h): Linear(in_features=300, out_features=300, bias=False)\n",
      "        (W_o): Linear(in_features=433, out_features=300, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (seq_embedder): Embedding(21, 36, padding_idx=20)\n",
      "  (rotary_embedder): RotaryEmbedding()\n",
      "  (multihead_attn): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=36, out_features=36, bias=True)\n",
      "  )\n",
      "  (attentive_pooler): AttentivePooling(\n",
      "    (linear1): Linear(in_features=1316, out_features=1316, bias=True)\n",
      "    (tanh): Tanh()\n",
      "    (linear2): Linear(in_features=1316, out_features=1, bias=True)\n",
      "    (softmax): Softmax(dim=1)\n",
      "  )\n",
      "  (readout): Sequential(\n",
      "    (0): Dropout(p=0.0, inplace=False)\n",
      "    (1): Linear(in_features=1616, out_features=300, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.0, inplace=False)\n",
      "    (4): Linear(in_features=300, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
      "Loading pretrained parameter \"seq_embedder.weight\".\n",
      "Loading pretrained parameter \"rotary_embedder.freqs\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_weight\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_bias\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.weight\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.bias\".\n",
      "Loading pretrained parameter \"readout.1.weight\".\n",
      "Loading pretrained parameter \"readout.1.bias\".\n",
      "Loading pretrained parameter \"readout.4.weight\".\n",
      "Loading pretrained parameter \"readout.4.bias\".\n",
      "Moving model to cuda\n",
      "Creating protein model\n",
      "MoleculeModel(\n",
      "  (softplus): Softplus(beta=1.0, threshold=20.0)\n",
      "  (encoder): MPN(\n",
      "    (encoder): ModuleList(\n",
      "      (0): MPNEncoder(\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (act_func): ReLU()\n",
      "        (W_i): Linear(in_features=147, out_features=300, bias=False)\n",
      "        (W_h): Linear(in_features=300, out_features=300, bias=False)\n",
      "        (W_o): Linear(in_features=433, out_features=300, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (seq_embedder): Embedding(21, 36, padding_idx=20)\n",
      "  (rotary_embedder): RotaryEmbedding()\n",
      "  (multihead_attn): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=36, out_features=36, bias=True)\n",
      "  )\n",
      "  (attentive_pooler): AttentivePooling(\n",
      "    (linear1): Linear(in_features=1316, out_features=1316, bias=True)\n",
      "    (tanh): Tanh()\n",
      "    (linear2): Linear(in_features=1316, out_features=1, bias=True)\n",
      "    (softmax): Softmax(dim=1)\n",
      "  )\n",
      "  (readout): Sequential(\n",
      "    (0): Dropout(p=0.0, inplace=False)\n",
      "    (1): Linear(in_features=1616, out_features=300, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.0, inplace=False)\n",
      "    (4): Linear(in_features=300, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
      "Loading pretrained parameter \"seq_embedder.weight\".\n",
      "Loading pretrained parameter \"rotary_embedder.freqs\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_weight\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_bias\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.weight\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.bias\".\n",
      "Loading pretrained parameter \"readout.1.weight\".\n",
      "Loading pretrained parameter \"readout.1.bias\".\n",
      "Loading pretrained parameter \"readout.4.weight\".\n",
      "Loading pretrained parameter \"readout.4.bias\".\n",
      "Moving model to cuda\n",
      "Creating protein model\n",
      "MoleculeModel(\n",
      "  (softplus): Softplus(beta=1.0, threshold=20.0)\n",
      "  (encoder): MPN(\n",
      "    (encoder): ModuleList(\n",
      "      (0): MPNEncoder(\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (act_func): ReLU()\n",
      "        (W_i): Linear(in_features=147, out_features=300, bias=False)\n",
      "        (W_h): Linear(in_features=300, out_features=300, bias=False)\n",
      "        (W_o): Linear(in_features=433, out_features=300, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (seq_embedder): Embedding(21, 36, padding_idx=20)\n",
      "  (rotary_embedder): RotaryEmbedding()\n",
      "  (multihead_attn): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=36, out_features=36, bias=True)\n",
      "  )\n",
      "  (attentive_pooler): AttentivePooling(\n",
      "    (linear1): Linear(in_features=1316, out_features=1316, bias=True)\n",
      "    (tanh): Tanh()\n",
      "    (linear2): Linear(in_features=1316, out_features=1, bias=True)\n",
      "    (softmax): Softmax(dim=1)\n",
      "  )\n",
      "  (readout): Sequential(\n",
      "    (0): Dropout(p=0.0, inplace=False)\n",
      "    (1): Linear(in_features=1616, out_features=300, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.0, inplace=False)\n",
      "    (4): Linear(in_features=300, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
      "Loading pretrained parameter \"seq_embedder.weight\".\n",
      "Loading pretrained parameter \"rotary_embedder.freqs\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_weight\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_bias\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.weight\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.bias\".\n",
      "Loading pretrained parameter \"readout.1.weight\".\n",
      "Loading pretrained parameter \"readout.1.bias\".\n",
      "Loading pretrained parameter \"readout.4.weight\".\n",
      "Loading pretrained parameter \"readout.4.bias\".\n",
      "Moving model to cuda\n",
      "Creating protein model\n",
      "MoleculeModel(\n",
      "  (softplus): Softplus(beta=1.0, threshold=20.0)\n",
      "  (encoder): MPN(\n",
      "    (encoder): ModuleList(\n",
      "      (0): MPNEncoder(\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (act_func): ReLU()\n",
      "        (W_i): Linear(in_features=147, out_features=300, bias=False)\n",
      "        (W_h): Linear(in_features=300, out_features=300, bias=False)\n",
      "        (W_o): Linear(in_features=433, out_features=300, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (seq_embedder): Embedding(21, 36, padding_idx=20)\n",
      "  (rotary_embedder): RotaryEmbedding()\n",
      "  (multihead_attn): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=36, out_features=36, bias=True)\n",
      "  )\n",
      "  (attentive_pooler): AttentivePooling(\n",
      "    (linear1): Linear(in_features=1316, out_features=1316, bias=True)\n",
      "    (tanh): Tanh()\n",
      "    (linear2): Linear(in_features=1316, out_features=1, bias=True)\n",
      "    (softmax): Softmax(dim=1)\n",
      "  )\n",
      "  (readout): Sequential(\n",
      "    (0): Dropout(p=0.0, inplace=False)\n",
      "    (1): Linear(in_features=1616, out_features=300, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.0, inplace=False)\n",
      "    (4): Linear(in_features=300, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
      "Loading pretrained parameter \"seq_embedder.weight\".\n",
      "Loading pretrained parameter \"rotary_embedder.freqs\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_weight\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_bias\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.weight\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.bias\".\n",
      "Loading pretrained parameter \"readout.1.weight\".\n",
      "Loading pretrained parameter \"readout.1.bias\".\n",
      "Loading pretrained parameter \"readout.4.weight\".\n",
      "Loading pretrained parameter \"readout.4.bias\".\n",
      "Moving model to cuda\n",
      "Creating protein model\n",
      "MoleculeModel(\n",
      "  (softplus): Softplus(beta=1.0, threshold=20.0)\n",
      "  (encoder): MPN(\n",
      "    (encoder): ModuleList(\n",
      "      (0): MPNEncoder(\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (act_func): ReLU()\n",
      "        (W_i): Linear(in_features=147, out_features=300, bias=False)\n",
      "        (W_h): Linear(in_features=300, out_features=300, bias=False)\n",
      "        (W_o): Linear(in_features=433, out_features=300, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (seq_embedder): Embedding(21, 36, padding_idx=20)\n",
      "  (rotary_embedder): RotaryEmbedding()\n",
      "  (multihead_attn): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=36, out_features=36, bias=True)\n",
      "  )\n",
      "  (attentive_pooler): AttentivePooling(\n",
      "    (linear1): Linear(in_features=1316, out_features=1316, bias=True)\n",
      "    (tanh): Tanh()\n",
      "    (linear2): Linear(in_features=1316, out_features=1, bias=True)\n",
      "    (softmax): Softmax(dim=1)\n",
      "  )\n",
      "  (readout): Sequential(\n",
      "    (0): Dropout(p=0.0, inplace=False)\n",
      "    (1): Linear(in_features=1616, out_features=300, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.0, inplace=False)\n",
      "    (4): Linear(in_features=300, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
      "Loading pretrained parameter \"seq_embedder.weight\".\n",
      "Loading pretrained parameter \"rotary_embedder.freqs\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_weight\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_bias\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.weight\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.bias\".\n",
      "Loading pretrained parameter \"readout.1.weight\".\n",
      "Loading pretrained parameter \"readout.1.bias\".\n",
      "Loading pretrained parameter \"readout.4.weight\".\n",
      "Loading pretrained parameter \"readout.4.bias\".\n",
      "Moving model to cuda\n",
      "Creating protein model\n",
      "MoleculeModel(\n",
      "  (softplus): Softplus(beta=1.0, threshold=20.0)\n",
      "  (encoder): MPN(\n",
      "    (encoder): ModuleList(\n",
      "      (0): MPNEncoder(\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (act_func): ReLU()\n",
      "        (W_i): Linear(in_features=147, out_features=300, bias=False)\n",
      "        (W_h): Linear(in_features=300, out_features=300, bias=False)\n",
      "        (W_o): Linear(in_features=433, out_features=300, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (seq_embedder): Embedding(21, 36, padding_idx=20)\n",
      "  (rotary_embedder): RotaryEmbedding()\n",
      "  (multihead_attn): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=36, out_features=36, bias=True)\n",
      "  )\n",
      "  (attentive_pooler): AttentivePooling(\n",
      "    (linear1): Linear(in_features=1316, out_features=1316, bias=True)\n",
      "    (tanh): Tanh()\n",
      "    (linear2): Linear(in_features=1316, out_features=1, bias=True)\n",
      "    (softmax): Softmax(dim=1)\n",
      "  )\n",
      "  (readout): Sequential(\n",
      "    (0): Dropout(p=0.0, inplace=False)\n",
      "    (1): Linear(in_features=1616, out_features=300, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.0, inplace=False)\n",
      "    (4): Linear(in_features=300, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
      "Loading pretrained parameter \"seq_embedder.weight\".\n",
      "Loading pretrained parameter \"rotary_embedder.freqs\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_weight\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_bias\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.weight\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.bias\".\n",
      "Loading pretrained parameter \"readout.1.weight\".\n",
      "Loading pretrained parameter \"readout.1.bias\".\n",
      "Loading pretrained parameter \"readout.4.weight\".\n",
      "Loading pretrained parameter \"readout.4.bias\".\n",
      "Moving model to cuda\n",
      "Creating protein model\n",
      "MoleculeModel(\n",
      "  (softplus): Softplus(beta=1.0, threshold=20.0)\n",
      "  (encoder): MPN(\n",
      "    (encoder): ModuleList(\n",
      "      (0): MPNEncoder(\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (act_func): ReLU()\n",
      "        (W_i): Linear(in_features=147, out_features=300, bias=False)\n",
      "        (W_h): Linear(in_features=300, out_features=300, bias=False)\n",
      "        (W_o): Linear(in_features=433, out_features=300, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (seq_embedder): Embedding(21, 36, padding_idx=20)\n",
      "  (rotary_embedder): RotaryEmbedding()\n",
      "  (multihead_attn): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=36, out_features=36, bias=True)\n",
      "  )\n",
      "  (attentive_pooler): AttentivePooling(\n",
      "    (linear1): Linear(in_features=1316, out_features=1316, bias=True)\n",
      "    (tanh): Tanh()\n",
      "    (linear2): Linear(in_features=1316, out_features=1, bias=True)\n",
      "    (softmax): Softmax(dim=1)\n",
      "  )\n",
      "  (readout): Sequential(\n",
      "    (0): Dropout(p=0.0, inplace=False)\n",
      "    (1): Linear(in_features=1616, out_features=300, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.0, inplace=False)\n",
      "    (4): Linear(in_features=300, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
      "Loading pretrained parameter \"seq_embedder.weight\".\n",
      "Loading pretrained parameter \"rotary_embedder.freqs\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_weight\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_bias\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.weight\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.bias\".\n",
      "Loading pretrained parameter \"readout.1.weight\".\n",
      "Loading pretrained parameter \"readout.1.bias\".\n",
      "Loading pretrained parameter \"readout.4.weight\".\n",
      "Loading pretrained parameter \"readout.4.bias\".\n",
      "Moving model to cuda\n",
      "Creating protein model\n",
      "MoleculeModel(\n",
      "  (softplus): Softplus(beta=1.0, threshold=20.0)\n",
      "  (encoder): MPN(\n",
      "    (encoder): ModuleList(\n",
      "      (0): MPNEncoder(\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (act_func): ReLU()\n",
      "        (W_i): Linear(in_features=147, out_features=300, bias=False)\n",
      "        (W_h): Linear(in_features=300, out_features=300, bias=False)\n",
      "        (W_o): Linear(in_features=433, out_features=300, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (seq_embedder): Embedding(21, 36, padding_idx=20)\n",
      "  (rotary_embedder): RotaryEmbedding()\n",
      "  (multihead_attn): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=36, out_features=36, bias=True)\n",
      "  )\n",
      "  (attentive_pooler): AttentivePooling(\n",
      "    (linear1): Linear(in_features=1316, out_features=1316, bias=True)\n",
      "    (tanh): Tanh()\n",
      "    (linear2): Linear(in_features=1316, out_features=1, bias=True)\n",
      "    (softmax): Softmax(dim=1)\n",
      "  )\n",
      "  (readout): Sequential(\n",
      "    (0): Dropout(p=0.0, inplace=False)\n",
      "    (1): Linear(in_features=1616, out_features=300, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.0, inplace=False)\n",
      "    (4): Linear(in_features=300, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
      "Loading pretrained parameter \"seq_embedder.weight\".\n",
      "Loading pretrained parameter \"rotary_embedder.freqs\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_weight\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_bias\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.weight\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.bias\".\n",
      "Loading pretrained parameter \"readout.1.weight\".\n",
      "Loading pretrained parameter \"readout.1.bias\".\n",
      "Loading pretrained parameter \"readout.4.weight\".\n",
      "Loading pretrained parameter \"readout.4.bias\".\n",
      "Moving model to cuda\n",
      "Creating protein model\n",
      "MoleculeModel(\n",
      "  (softplus): Softplus(beta=1.0, threshold=20.0)\n",
      "  (encoder): MPN(\n",
      "    (encoder): ModuleList(\n",
      "      (0): MPNEncoder(\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (act_func): ReLU()\n",
      "        (W_i): Linear(in_features=147, out_features=300, bias=False)\n",
      "        (W_h): Linear(in_features=300, out_features=300, bias=False)\n",
      "        (W_o): Linear(in_features=433, out_features=300, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (seq_embedder): Embedding(21, 36, padding_idx=20)\n",
      "  (rotary_embedder): RotaryEmbedding()\n",
      "  (multihead_attn): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=36, out_features=36, bias=True)\n",
      "  )\n",
      "  (attentive_pooler): AttentivePooling(\n",
      "    (linear1): Linear(in_features=1316, out_features=1316, bias=True)\n",
      "    (tanh): Tanh()\n",
      "    (linear2): Linear(in_features=1316, out_features=1, bias=True)\n",
      "    (softmax): Softmax(dim=1)\n",
      "  )\n",
      "  (readout): Sequential(\n",
      "    (0): Dropout(p=0.0, inplace=False)\n",
      "    (1): Linear(in_features=1616, out_features=300, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.0, inplace=False)\n",
      "    (4): Linear(in_features=300, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
      "Loading pretrained parameter \"seq_embedder.weight\".\n",
      "Loading pretrained parameter \"rotary_embedder.freqs\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_weight\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_bias\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.weight\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.bias\".\n",
      "Loading pretrained parameter \"readout.1.weight\".\n",
      "Loading pretrained parameter \"readout.1.bias\".\n",
      "Loading pretrained parameter \"readout.4.weight\".\n",
      "Loading pretrained parameter \"readout.4.bias\".\n",
      "Moving model to cuda\n",
      "Saving predictions to ../pipeline_entry_kcat_input_output.csv\n",
      "Elapsed time = 0:00:07\n",
      "Prediction completed.\n",
      "Output saved to results/ pipeline_entry_kcat_input_output.csv\n",
      "\n",
      "/home/xuvin/py_repos/wccm/src/catpred_pipeline/catpred\n",
      "Predicting.. This will take a while..\n",
      "calculating protein embed only on cpu\n",
      "Loading training args\n",
      "Loading models\n",
      "Setting molecule featurization parameters to default.\n",
      "Loading data\n",
      "Validating SMILES\n",
      "Test size = 4\n",
      "Creating protein model\n",
      "MoleculeModel(\n",
      "  (softplus): Softplus(beta=1.0, threshold=20.0)\n",
      "  (encoder): MPN(\n",
      "    (encoder): ModuleList(\n",
      "      (0): MPNEncoder(\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (act_func): ReLU()\n",
      "        (W_i): Linear(in_features=147, out_features=300, bias=False)\n",
      "        (W_h): Linear(in_features=300, out_features=300, bias=False)\n",
      "        (W_o): Linear(in_features=433, out_features=300, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (seq_embedder): Embedding(21, 36, padding_idx=20)\n",
      "  (rotary_embedder): RotaryEmbedding()\n",
      "  (multihead_attn): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=36, out_features=36, bias=True)\n",
      "  )\n",
      "  (attentive_pooler): AttentivePooling(\n",
      "    (linear1): Linear(in_features=1316, out_features=1316, bias=True)\n",
      "    (tanh): Tanh()\n",
      "    (linear2): Linear(in_features=1316, out_features=1, bias=True)\n",
      "    (softmax): Softmax(dim=1)\n",
      "  )\n",
      "  (readout): Sequential(\n",
      "    (0): Dropout(p=0.0, inplace=False)\n",
      "    (1): Linear(in_features=1616, out_features=300, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.0, inplace=False)\n",
      "    (4): Linear(in_features=300, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
      "Loading pretrained parameter \"seq_embedder.weight\".\n",
      "Loading pretrained parameter \"rotary_embedder.freqs\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_weight\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_bias\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.weight\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.bias\".\n",
      "Loading pretrained parameter \"readout.1.weight\".\n",
      "Loading pretrained parameter \"readout.1.bias\".\n",
      "Loading pretrained parameter \"readout.4.weight\".\n",
      "Loading pretrained parameter \"readout.4.bias\".\n",
      "Moving model to cuda\n",
      "Creating protein model\n",
      "MoleculeModel(\n",
      "  (softplus): Softplus(beta=1.0, threshold=20.0)\n",
      "  (encoder): MPN(\n",
      "    (encoder): ModuleList(\n",
      "      (0): MPNEncoder(\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (act_func): ReLU()\n",
      "        (W_i): Linear(in_features=147, out_features=300, bias=False)\n",
      "        (W_h): Linear(in_features=300, out_features=300, bias=False)\n",
      "        (W_o): Linear(in_features=433, out_features=300, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (seq_embedder): Embedding(21, 36, padding_idx=20)\n",
      "  (rotary_embedder): RotaryEmbedding()\n",
      "  (multihead_attn): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=36, out_features=36, bias=True)\n",
      "  )\n",
      "  (attentive_pooler): AttentivePooling(\n",
      "    (linear1): Linear(in_features=1316, out_features=1316, bias=True)\n",
      "    (tanh): Tanh()\n",
      "    (linear2): Linear(in_features=1316, out_features=1, bias=True)\n",
      "    (softmax): Softmax(dim=1)\n",
      "  )\n",
      "  (readout): Sequential(\n",
      "    (0): Dropout(p=0.0, inplace=False)\n",
      "    (1): Linear(in_features=1616, out_features=300, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.0, inplace=False)\n",
      "    (4): Linear(in_features=300, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
      "Loading pretrained parameter \"seq_embedder.weight\".\n",
      "Loading pretrained parameter \"rotary_embedder.freqs\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_weight\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_bias\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.weight\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.bias\".\n",
      "Loading pretrained parameter \"readout.1.weight\".\n",
      "Loading pretrained parameter \"readout.1.bias\".\n",
      "Loading pretrained parameter \"readout.4.weight\".\n",
      "Loading pretrained parameter \"readout.4.bias\".\n",
      "Moving model to cuda\n",
      "Creating protein model\n",
      "MoleculeModel(\n",
      "  (softplus): Softplus(beta=1.0, threshold=20.0)\n",
      "  (encoder): MPN(\n",
      "    (encoder): ModuleList(\n",
      "      (0): MPNEncoder(\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (act_func): ReLU()\n",
      "        (W_i): Linear(in_features=147, out_features=300, bias=False)\n",
      "        (W_h): Linear(in_features=300, out_features=300, bias=False)\n",
      "        (W_o): Linear(in_features=433, out_features=300, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (seq_embedder): Embedding(21, 36, padding_idx=20)\n",
      "  (rotary_embedder): RotaryEmbedding()\n",
      "  (multihead_attn): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=36, out_features=36, bias=True)\n",
      "  )\n",
      "  (attentive_pooler): AttentivePooling(\n",
      "    (linear1): Linear(in_features=1316, out_features=1316, bias=True)\n",
      "    (tanh): Tanh()\n",
      "    (linear2): Linear(in_features=1316, out_features=1, bias=True)\n",
      "    (softmax): Softmax(dim=1)\n",
      "  )\n",
      "  (readout): Sequential(\n",
      "    (0): Dropout(p=0.0, inplace=False)\n",
      "    (1): Linear(in_features=1616, out_features=300, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.0, inplace=False)\n",
      "    (4): Linear(in_features=300, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
      "Loading pretrained parameter \"seq_embedder.weight\".\n",
      "Loading pretrained parameter \"rotary_embedder.freqs\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_weight\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_bias\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.weight\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.bias\".\n",
      "Loading pretrained parameter \"readout.1.weight\".\n",
      "Loading pretrained parameter \"readout.1.bias\".\n",
      "Loading pretrained parameter \"readout.4.weight\".\n",
      "Loading pretrained parameter \"readout.4.bias\".\n",
      "Moving model to cuda\n",
      "Creating protein model\n",
      "MoleculeModel(\n",
      "  (softplus): Softplus(beta=1.0, threshold=20.0)\n",
      "  (encoder): MPN(\n",
      "    (encoder): ModuleList(\n",
      "      (0): MPNEncoder(\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (act_func): ReLU()\n",
      "        (W_i): Linear(in_features=147, out_features=300, bias=False)\n",
      "        (W_h): Linear(in_features=300, out_features=300, bias=False)\n",
      "        (W_o): Linear(in_features=433, out_features=300, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (seq_embedder): Embedding(21, 36, padding_idx=20)\n",
      "  (rotary_embedder): RotaryEmbedding()\n",
      "  (multihead_attn): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=36, out_features=36, bias=True)\n",
      "  )\n",
      "  (attentive_pooler): AttentivePooling(\n",
      "    (linear1): Linear(in_features=1316, out_features=1316, bias=True)\n",
      "    (tanh): Tanh()\n",
      "    (linear2): Linear(in_features=1316, out_features=1, bias=True)\n",
      "    (softmax): Softmax(dim=1)\n",
      "  )\n",
      "  (readout): Sequential(\n",
      "    (0): Dropout(p=0.0, inplace=False)\n",
      "    (1): Linear(in_features=1616, out_features=300, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.0, inplace=False)\n",
      "    (4): Linear(in_features=300, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
      "Loading pretrained parameter \"seq_embedder.weight\".\n",
      "Loading pretrained parameter \"rotary_embedder.freqs\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_weight\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_bias\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.weight\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.bias\".\n",
      "Loading pretrained parameter \"readout.1.weight\".\n",
      "Loading pretrained parameter \"readout.1.bias\".\n",
      "Loading pretrained parameter \"readout.4.weight\".\n",
      "Loading pretrained parameter \"readout.4.bias\".\n",
      "Moving model to cuda\n",
      "Creating protein model\n",
      "MoleculeModel(\n",
      "  (softplus): Softplus(beta=1.0, threshold=20.0)\n",
      "  (encoder): MPN(\n",
      "    (encoder): ModuleList(\n",
      "      (0): MPNEncoder(\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (act_func): ReLU()\n",
      "        (W_i): Linear(in_features=147, out_features=300, bias=False)\n",
      "        (W_h): Linear(in_features=300, out_features=300, bias=False)\n",
      "        (W_o): Linear(in_features=433, out_features=300, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (seq_embedder): Embedding(21, 36, padding_idx=20)\n",
      "  (rotary_embedder): RotaryEmbedding()\n",
      "  (multihead_attn): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=36, out_features=36, bias=True)\n",
      "  )\n",
      "  (attentive_pooler): AttentivePooling(\n",
      "    (linear1): Linear(in_features=1316, out_features=1316, bias=True)\n",
      "    (tanh): Tanh()\n",
      "    (linear2): Linear(in_features=1316, out_features=1, bias=True)\n",
      "    (softmax): Softmax(dim=1)\n",
      "  )\n",
      "  (readout): Sequential(\n",
      "    (0): Dropout(p=0.0, inplace=False)\n",
      "    (1): Linear(in_features=1616, out_features=300, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.0, inplace=False)\n",
      "    (4): Linear(in_features=300, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
      "Loading pretrained parameter \"seq_embedder.weight\".\n",
      "Loading pretrained parameter \"rotary_embedder.freqs\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_weight\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_bias\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.weight\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.bias\".\n",
      "Loading pretrained parameter \"readout.1.weight\".\n",
      "Loading pretrained parameter \"readout.1.bias\".\n",
      "Loading pretrained parameter \"readout.4.weight\".\n",
      "Loading pretrained parameter \"readout.4.bias\".\n",
      "Moving model to cuda\n",
      "Creating protein model\n",
      "MoleculeModel(\n",
      "  (softplus): Softplus(beta=1.0, threshold=20.0)\n",
      "  (encoder): MPN(\n",
      "    (encoder): ModuleList(\n",
      "      (0): MPNEncoder(\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (act_func): ReLU()\n",
      "        (W_i): Linear(in_features=147, out_features=300, bias=False)\n",
      "        (W_h): Linear(in_features=300, out_features=300, bias=False)\n",
      "        (W_o): Linear(in_features=433, out_features=300, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (seq_embedder): Embedding(21, 36, padding_idx=20)\n",
      "  (rotary_embedder): RotaryEmbedding()\n",
      "  (multihead_attn): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=36, out_features=36, bias=True)\n",
      "  )\n",
      "  (attentive_pooler): AttentivePooling(\n",
      "    (linear1): Linear(in_features=1316, out_features=1316, bias=True)\n",
      "    (tanh): Tanh()\n",
      "    (linear2): Linear(in_features=1316, out_features=1, bias=True)\n",
      "    (softmax): Softmax(dim=1)\n",
      "  )\n",
      "  (readout): Sequential(\n",
      "    (0): Dropout(p=0.0, inplace=False)\n",
      "    (1): Linear(in_features=1616, out_features=300, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.0, inplace=False)\n",
      "    (4): Linear(in_features=300, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
      "Loading pretrained parameter \"seq_embedder.weight\".\n",
      "Loading pretrained parameter \"rotary_embedder.freqs\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_weight\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_bias\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.weight\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.bias\".\n",
      "Loading pretrained parameter \"readout.1.weight\".\n",
      "Loading pretrained parameter \"readout.1.bias\".\n",
      "Loading pretrained parameter \"readout.4.weight\".\n",
      "Loading pretrained parameter \"readout.4.bias\".\n",
      "Moving model to cuda\n",
      "Creating protein model\n",
      "MoleculeModel(\n",
      "  (softplus): Softplus(beta=1.0, threshold=20.0)\n",
      "  (encoder): MPN(\n",
      "    (encoder): ModuleList(\n",
      "      (0): MPNEncoder(\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (act_func): ReLU()\n",
      "        (W_i): Linear(in_features=147, out_features=300, bias=False)\n",
      "        (W_h): Linear(in_features=300, out_features=300, bias=False)\n",
      "        (W_o): Linear(in_features=433, out_features=300, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (seq_embedder): Embedding(21, 36, padding_idx=20)\n",
      "  (rotary_embedder): RotaryEmbedding()\n",
      "  (multihead_attn): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=36, out_features=36, bias=True)\n",
      "  )\n",
      "  (attentive_pooler): AttentivePooling(\n",
      "    (linear1): Linear(in_features=1316, out_features=1316, bias=True)\n",
      "    (tanh): Tanh()\n",
      "    (linear2): Linear(in_features=1316, out_features=1, bias=True)\n",
      "    (softmax): Softmax(dim=1)\n",
      "  )\n",
      "  (readout): Sequential(\n",
      "    (0): Dropout(p=0.0, inplace=False)\n",
      "    (1): Linear(in_features=1616, out_features=300, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.0, inplace=False)\n",
      "    (4): Linear(in_features=300, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
      "Loading pretrained parameter \"seq_embedder.weight\".\n",
      "Loading pretrained parameter \"rotary_embedder.freqs\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_weight\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_bias\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.weight\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.bias\".\n",
      "Loading pretrained parameter \"readout.1.weight\".\n",
      "Loading pretrained parameter \"readout.1.bias\".\n",
      "Loading pretrained parameter \"readout.4.weight\".\n",
      "Loading pretrained parameter \"readout.4.bias\".\n",
      "Moving model to cuda\n",
      "Creating protein model\n",
      "MoleculeModel(\n",
      "  (softplus): Softplus(beta=1.0, threshold=20.0)\n",
      "  (encoder): MPN(\n",
      "    (encoder): ModuleList(\n",
      "      (0): MPNEncoder(\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (act_func): ReLU()\n",
      "        (W_i): Linear(in_features=147, out_features=300, bias=False)\n",
      "        (W_h): Linear(in_features=300, out_features=300, bias=False)\n",
      "        (W_o): Linear(in_features=433, out_features=300, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (seq_embedder): Embedding(21, 36, padding_idx=20)\n",
      "  (rotary_embedder): RotaryEmbedding()\n",
      "  (multihead_attn): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=36, out_features=36, bias=True)\n",
      "  )\n",
      "  (attentive_pooler): AttentivePooling(\n",
      "    (linear1): Linear(in_features=1316, out_features=1316, bias=True)\n",
      "    (tanh): Tanh()\n",
      "    (linear2): Linear(in_features=1316, out_features=1, bias=True)\n",
      "    (softmax): Softmax(dim=1)\n",
      "  )\n",
      "  (readout): Sequential(\n",
      "    (0): Dropout(p=0.0, inplace=False)\n",
      "    (1): Linear(in_features=1616, out_features=300, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.0, inplace=False)\n",
      "    (4): Linear(in_features=300, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
      "Loading pretrained parameter \"seq_embedder.weight\".\n",
      "Loading pretrained parameter \"rotary_embedder.freqs\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_weight\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_bias\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.weight\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.bias\".\n",
      "Loading pretrained parameter \"readout.1.weight\".\n",
      "Loading pretrained parameter \"readout.1.bias\".\n",
      "Loading pretrained parameter \"readout.4.weight\".\n",
      "Loading pretrained parameter \"readout.4.bias\".\n",
      "Moving model to cuda\n",
      "Creating protein model\n",
      "MoleculeModel(\n",
      "  (softplus): Softplus(beta=1.0, threshold=20.0)\n",
      "  (encoder): MPN(\n",
      "    (encoder): ModuleList(\n",
      "      (0): MPNEncoder(\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (act_func): ReLU()\n",
      "        (W_i): Linear(in_features=147, out_features=300, bias=False)\n",
      "        (W_h): Linear(in_features=300, out_features=300, bias=False)\n",
      "        (W_o): Linear(in_features=433, out_features=300, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (seq_embedder): Embedding(21, 36, padding_idx=20)\n",
      "  (rotary_embedder): RotaryEmbedding()\n",
      "  (multihead_attn): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=36, out_features=36, bias=True)\n",
      "  )\n",
      "  (attentive_pooler): AttentivePooling(\n",
      "    (linear1): Linear(in_features=1316, out_features=1316, bias=True)\n",
      "    (tanh): Tanh()\n",
      "    (linear2): Linear(in_features=1316, out_features=1, bias=True)\n",
      "    (softmax): Softmax(dim=1)\n",
      "  )\n",
      "  (readout): Sequential(\n",
      "    (0): Dropout(p=0.0, inplace=False)\n",
      "    (1): Linear(in_features=1616, out_features=300, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.0, inplace=False)\n",
      "    (4): Linear(in_features=300, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
      "Loading pretrained parameter \"seq_embedder.weight\".\n",
      "Loading pretrained parameter \"rotary_embedder.freqs\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_weight\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_bias\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.weight\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.bias\".\n",
      "Loading pretrained parameter \"readout.1.weight\".\n",
      "Loading pretrained parameter \"readout.1.bias\".\n",
      "Loading pretrained parameter \"readout.4.weight\".\n",
      "Loading pretrained parameter \"readout.4.bias\".\n",
      "Moving model to cuda\n",
      "Creating protein model\n",
      "MoleculeModel(\n",
      "  (softplus): Softplus(beta=1.0, threshold=20.0)\n",
      "  (encoder): MPN(\n",
      "    (encoder): ModuleList(\n",
      "      (0): MPNEncoder(\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (act_func): ReLU()\n",
      "        (W_i): Linear(in_features=147, out_features=300, bias=False)\n",
      "        (W_h): Linear(in_features=300, out_features=300, bias=False)\n",
      "        (W_o): Linear(in_features=433, out_features=300, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (seq_embedder): Embedding(21, 36, padding_idx=20)\n",
      "  (rotary_embedder): RotaryEmbedding()\n",
      "  (multihead_attn): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=36, out_features=36, bias=True)\n",
      "  )\n",
      "  (attentive_pooler): AttentivePooling(\n",
      "    (linear1): Linear(in_features=1316, out_features=1316, bias=True)\n",
      "    (tanh): Tanh()\n",
      "    (linear2): Linear(in_features=1316, out_features=1, bias=True)\n",
      "    (softmax): Softmax(dim=1)\n",
      "  )\n",
      "  (readout): Sequential(\n",
      "    (0): Dropout(p=0.0, inplace=False)\n",
      "    (1): Linear(in_features=1616, out_features=300, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.0, inplace=False)\n",
      "    (4): Linear(in_features=300, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
      "Loading pretrained parameter \"seq_embedder.weight\".\n",
      "Loading pretrained parameter \"rotary_embedder.freqs\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_weight\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_bias\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.weight\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.bias\".\n",
      "Loading pretrained parameter \"readout.1.weight\".\n",
      "Loading pretrained parameter \"readout.1.bias\".\n",
      "Loading pretrained parameter \"readout.4.weight\".\n",
      "Loading pretrained parameter \"readout.4.bias\".\n",
      "Moving model to cuda\n",
      "Saving predictions to ../pipeline_entry_km_input_output.csv\n",
      "Elapsed time = 0:00:07\n",
      "Prediction completed.\n",
      "Output saved to results/ pipeline_entry_km_input_output.csv\n",
      "\n",
      "output |    |   Unnamed: 0.2 |   Unnamed: 0.1 |   Unnamed: 0 | substrate    | SMILES                                                                                                                    | sequence                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 | pdbpath   |   log10kcat_max |   log10kcat_max_mve_uncal_var |   log10kcat_max_model_0 |   log10kcat_max_model_1 |   log10kcat_max_model_2 |   log10kcat_max_model_3 |   log10kcat_max_model_4 |   log10kcat_max_model_5 |   log10kcat_max_model_6 |   log10kcat_max_model_7 |   log10kcat_max_model_8 |   log10kcat_max_model_9 |   Prediction_(s^(-1)) |   Prediction_log10 |   SD_total |   SD_aleatoric |   SD_epistemic |\n",
      "|---:|---------------:|---------------:|-------------:|:-------------|:--------------------------------------------------------------------------------------------------------------------------|:-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:----------|----------------:|------------------------------:|------------------------:|------------------------:|------------------------:|------------------------:|------------------------:|------------------------:|------------------------:|------------------------:|------------------------:|------------------------:|----------------------:|-------------------:|-----------:|---------------:|---------------:|\n",
      "|  0 |              0 |              0 |            0 | Acetate      | CC(=O)[O-]                                                                                                                | MAENHDYEREINRLFELQKKNVVRLRTSSIDERIAKLKKLKEYIWENKEKIQEAVYNDLRKPPEEVLLTEIYPVVSEIRHVIKNLKKWTKPKKVRTPISLFGAKSYYRFEAKGVVLIISPWNYPFELSIGPLITAIAAGNAVVLKPSELSPHTSGYIKKLVADIFDESEVAVVEGDAVVAQKLLEMGFNHIFFTGSTKVAKAVLKKASETLSSVTLELGGKSPVIIDGKFDIEEAAKKITWGKYLNAGQTCIAPDYVFVKKELLGDFVSHLKHYIKKYYYSDGSGRCSNYCGIINERHFNRLKNVFEVTVKEGAKVCEGGLFVENECYISPTVLTDVGRDSYIMEEEIFGPILPVLTYEKIDDVIEYINSKPAPLVLYVFSRDRKFYRHVINNVISGDCLINDVIAHFANPRLPFGGHNASGIGKSHGYYGFREFSHLRSIMIQPKRTMLQLLYPPYGEFVKKLIEWSTKYF | seq2.seq  |         1.95968 |                      0.634501 |                 3.27818 |                 1.82372 |                1.19568  |                 2.73014 |                 2.01807 |                 2.13049 |                 1.31395 |                2.73981  |                 1.57783 |                0.788914 |               91.1336 |            1.95968 |   0.796556 |       0.289153 |       0.742221 |\n",
      "|  1 |              1 |              1 |            1 | NADH         | NC(=O)C1=CN([C@@H]2O[C@H](COP(=O)(O)OP(=O)(O)OC[C@H]3O[C@@H](n4cnc5c(N)ncnc54)[C@H](O)[C@@H]3O)[C@@H](O)[C@H]2O)C=CC1     | MAENHDYEREINRLFELQKKNVVRLRTSSIDERIAKLKKLKEYIWENKEKIQEAVYNDLRKPPEEVLLTEIYPVVSEIRHVIKNLKKWTKPKKVRTPISLFGAKSYYRFEAKGVVLIISPWNYPFELSIGPLITAIAAGNAVVLKPSELSPHTSGYIKKLVADIFDESEVAVVEGDAVVAQKLLEMGFNHIFFTGSTKVAKAVLKKASETLSSVTLELGGKSPVIIDGKFDIEEAAKKITWGKYLNAGQTCIAPDYVFVKKELLGDFVSHLKHYIKKYYYSDGSGRCSNYCGIINERHFNRLKNVFEVTVKEGAKVCEGGLFVENECYISPTVLTDVGRDSYIMEEEIFGPILPVLTYEKIDDVIEYINSKPAPLVLYVFSRDRKFYRHVINNVISGDCLINDVIAHFANPRLPFGGHNASGIGKSHGYYGFREFSHLRSIMIQPKRTMLQLLYPPYGEFVKKLIEWSTKYF | seq2.seq  |         1.34998 |                      0.291451 |                 1.99435 |                 1.38465 |                0.890309 |                 2.22857 |                 1.41556 |                 1.26048 |                 1.42937 |                0.926598 |                 1.36747 |                0.602416 |               22.386  |            1.34998 |   0.539862 |       0.275292 |       0.464398 |\n",
      "|  2 |              2 |              2 |            2 | Acetaldehyde | CC=O                                                                                                                      | MAENHDYEREINRLFELQKKNVVRLRTSSIDERIAKLKKLKEYIWENKEKIQEAVYNDLRKPPEEVLLTEIYPVVSEIRHVIKNLKKWTKPKKVRTPISLFGAKSYYRFEAKGVVLIISPWNYPFELSIGPLITAIAAGNAVVLKPSELSPHTSGYIKKLVADIFDESEVAVVEGDAVVAQKLLEMGFNHIFFTGSTKVAKAVLKKASETLSSVTLELGGKSPVIIDGKFDIEEAAKKITWGKYLNAGQTCIAPDYVFVKKELLGDFVSHLKHYIKKYYYSDGSGRCSNYCGIINERHFNRLKNVFEVTVKEGAKVCEGGLFVENECYISPTVLTDVGRDSYIMEEEIFGPILPVLTYEKIDDVIEYINSKPAPLVLYVFSRDRKFYRHVINNVISGDCLINDVIAHFANPRLPFGGHNASGIGKSHGYYGFREFSHLRSIMIQPKRTMLQLLYPPYGEFVKKLIEWSTKYF | seq2.seq  |         1.44485 |                      0.274044 |                 2.55269 |                 1.24864 |                0.535019 |                 1.72355 |                 1.41587 |                 1.46403 |                 1.52217 |                1.26449  |                 1.54358 |                1.1785   |               27.8518 |            1.44485 |   0.523492 |       0.21184  |       0.478715 |\n",
      "|  3 |              3 |              3 |            3 | NAD+         | NC(=O)c1ccc[n+]([C@@H]2O[C@H](COP(=O)([O-])OP(=O)(O)OC[C@H]3O[C@@H](n4cnc5c(N)ncnc54)[C@H](O)[C@@H]3O)[C@@H](O)[C@H]2O)c1 | MAENHDYEREINRLFELQKKNVVRLRTSSIDERIAKLKKLKEYIWENKEKIQEAVYNDLRKPPEEVLLTEIYPVVSEIRHVIKNLKKWTKPKKVRTPISLFGAKSYYRFEAKGVVLIISPWNYPFELSIGPLITAIAAGNAVVLKPSELSPHTSGYIKKLVADIFDESEVAVVEGDAVVAQKLLEMGFNHIFFTGSTKVAKAVLKKASETLSSVTLELGGKSPVIIDGKFDIEEAAKKITWGKYLNAGQTCIAPDYVFVKKELLGDFVSHLKHYIKKYYYSDGSGRCSNYCGIINERHFNRLKNVFEVTVKEGAKVCEGGLFVENECYISPTVLTDVGRDSYIMEEEIFGPILPVLTYEKIDDVIEYINSKPAPLVLYVFSRDRKFYRHVINNVISGDCLINDVIAHFANPRLPFGGHNASGIGKSHGYYGFREFSHLRSIMIQPKRTMLQLLYPPYGEFVKKLIEWSTKYF | seq2.seq  |         1.30849 |                      0.268198 |                 1.97728 |                 1.28595 |                0.636765 |                 2.10151 |                 1.20736 |                 1.1243  |                 1.25603 |                1.37277  |                 1.24605 |                0.876884 |               20.3465 |            1.30849 |   0.517879 |       0.30214  |       0.420606 |\n",
      "SMILES ['CC(=O)[O-]', 'C1C=CN(C=C1C(=O)N)[C@H]2[C@@H]([C@@H]([C@H](O2)COP(=O)(O)OP(=O)(O)OC[C@@H]3[C@H]([C@H]([C@@H](O3)N4C=NC5=C(N=CN=C54)N)O)O)O)O', 'CC=O', 'C1=CC(=C[N+](=C1)[C@H]2[C@@H]([C@@H]([C@H](O2)COP(=O)([O-])OP(=O)(O)OC[C@@H]3[C@H]([C@H]([C@@H](O3)N4C=NC5=C(N=CN=C54)N)O)O)O)O)C(=O)N']\n",
      "smiles_list CC(=O)[O-] ['C(=O)[O-]', 'CC(=O)SCCNC(=O)CCNC(=O)[C@@H](C(C)(C)COP(=O)(O)OP(=O)(O)OC[C@@H]1[C@H]([C@H]([C@@H](O1)N2C=NC3=C(N=CN=C32)N)O)OP(=O)(O)O)O', 'CC(=O)C(=O)[O-]', 'CC(C)(COP(=O)(O)OP(=O)(O)OC[C@@H]1[C@H]([C@H]([C@@H](O1)N2C=NC3=C(N=CN=C32)N)O)OP(=O)(O)O)[C@H](C(=O)NCCC(=O)NCCS)O', 'CC(=O)[O-]']\n",
      "smiles_list C1C=CN(C=C1C(=O)N)[C@H]2[C@@H]([C@@H]([C@H](O2)COP(=O)(O)OP(=O)(O)OC[C@@H]3[C@H]([C@H]([C@@H](O3)N4C=NC5=C(N=CN=C54)N)O)O)O)O ['C(=O)[O-]', 'CC(=O)SCCNC(=O)CCNC(=O)[C@@H](C(C)(C)COP(=O)(O)OP(=O)(O)OC[C@@H]1[C@H]([C@H]([C@@H](O1)N2C=NC3=C(N=CN=C32)N)O)OP(=O)(O)O)O', 'CC(=O)C(=O)[O-]', 'CC(C)(COP(=O)(O)OP(=O)(O)OC[C@@H]1[C@H]([C@H]([C@@H](O1)N2C=NC3=C(N=CN=C32)N)O)OP(=O)(O)O)[C@H](C(=O)NCCC(=O)NCCS)O', 'CC(=O)[O-]', 'C1C=CN(C=C1C(=O)N)[C@H]2[C@@H]([C@@H]([C@H](O2)COP(=O)(O)OP(=O)(O)OC[C@@H]3[C@H]([C@H]([C@@H](O3)N4C=NC5=C(N=CN=C54)N)O)O)O)O']\n",
      "smiles_list CC=O ['C(=O)[O-]', 'CC(=O)SCCNC(=O)CCNC(=O)[C@@H](C(C)(C)COP(=O)(O)OP(=O)(O)OC[C@@H]1[C@H]([C@H]([C@@H](O1)N2C=NC3=C(N=CN=C32)N)O)OP(=O)(O)O)O', 'CC(=O)C(=O)[O-]', 'CC(C)(COP(=O)(O)OP(=O)(O)OC[C@@H]1[C@H]([C@H]([C@@H](O1)N2C=NC3=C(N=CN=C32)N)O)OP(=O)(O)O)[C@H](C(=O)NCCC(=O)NCCS)O', 'CC(=O)[O-]', 'C1C=CN(C=C1C(=O)N)[C@H]2[C@@H]([C@@H]([C@H](O2)COP(=O)(O)OP(=O)(O)OC[C@@H]3[C@H]([C@H]([C@@H](O3)N4C=NC5=C(N=CN=C54)N)O)O)O)O', 'CC=O']\n",
      "smiles_list C1=CC(=C[N+](=C1)[C@H]2[C@@H]([C@@H]([C@H](O2)COP(=O)([O-])OP(=O)(O)OC[C@@H]3[C@H]([C@H]([C@@H](O3)N4C=NC5=C(N=CN=C54)N)O)O)O)O)C(=O)N ['C(=O)[O-]', 'CC(=O)SCCNC(=O)CCNC(=O)[C@@H](C(C)(C)COP(=O)(O)OP(=O)(O)OC[C@@H]1[C@H]([C@H]([C@@H](O1)N2C=NC3=C(N=CN=C32)N)O)OP(=O)(O)O)O', 'CC(=O)C(=O)[O-]', 'CC(C)(COP(=O)(O)OP(=O)(O)OC[C@@H]1[C@H]([C@H]([C@@H](O1)N2C=NC3=C(N=CN=C32)N)O)OP(=O)(O)O)[C@H](C(=O)NCCC(=O)NCCS)O', 'CC(=O)[O-]', 'C1C=CN(C=C1C(=O)N)[C@H]2[C@@H]([C@@H]([C@H](O2)COP(=O)(O)OP(=O)(O)OC[C@@H]3[C@H]([C@H]([C@@H](O3)N4C=NC5=C(N=CN=C54)N)O)O)O)O', 'CC=O', 'C1=CC(=C[N+](=C1)[C@H]2[C@@H]([C@@H]([C@H](O2)COP(=O)([O-])OP(=O)(O)OC[C@@H]3[C@H]([C@H]([C@@H](O3)N4C=NC5=C(N=CN=C54)N)O)O)O)O)C(=O)N']\n",
      "item {'substrates': ['Ethanol', 'NAD+', 'Acetaldehyde', 'NADH'], 'SMILES': ['CCO', 'C1=CC(=C[N+](=C1)[C@H]2[C@@H]([C@@H]([C@H](O2)COP(=O)([O-])OP(=O)(O)OC[C@@H]3[C@H]([C@H]([C@@H](O3)N4C=NC5=C(N=CN=C54)N)O)O)O)O)C(=O)N', 'CC=O', 'C1C=CN(C=C1C(=O)N)[C@H]2[C@@H]([C@@H]([C@H](O2)COP(=O)(O)OP(=O)(O)OC[C@@H]3[C@H]([C@H]([C@@H](O3)N4C=NC5=C(N=CN=C54)N)O)O)O)O'], 'sequences': ['MTKIANKYEVIDNVEKLEKALKRLREAQSVYATYTQEQVDKIFFEAAMAANKMRIPLAKMAVEETGMGVVEDKVIKNHYASEYIYNAYKNTKTCGVIEEDPAFGIKKIAEPLGVIAAVIPTTNPTSTAIFKTLIALKTRNAIIISPHPRAKNSTIEAAKIVLEAAVKAGAPEGIIGWIDVPSLELTNLVMREADVILATGGPGLVKAAYSSGKPAIGVGAGNTPAIIDDSADIVLAVNSIIHSKTFDNGMICASEQSVIVLDGVYKEVKKEFEKRGCYFLNEDETEKVRKTIIINGALNAKIVGQKAHTIANLAGFEVPETTKILIGEVTSVDISEEFAHEKLCPVLAMYRAKDFDDALDKAERLVADGGFGHTSSLYIDTVTQKEKLQKFSERMKTCRILVNTPSSQGGIGDLYNFKLAPSLTLGCGSWGGNSVSDNVGVKHLLNIKTVAERRENMLWFRTPEKIYIKRGCLPVALDELKNVMGKKKAFIVTDNFLYNNGYTKPITDKLDEMGIVHKTFFDVSPDPSLASAKAGAAEMLAFQPDTIIAVGGGSAMDAAKIMWVMYEHPEVDFMDMAMRFMDIRKRVYTFPKMGQKAYFIAIPTSAGTGSEVTPFAVITDEKTGIKYPLADYELLPDMAIVDADMMMNAPKGLTAASGIDALTHALEAYVSMLATDYTDSLALRAIKMIFEYLPRAYENGASDPVAREKMANAATIAGMAFANAFLGVCHSMAHKLGAFYHLPHGVANALMINEVIRFNSSEAPTKMGTFPQYDHPRTLERYAEIADYIGLKGKNNEEKVENLIKAIDELKEKVGIRKTIKDYDIDEKEFLDRLDEMVEQAFDDQCTGTNPRYPLMNEIRQMYLNAYYGGAKK', 'MTKIANKYEVIDNVEKLEKALKRLREAQSVYATYTQEQVDKIFFEAAMAANKMRIPLAKMAVEETGMGVVEDKVIKNHYASEYIYNAYKNTKTCGVIEEDPAFGIKKIAEPLGVIAAVIPTTNPTSTAIFKTLIALKTRNAIIISPHPRAKNSTIEAAKIVLEAAVKAGAPEGIIGWIDVPSLELTNLVMREADVILATGGPGLVKAAYSSGKPAIGVGAGNTPAIIDDSADIVLAVNSIIHSKTFDNGMICASEQSVIVLDGVYKEVKKEFEKRGCYFLNEDETEKVRKTIIINGALNAKIVGQKAHTIANLAGFEVPETTKILIGEVTSVDISEEFAHEKLCPVLAMYRAKDFDDALDKAERLVADGGFGHTSSLYIDTVTQKEKLQKFSERMKTCRILVNTPSSQGGIGDLYNFKLAPSLTLGCGSWGGNSVSDNVGVKHLLNIKTVAERRENMLWFRTPEKIYIKRGCLPVALDELKNVMGKKKAFIVTDNFLYNNGYTKPITDKLDEMGIVHKTFFDVSPDPSLASAKAGAAEMLAFQPDTIIAVGGGSAMDAAKIMWVMYEHPEVDFMDMAMRFMDIRKRVYTFPKMGQKAYFIAIPTSAGTGSEVTPFAVITDEKTGIKYPLADYELLPDMAIVDADMMMNAPKGLTAASGIDALTHALEAYVSMLATDYTDSLALRAIKMIFEYLPRAYENGASDPVAREKMANAATIAGMAFANAFLGVCHSMAHKLGAFYHLPHGVANALMINEVIRFNSSEAPTKMGTFPQYDHPRTLERYAEIADYIGLKGKNNEEKVENLIKAIDELKEKVGIRKTIKDYDIDEKEFLDRLDEMVEQAFDDQCTGTNPRYPLMNEIRQMYLNAYYGGAKK', 'MTKIANKYEVIDNVEKLEKALKRLREAQSVYATYTQEQVDKIFFEAAMAANKMRIPLAKMAVEETGMGVVEDKVIKNHYASEYIYNAYKNTKTCGVIEEDPAFGIKKIAEPLGVIAAVIPTTNPTSTAIFKTLIALKTRNAIIISPHPRAKNSTIEAAKIVLEAAVKAGAPEGIIGWIDVPSLELTNLVMREADVILATGGPGLVKAAYSSGKPAIGVGAGNTPAIIDDSADIVLAVNSIIHSKTFDNGMICASEQSVIVLDGVYKEVKKEFEKRGCYFLNEDETEKVRKTIIINGALNAKIVGQKAHTIANLAGFEVPETTKILIGEVTSVDISEEFAHEKLCPVLAMYRAKDFDDALDKAERLVADGGFGHTSSLYIDTVTQKEKLQKFSERMKTCRILVNTPSSQGGIGDLYNFKLAPSLTLGCGSWGGNSVSDNVGVKHLLNIKTVAERRENMLWFRTPEKIYIKRGCLPVALDELKNVMGKKKAFIVTDNFLYNNGYTKPITDKLDEMGIVHKTFFDVSPDPSLASAKAGAAEMLAFQPDTIIAVGGGSAMDAAKIMWVMYEHPEVDFMDMAMRFMDIRKRVYTFPKMGQKAYFIAIPTSAGTGSEVTPFAVITDEKTGIKYPLADYELLPDMAIVDADMMMNAPKGLTAASGIDALTHALEAYVSMLATDYTDSLALRAIKMIFEYLPRAYENGASDPVAREKMANAATIAGMAFANAFLGVCHSMAHKLGAFYHLPHGVANALMINEVIRFNSSEAPTKMGTFPQYDHPRTLERYAEIADYIGLKGKNNEEKVENLIKAIDELKEKVGIRKTIKDYDIDEKEFLDRLDEMVEQAFDDQCTGTNPRYPLMNEIRQMYLNAYYGGAKK', 'MTKIANKYEVIDNVEKLEKALKRLREAQSVYATYTQEQVDKIFFEAAMAANKMRIPLAKMAVEETGMGVVEDKVIKNHYASEYIYNAYKNTKTCGVIEEDPAFGIKKIAEPLGVIAAVIPTTNPTSTAIFKTLIALKTRNAIIISPHPRAKNSTIEAAKIVLEAAVKAGAPEGIIGWIDVPSLELTNLVMREADVILATGGPGLVKAAYSSGKPAIGVGAGNTPAIIDDSADIVLAVNSIIHSKTFDNGMICASEQSVIVLDGVYKEVKKEFEKRGCYFLNEDETEKVRKTIIINGALNAKIVGQKAHTIANLAGFEVPETTKILIGEVTSVDISEEFAHEKLCPVLAMYRAKDFDDALDKAERLVADGGFGHTSSLYIDTVTQKEKLQKFSERMKTCRILVNTPSSQGGIGDLYNFKLAPSLTLGCGSWGGNSVSDNVGVKHLLNIKTVAERRENMLWFRTPEKIYIKRGCLPVALDELKNVMGKKKAFIVTDNFLYNNGYTKPITDKLDEMGIVHKTFFDVSPDPSLASAKAGAAEMLAFQPDTIIAVGGGSAMDAAKIMWVMYEHPEVDFMDMAMRFMDIRKRVYTFPKMGQKAYFIAIPTSAGTGSEVTPFAVITDEKTGIKYPLADYELLPDMAIVDADMMMNAPKGLTAASGIDALTHALEAYVSMLATDYTDSLALRAIKMIFEYLPRAYENGASDPVAREKMANAATIAGMAFANAFLGVCHSMAHKLGAFYHLPHGVANALMINEVIRFNSSEAPTKMGTFPQYDHPRTLERYAEIADYIGLKGKNNEEKVENLIKAIDELKEKVGIRKTIKDYDIDEKEFLDRLDEMVEQAFDDQCTGTNPRYPLMNEIRQMYLNAYYGGAKK'], 'pdb_paths': ['seq3.seq', 'seq3.seq', 'seq3.seq', 'seq3.seq'], 'subs': ['CCO', 'C1=CC(=C[N+](=C1)[C@H]2[C@@H]([C@@H]([C@H](O2)COP(=O)([O-])OP(=O)(O)OC[C@@H]3[C@H]([C@H]([C@@H](O3)N4C=NC5=C(N=CN=C54)N)O)O)O)O)C(=O)N'], 'stoich': [1, 1, 1, 1], 'EC': '1.1.1.71'}\n",
      "starting R00746\n",
      "input |    | substrate    | SMILES                                                                                                                                 | sequence                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  | pdbpath   |\n",
      "|---:|:-------------|:---------------------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:----------|\n",
      "|  0 | Ethanol      | CCO                                                                                                                                    | MTKIANKYEVIDNVEKLEKALKRLREAQSVYATYTQEQVDKIFFEAAMAANKMRIPLAKMAVEETGMGVVEDKVIKNHYASEYIYNAYKNTKTCGVIEEDPAFGIKKIAEPLGVIAAVIPTTNPTSTAIFKTLIALKTRNAIIISPHPRAKNSTIEAAKIVLEAAVKAGAPEGIIGWIDVPSLELTNLVMREADVILATGGPGLVKAAYSSGKPAIGVGAGNTPAIIDDSADIVLAVNSIIHSKTFDNGMICASEQSVIVLDGVYKEVKKEFEKRGCYFLNEDETEKVRKTIIINGALNAKIVGQKAHTIANLAGFEVPETTKILIGEVTSVDISEEFAHEKLCPVLAMYRAKDFDDALDKAERLVADGGFGHTSSLYIDTVTQKEKLQKFSERMKTCRILVNTPSSQGGIGDLYNFKLAPSLTLGCGSWGGNSVSDNVGVKHLLNIKTVAERRENMLWFRTPEKIYIKRGCLPVALDELKNVMGKKKAFIVTDNFLYNNGYTKPITDKLDEMGIVHKTFFDVSPDPSLASAKAGAAEMLAFQPDTIIAVGGGSAMDAAKIMWVMYEHPEVDFMDMAMRFMDIRKRVYTFPKMGQKAYFIAIPTSAGTGSEVTPFAVITDEKTGIKYPLADYELLPDMAIVDADMMMNAPKGLTAASGIDALTHALEAYVSMLATDYTDSLALRAIKMIFEYLPRAYENGASDPVAREKMANAATIAGMAFANAFLGVCHSMAHKLGAFYHLPHGVANALMINEVIRFNSSEAPTKMGTFPQYDHPRTLERYAEIADYIGLKGKNNEEKVENLIKAIDELKEKVGIRKTIKDYDIDEKEFLDRLDEMVEQAFDDQCTGTNPRYPLMNEIRQMYLNAYYGGAKK | seq3.seq  |\n",
      "|  1 | NAD+         | C1=CC(=C[N+](=C1)[C@H]2[C@@H]([C@@H]([C@H](O2)COP(=O)([O-])OP(=O)(O)OC[C@@H]3[C@H]([C@H]([C@@H](O3)N4C=NC5=C(N=CN=C54)N)O)O)O)O)C(=O)N | MTKIANKYEVIDNVEKLEKALKRLREAQSVYATYTQEQVDKIFFEAAMAANKMRIPLAKMAVEETGMGVVEDKVIKNHYASEYIYNAYKNTKTCGVIEEDPAFGIKKIAEPLGVIAAVIPTTNPTSTAIFKTLIALKTRNAIIISPHPRAKNSTIEAAKIVLEAAVKAGAPEGIIGWIDVPSLELTNLVMREADVILATGGPGLVKAAYSSGKPAIGVGAGNTPAIIDDSADIVLAVNSIIHSKTFDNGMICASEQSVIVLDGVYKEVKKEFEKRGCYFLNEDETEKVRKTIIINGALNAKIVGQKAHTIANLAGFEVPETTKILIGEVTSVDISEEFAHEKLCPVLAMYRAKDFDDALDKAERLVADGGFGHTSSLYIDTVTQKEKLQKFSERMKTCRILVNTPSSQGGIGDLYNFKLAPSLTLGCGSWGGNSVSDNVGVKHLLNIKTVAERRENMLWFRTPEKIYIKRGCLPVALDELKNVMGKKKAFIVTDNFLYNNGYTKPITDKLDEMGIVHKTFFDVSPDPSLASAKAGAAEMLAFQPDTIIAVGGGSAMDAAKIMWVMYEHPEVDFMDMAMRFMDIRKRVYTFPKMGQKAYFIAIPTSAGTGSEVTPFAVITDEKTGIKYPLADYELLPDMAIVDADMMMNAPKGLTAASGIDALTHALEAYVSMLATDYTDSLALRAIKMIFEYLPRAYENGASDPVAREKMANAATIAGMAFANAFLGVCHSMAHKLGAFYHLPHGVANALMINEVIRFNSSEAPTKMGTFPQYDHPRTLERYAEIADYIGLKGKNNEEKVENLIKAIDELKEKVGIRKTIKDYDIDEKEFLDRLDEMVEQAFDDQCTGTNPRYPLMNEIRQMYLNAYYGGAKK | seq3.seq  |\n",
      "|  2 | Acetaldehyde | CC=O                                                                                                                                   | MTKIANKYEVIDNVEKLEKALKRLREAQSVYATYTQEQVDKIFFEAAMAANKMRIPLAKMAVEETGMGVVEDKVIKNHYASEYIYNAYKNTKTCGVIEEDPAFGIKKIAEPLGVIAAVIPTTNPTSTAIFKTLIALKTRNAIIISPHPRAKNSTIEAAKIVLEAAVKAGAPEGIIGWIDVPSLELTNLVMREADVILATGGPGLVKAAYSSGKPAIGVGAGNTPAIIDDSADIVLAVNSIIHSKTFDNGMICASEQSVIVLDGVYKEVKKEFEKRGCYFLNEDETEKVRKTIIINGALNAKIVGQKAHTIANLAGFEVPETTKILIGEVTSVDISEEFAHEKLCPVLAMYRAKDFDDALDKAERLVADGGFGHTSSLYIDTVTQKEKLQKFSERMKTCRILVNTPSSQGGIGDLYNFKLAPSLTLGCGSWGGNSVSDNVGVKHLLNIKTVAERRENMLWFRTPEKIYIKRGCLPVALDELKNVMGKKKAFIVTDNFLYNNGYTKPITDKLDEMGIVHKTFFDVSPDPSLASAKAGAAEMLAFQPDTIIAVGGGSAMDAAKIMWVMYEHPEVDFMDMAMRFMDIRKRVYTFPKMGQKAYFIAIPTSAGTGSEVTPFAVITDEKTGIKYPLADYELLPDMAIVDADMMMNAPKGLTAASGIDALTHALEAYVSMLATDYTDSLALRAIKMIFEYLPRAYENGASDPVAREKMANAATIAGMAFANAFLGVCHSMAHKLGAFYHLPHGVANALMINEVIRFNSSEAPTKMGTFPQYDHPRTLERYAEIADYIGLKGKNNEEKVENLIKAIDELKEKVGIRKTIKDYDIDEKEFLDRLDEMVEQAFDDQCTGTNPRYPLMNEIRQMYLNAYYGGAKK | seq3.seq  |\n",
      "|  3 | NADH         | C1C=CN(C=C1C(=O)N)[C@H]2[C@@H]([C@@H]([C@H](O2)COP(=O)(O)OP(=O)(O)OC[C@@H]3[C@H]([C@H]([C@@H](O3)N4C=NC5=C(N=CN=C54)N)O)O)O)O          | MTKIANKYEVIDNVEKLEKALKRLREAQSVYATYTQEQVDKIFFEAAMAANKMRIPLAKMAVEETGMGVVEDKVIKNHYASEYIYNAYKNTKTCGVIEEDPAFGIKKIAEPLGVIAAVIPTTNPTSTAIFKTLIALKTRNAIIISPHPRAKNSTIEAAKIVLEAAVKAGAPEGIIGWIDVPSLELTNLVMREADVILATGGPGLVKAAYSSGKPAIGVGAGNTPAIIDDSADIVLAVNSIIHSKTFDNGMICASEQSVIVLDGVYKEVKKEFEKRGCYFLNEDETEKVRKTIIINGALNAKIVGQKAHTIANLAGFEVPETTKILIGEVTSVDISEEFAHEKLCPVLAMYRAKDFDDALDKAERLVADGGFGHTSSLYIDTVTQKEKLQKFSERMKTCRILVNTPSSQGGIGDLYNFKLAPSLTLGCGSWGGNSVSDNVGVKHLLNIKTVAERRENMLWFRTPEKIYIKRGCLPVALDELKNVMGKKKAFIVTDNFLYNNGYTKPITDKLDEMGIVHKTFFDVSPDPSLASAKAGAAEMLAFQPDTIIAVGGGSAMDAAKIMWVMYEHPEVDFMDMAMRFMDIRKRVYTFPKMGQKAYFIAIPTSAGTGSEVTPFAVITDEKTGIKYPLADYELLPDMAIVDADMMMNAPKGLTAASGIDALTHALEAYVSMLATDYTDSLALRAIKMIFEYLPRAYENGASDPVAREKMANAATIAGMAFANAFLGVCHSMAHKLGAFYHLPHGVANALMINEVIRFNSSEAPTKMGTFPQYDHPRTLERYAEIADYIGLKGKNNEEKVENLIKAIDELKEKVGIRKTIKDYDIDEKEFLDRLDEMVEQAFDDQCTGTNPRYPLMNEIRQMYLNAYYGGAKK | seq3.seq  |\n",
      "input2 ['CCO', 'C1=CC(=C[N+](=C1)[C@H]2[C@@H]([C@@H]([C@H](O2)COP(=O)([O-])OP(=O)(O)OC[C@@H]3[C@H]([C@H]([C@@H](O3)N4C=NC5=C(N=CN=C54)N)O)O)O)O)C(=O)N', 'CC=O', 'C1C=CN(C=C1C(=O)N)[C@H]2[C@@H]([C@@H]([C@H](O2)COP(=O)(O)OP(=O)(O)OC[C@@H]3[C@H]([C@H]([C@@H](O3)N4C=NC5=C(N=CN=C54)N)O)O)O)O']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xuvin/.conda/envs/catpred/lib/python3.9/site-packages/rotary_embedding_torch/rotary_embedding_torch.py:35: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  @autocast(enabled = False)\n",
      "/home/xuvin/.conda/envs/catpred/lib/python3.9/site-packages/rotary_embedding_torch/rotary_embedding_torch.py:268: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  @autocast(enabled = False)\n",
      "\n",
      "0it [00:00, ?it/s]/home/xuvin/py_repos/wccm/src/catpred_pipeline/catpred/catpred/data/cache_utils.py:95: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(str(entry_path))\n",
      "\n",
      "4it [00:00, 72.60it/s]\n",
      "\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 40233.13it/s]\n",
      "\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 2629.66it/s]\n",
      "\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]/home/xuvin/py_repos/wccm/src/catpred_pipeline/catpred/catpred/utils.py:113: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(path, map_location=lambda storage, loc: storage)\n",
      "/home/xuvin/py_repos/wccm/src/catpred_pipeline/catpred/catpred/utils.py:446: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(path, map_location=lambda storage, loc: storage)\n",
      "\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.45s/it]\u001b[A\n",
      "\n",
      "                                             \u001b[A\n",
      " 10%|█         | 1/10 [00:03<00:35,  3.97s/it]/home/xuvin/py_repos/wccm/src/catpred_pipeline/catpred/catpred/utils.py:113: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(path, map_location=lambda storage, loc: storage)\n",
      "/home/xuvin/py_repos/wccm/src/catpred_pipeline/catpred/catpred/utils.py:446: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(path, map_location=lambda storage, loc: storage)\n",
      "\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "                                     \u001b[A\n",
      " 20%|██        | 2/10 [00:04<00:14,  1.80s/it]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "                                     \u001b[A\n",
      " 30%|███       | 3/10 [00:04<00:08,  1.17s/it]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "                                     \u001b[A\n",
      " 40%|████      | 4/10 [00:04<00:04,  1.21it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "                                     \u001b[A\n",
      " 50%|█████     | 5/10 [00:05<00:03,  1.61it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "                                     \u001b[A\n",
      " 60%|██████    | 6/10 [00:05<00:01,  2.05it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "                                     \u001b[A\n",
      " 70%|███████   | 7/10 [00:05<00:01,  2.08it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "                                     \u001b[A\n",
      " 80%|████████  | 8/10 [00:06<00:00,  2.44it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "                                     \u001b[A\n",
      " 90%|█████████ | 9/10 [00:06<00:00,  2.74it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "                                     \u001b[A\n",
      "100%|██████████| 10/10 [00:06<00:00,  2.97it/s]\n",
      "100%|██████████| 10/10 [00:06<00:00,  1.49it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/xuvin/py_repos/wccm/src/catpred_pipeline/catpred\n",
      "Predicting.. This will take a while..\n",
      "calculating protein embed only on cpu\n",
      "Loading training args\n",
      "Loading models\n",
      "Setting molecule featurization parameters to default.\n",
      "Loading data\n",
      "Validating SMILES\n",
      "Test size = 4\n",
      "Creating protein model\n",
      "MoleculeModel(\n",
      "  (softplus): Softplus(beta=1.0, threshold=20.0)\n",
      "  (encoder): MPN(\n",
      "    (encoder): ModuleList(\n",
      "      (0): MPNEncoder(\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (act_func): ReLU()\n",
      "        (W_i): Linear(in_features=147, out_features=300, bias=False)\n",
      "        (W_h): Linear(in_features=300, out_features=300, bias=False)\n",
      "        (W_o): Linear(in_features=433, out_features=300, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (seq_embedder): Embedding(21, 36, padding_idx=20)\n",
      "  (rotary_embedder): RotaryEmbedding()\n",
      "  (multihead_attn): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=36, out_features=36, bias=True)\n",
      "  )\n",
      "  (attentive_pooler): AttentivePooling(\n",
      "    (linear1): Linear(in_features=1316, out_features=1316, bias=True)\n",
      "    (tanh): Tanh()\n",
      "    (linear2): Linear(in_features=1316, out_features=1, bias=True)\n",
      "    (softmax): Softmax(dim=1)\n",
      "  )\n",
      "  (readout): Sequential(\n",
      "    (0): Dropout(p=0.0, inplace=False)\n",
      "    (1): Linear(in_features=1616, out_features=300, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.0, inplace=False)\n",
      "    (4): Linear(in_features=300, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
      "Loading pretrained parameter \"seq_embedder.weight\".\n",
      "Loading pretrained parameter \"rotary_embedder.freqs\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_weight\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_bias\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.weight\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.bias\".\n",
      "Loading pretrained parameter \"readout.1.weight\".\n",
      "Loading pretrained parameter \"readout.1.bias\".\n",
      "Loading pretrained parameter \"readout.4.weight\".\n",
      "Loading pretrained parameter \"readout.4.bias\".\n",
      "Moving model to cuda\n",
      "Creating protein model\n",
      "MoleculeModel(\n",
      "  (softplus): Softplus(beta=1.0, threshold=20.0)\n",
      "  (encoder): MPN(\n",
      "    (encoder): ModuleList(\n",
      "      (0): MPNEncoder(\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (act_func): ReLU()\n",
      "        (W_i): Linear(in_features=147, out_features=300, bias=False)\n",
      "        (W_h): Linear(in_features=300, out_features=300, bias=False)\n",
      "        (W_o): Linear(in_features=433, out_features=300, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (seq_embedder): Embedding(21, 36, padding_idx=20)\n",
      "  (rotary_embedder): RotaryEmbedding()\n",
      "  (multihead_attn): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=36, out_features=36, bias=True)\n",
      "  )\n",
      "  (attentive_pooler): AttentivePooling(\n",
      "    (linear1): Linear(in_features=1316, out_features=1316, bias=True)\n",
      "    (tanh): Tanh()\n",
      "    (linear2): Linear(in_features=1316, out_features=1, bias=True)\n",
      "    (softmax): Softmax(dim=1)\n",
      "  )\n",
      "  (readout): Sequential(\n",
      "    (0): Dropout(p=0.0, inplace=False)\n",
      "    (1): Linear(in_features=1616, out_features=300, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.0, inplace=False)\n",
      "    (4): Linear(in_features=300, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
      "Loading pretrained parameter \"seq_embedder.weight\".\n",
      "Loading pretrained parameter \"rotary_embedder.freqs\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_weight\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_bias\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.weight\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.bias\".\n",
      "Loading pretrained parameter \"readout.1.weight\".\n",
      "Loading pretrained parameter \"readout.1.bias\".\n",
      "Loading pretrained parameter \"readout.4.weight\".\n",
      "Loading pretrained parameter \"readout.4.bias\".\n",
      "Moving model to cuda\n",
      "Creating protein model\n",
      "MoleculeModel(\n",
      "  (softplus): Softplus(beta=1.0, threshold=20.0)\n",
      "  (encoder): MPN(\n",
      "    (encoder): ModuleList(\n",
      "      (0): MPNEncoder(\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (act_func): ReLU()\n",
      "        (W_i): Linear(in_features=147, out_features=300, bias=False)\n",
      "        (W_h): Linear(in_features=300, out_features=300, bias=False)\n",
      "        (W_o): Linear(in_features=433, out_features=300, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (seq_embedder): Embedding(21, 36, padding_idx=20)\n",
      "  (rotary_embedder): RotaryEmbedding()\n",
      "  (multihead_attn): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=36, out_features=36, bias=True)\n",
      "  )\n",
      "  (attentive_pooler): AttentivePooling(\n",
      "    (linear1): Linear(in_features=1316, out_features=1316, bias=True)\n",
      "    (tanh): Tanh()\n",
      "    (linear2): Linear(in_features=1316, out_features=1, bias=True)\n",
      "    (softmax): Softmax(dim=1)\n",
      "  )\n",
      "  (readout): Sequential(\n",
      "    (0): Dropout(p=0.0, inplace=False)\n",
      "    (1): Linear(in_features=1616, out_features=300, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.0, inplace=False)\n",
      "    (4): Linear(in_features=300, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
      "Loading pretrained parameter \"seq_embedder.weight\".\n",
      "Loading pretrained parameter \"rotary_embedder.freqs\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_weight\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_bias\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.weight\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.bias\".\n",
      "Loading pretrained parameter \"readout.1.weight\".\n",
      "Loading pretrained parameter \"readout.1.bias\".\n",
      "Loading pretrained parameter \"readout.4.weight\".\n",
      "Loading pretrained parameter \"readout.4.bias\".\n",
      "Moving model to cuda\n",
      "Creating protein model\n",
      "MoleculeModel(\n",
      "  (softplus): Softplus(beta=1.0, threshold=20.0)\n",
      "  (encoder): MPN(\n",
      "    (encoder): ModuleList(\n",
      "      (0): MPNEncoder(\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (act_func): ReLU()\n",
      "        (W_i): Linear(in_features=147, out_features=300, bias=False)\n",
      "        (W_h): Linear(in_features=300, out_features=300, bias=False)\n",
      "        (W_o): Linear(in_features=433, out_features=300, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (seq_embedder): Embedding(21, 36, padding_idx=20)\n",
      "  (rotary_embedder): RotaryEmbedding()\n",
      "  (multihead_attn): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=36, out_features=36, bias=True)\n",
      "  )\n",
      "  (attentive_pooler): AttentivePooling(\n",
      "    (linear1): Linear(in_features=1316, out_features=1316, bias=True)\n",
      "    (tanh): Tanh()\n",
      "    (linear2): Linear(in_features=1316, out_features=1, bias=True)\n",
      "    (softmax): Softmax(dim=1)\n",
      "  )\n",
      "  (readout): Sequential(\n",
      "    (0): Dropout(p=0.0, inplace=False)\n",
      "    (1): Linear(in_features=1616, out_features=300, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.0, inplace=False)\n",
      "    (4): Linear(in_features=300, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
      "Loading pretrained parameter \"seq_embedder.weight\".\n",
      "Loading pretrained parameter \"rotary_embedder.freqs\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_weight\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_bias\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.weight\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.bias\".\n",
      "Loading pretrained parameter \"readout.1.weight\".\n",
      "Loading pretrained parameter \"readout.1.bias\".\n",
      "Loading pretrained parameter \"readout.4.weight\".\n",
      "Loading pretrained parameter \"readout.4.bias\".\n",
      "Moving model to cuda\n",
      "Creating protein model\n",
      "MoleculeModel(\n",
      "  (softplus): Softplus(beta=1.0, threshold=20.0)\n",
      "  (encoder): MPN(\n",
      "    (encoder): ModuleList(\n",
      "      (0): MPNEncoder(\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (act_func): ReLU()\n",
      "        (W_i): Linear(in_features=147, out_features=300, bias=False)\n",
      "        (W_h): Linear(in_features=300, out_features=300, bias=False)\n",
      "        (W_o): Linear(in_features=433, out_features=300, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (seq_embedder): Embedding(21, 36, padding_idx=20)\n",
      "  (rotary_embedder): RotaryEmbedding()\n",
      "  (multihead_attn): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=36, out_features=36, bias=True)\n",
      "  )\n",
      "  (attentive_pooler): AttentivePooling(\n",
      "    (linear1): Linear(in_features=1316, out_features=1316, bias=True)\n",
      "    (tanh): Tanh()\n",
      "    (linear2): Linear(in_features=1316, out_features=1, bias=True)\n",
      "    (softmax): Softmax(dim=1)\n",
      "  )\n",
      "  (readout): Sequential(\n",
      "    (0): Dropout(p=0.0, inplace=False)\n",
      "    (1): Linear(in_features=1616, out_features=300, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.0, inplace=False)\n",
      "    (4): Linear(in_features=300, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
      "Loading pretrained parameter \"seq_embedder.weight\".\n",
      "Loading pretrained parameter \"rotary_embedder.freqs\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_weight\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_bias\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.weight\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.bias\".\n",
      "Loading pretrained parameter \"readout.1.weight\".\n",
      "Loading pretrained parameter \"readout.1.bias\".\n",
      "Loading pretrained parameter \"readout.4.weight\".\n",
      "Loading pretrained parameter \"readout.4.bias\".\n",
      "Moving model to cuda\n",
      "Creating protein model\n",
      "MoleculeModel(\n",
      "  (softplus): Softplus(beta=1.0, threshold=20.0)\n",
      "  (encoder): MPN(\n",
      "    (encoder): ModuleList(\n",
      "      (0): MPNEncoder(\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (act_func): ReLU()\n",
      "        (W_i): Linear(in_features=147, out_features=300, bias=False)\n",
      "        (W_h): Linear(in_features=300, out_features=300, bias=False)\n",
      "        (W_o): Linear(in_features=433, out_features=300, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (seq_embedder): Embedding(21, 36, padding_idx=20)\n",
      "  (rotary_embedder): RotaryEmbedding()\n",
      "  (multihead_attn): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=36, out_features=36, bias=True)\n",
      "  )\n",
      "  (attentive_pooler): AttentivePooling(\n",
      "    (linear1): Linear(in_features=1316, out_features=1316, bias=True)\n",
      "    (tanh): Tanh()\n",
      "    (linear2): Linear(in_features=1316, out_features=1, bias=True)\n",
      "    (softmax): Softmax(dim=1)\n",
      "  )\n",
      "  (readout): Sequential(\n",
      "    (0): Dropout(p=0.0, inplace=False)\n",
      "    (1): Linear(in_features=1616, out_features=300, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.0, inplace=False)\n",
      "    (4): Linear(in_features=300, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
      "Loading pretrained parameter \"seq_embedder.weight\".\n",
      "Loading pretrained parameter \"rotary_embedder.freqs\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_weight\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_bias\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.weight\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.bias\".\n",
      "Loading pretrained parameter \"readout.1.weight\".\n",
      "Loading pretrained parameter \"readout.1.bias\".\n",
      "Loading pretrained parameter \"readout.4.weight\".\n",
      "Loading pretrained parameter \"readout.4.bias\".\n",
      "Moving model to cuda\n",
      "Creating protein model\n",
      "MoleculeModel(\n",
      "  (softplus): Softplus(beta=1.0, threshold=20.0)\n",
      "  (encoder): MPN(\n",
      "    (encoder): ModuleList(\n",
      "      (0): MPNEncoder(\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (act_func): ReLU()\n",
      "        (W_i): Linear(in_features=147, out_features=300, bias=False)\n",
      "        (W_h): Linear(in_features=300, out_features=300, bias=False)\n",
      "        (W_o): Linear(in_features=433, out_features=300, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (seq_embedder): Embedding(21, 36, padding_idx=20)\n",
      "  (rotary_embedder): RotaryEmbedding()\n",
      "  (multihead_attn): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=36, out_features=36, bias=True)\n",
      "  )\n",
      "  (attentive_pooler): AttentivePooling(\n",
      "    (linear1): Linear(in_features=1316, out_features=1316, bias=True)\n",
      "    (tanh): Tanh()\n",
      "    (linear2): Linear(in_features=1316, out_features=1, bias=True)\n",
      "    (softmax): Softmax(dim=1)\n",
      "  )\n",
      "  (readout): Sequential(\n",
      "    (0): Dropout(p=0.0, inplace=False)\n",
      "    (1): Linear(in_features=1616, out_features=300, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.0, inplace=False)\n",
      "    (4): Linear(in_features=300, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
      "Loading pretrained parameter \"seq_embedder.weight\".\n",
      "Loading pretrained parameter \"rotary_embedder.freqs\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_weight\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_bias\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.weight\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.bias\".\n",
      "Loading pretrained parameter \"readout.1.weight\".\n",
      "Loading pretrained parameter \"readout.1.bias\".\n",
      "Loading pretrained parameter \"readout.4.weight\".\n",
      "Loading pretrained parameter \"readout.4.bias\".\n",
      "Moving model to cuda\n",
      "Creating protein model\n",
      "MoleculeModel(\n",
      "  (softplus): Softplus(beta=1.0, threshold=20.0)\n",
      "  (encoder): MPN(\n",
      "    (encoder): ModuleList(\n",
      "      (0): MPNEncoder(\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (act_func): ReLU()\n",
      "        (W_i): Linear(in_features=147, out_features=300, bias=False)\n",
      "        (W_h): Linear(in_features=300, out_features=300, bias=False)\n",
      "        (W_o): Linear(in_features=433, out_features=300, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (seq_embedder): Embedding(21, 36, padding_idx=20)\n",
      "  (rotary_embedder): RotaryEmbedding()\n",
      "  (multihead_attn): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=36, out_features=36, bias=True)\n",
      "  )\n",
      "  (attentive_pooler): AttentivePooling(\n",
      "    (linear1): Linear(in_features=1316, out_features=1316, bias=True)\n",
      "    (tanh): Tanh()\n",
      "    (linear2): Linear(in_features=1316, out_features=1, bias=True)\n",
      "    (softmax): Softmax(dim=1)\n",
      "  )\n",
      "  (readout): Sequential(\n",
      "    (0): Dropout(p=0.0, inplace=False)\n",
      "    (1): Linear(in_features=1616, out_features=300, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.0, inplace=False)\n",
      "    (4): Linear(in_features=300, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
      "Loading pretrained parameter \"seq_embedder.weight\".\n",
      "Loading pretrained parameter \"rotary_embedder.freqs\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_weight\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_bias\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.weight\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.bias\".\n",
      "Loading pretrained parameter \"readout.1.weight\".\n",
      "Loading pretrained parameter \"readout.1.bias\".\n",
      "Loading pretrained parameter \"readout.4.weight\".\n",
      "Loading pretrained parameter \"readout.4.bias\".\n",
      "Moving model to cuda\n",
      "Creating protein model\n",
      "MoleculeModel(\n",
      "  (softplus): Softplus(beta=1.0, threshold=20.0)\n",
      "  (encoder): MPN(\n",
      "    (encoder): ModuleList(\n",
      "      (0): MPNEncoder(\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (act_func): ReLU()\n",
      "        (W_i): Linear(in_features=147, out_features=300, bias=False)\n",
      "        (W_h): Linear(in_features=300, out_features=300, bias=False)\n",
      "        (W_o): Linear(in_features=433, out_features=300, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (seq_embedder): Embedding(21, 36, padding_idx=20)\n",
      "  (rotary_embedder): RotaryEmbedding()\n",
      "  (multihead_attn): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=36, out_features=36, bias=True)\n",
      "  )\n",
      "  (attentive_pooler): AttentivePooling(\n",
      "    (linear1): Linear(in_features=1316, out_features=1316, bias=True)\n",
      "    (tanh): Tanh()\n",
      "    (linear2): Linear(in_features=1316, out_features=1, bias=True)\n",
      "    (softmax): Softmax(dim=1)\n",
      "  )\n",
      "  (readout): Sequential(\n",
      "    (0): Dropout(p=0.0, inplace=False)\n",
      "    (1): Linear(in_features=1616, out_features=300, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.0, inplace=False)\n",
      "    (4): Linear(in_features=300, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
      "Loading pretrained parameter \"seq_embedder.weight\".\n",
      "Loading pretrained parameter \"rotary_embedder.freqs\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_weight\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_bias\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.weight\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.bias\".\n",
      "Loading pretrained parameter \"readout.1.weight\".\n",
      "Loading pretrained parameter \"readout.1.bias\".\n",
      "Loading pretrained parameter \"readout.4.weight\".\n",
      "Loading pretrained parameter \"readout.4.bias\".\n",
      "Moving model to cuda\n",
      "Creating protein model\n",
      "MoleculeModel(\n",
      "  (softplus): Softplus(beta=1.0, threshold=20.0)\n",
      "  (encoder): MPN(\n",
      "    (encoder): ModuleList(\n",
      "      (0): MPNEncoder(\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (act_func): ReLU()\n",
      "        (W_i): Linear(in_features=147, out_features=300, bias=False)\n",
      "        (W_h): Linear(in_features=300, out_features=300, bias=False)\n",
      "        (W_o): Linear(in_features=433, out_features=300, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (seq_embedder): Embedding(21, 36, padding_idx=20)\n",
      "  (rotary_embedder): RotaryEmbedding()\n",
      "  (multihead_attn): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=36, out_features=36, bias=True)\n",
      "  )\n",
      "  (attentive_pooler): AttentivePooling(\n",
      "    (linear1): Linear(in_features=1316, out_features=1316, bias=True)\n",
      "    (tanh): Tanh()\n",
      "    (linear2): Linear(in_features=1316, out_features=1, bias=True)\n",
      "    (softmax): Softmax(dim=1)\n",
      "  )\n",
      "  (readout): Sequential(\n",
      "    (0): Dropout(p=0.0, inplace=False)\n",
      "    (1): Linear(in_features=1616, out_features=300, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.0, inplace=False)\n",
      "    (4): Linear(in_features=300, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
      "Loading pretrained parameter \"seq_embedder.weight\".\n",
      "Loading pretrained parameter \"rotary_embedder.freqs\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_weight\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_bias\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.weight\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.bias\".\n",
      "Loading pretrained parameter \"readout.1.weight\".\n",
      "Loading pretrained parameter \"readout.1.bias\".\n",
      "Loading pretrained parameter \"readout.4.weight\".\n",
      "Loading pretrained parameter \"readout.4.bias\".\n",
      "Moving model to cuda\n",
      "Saving predictions to ../pipeline_entry_kcat_input_output.csv\n",
      "Elapsed time = 0:00:06\n",
      "Prediction completed.\n",
      "Output saved to results/ pipeline_entry_kcat_input_output.csv\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xuvin/.conda/envs/catpred/lib/python3.9/site-packages/rotary_embedding_torch/rotary_embedding_torch.py:35: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  @autocast(enabled = False)\n",
      "/home/xuvin/.conda/envs/catpred/lib/python3.9/site-packages/rotary_embedding_torch/rotary_embedding_torch.py:268: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  @autocast(enabled = False)\n",
      "\n",
      "0it [00:00, ?it/s]/home/xuvin/py_repos/wccm/src/catpred_pipeline/catpred/catpred/data/cache_utils.py:95: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(str(entry_path))\n",
      "\n",
      "4it [00:00, 53.81it/s]\n",
      "\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 30393.51it/s]\n",
      "\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 841.76it/s]\n",
      "\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]/home/xuvin/py_repos/wccm/src/catpred_pipeline/catpred/catpred/utils.py:113: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(path, map_location=lambda storage, loc: storage)\n",
      "/home/xuvin/py_repos/wccm/src/catpred_pipeline/catpred/catpred/utils.py:446: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(path, map_location=lambda storage, loc: storage)\n",
      "\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.37s/it]\u001b[A\n",
      "\n",
      "                                             \u001b[A\n",
      " 10%|█         | 1/10 [00:02<00:26,  2.99s/it]/home/xuvin/py_repos/wccm/src/catpred_pipeline/catpred/catpred/utils.py:113: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(path, map_location=lambda storage, loc: storage)\n",
      "/home/xuvin/py_repos/wccm/src/catpred_pipeline/catpred/catpred/utils.py:446: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(path, map_location=lambda storage, loc: storage)\n",
      "\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "                                     \u001b[A\n",
      " 20%|██        | 2/10 [00:03<00:11,  1.41s/it]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "                                     \u001b[A\n",
      " 30%|███       | 3/10 [00:03<00:06,  1.04it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "                                     \u001b[A\n",
      " 40%|████      | 4/10 [00:04<00:04,  1.42it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "                                     \u001b[A\n",
      " 50%|█████     | 5/10 [00:04<00:02,  1.80it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "                                     \u001b[A\n",
      " 60%|██████    | 6/10 [00:04<00:01,  2.19it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "                                     \u001b[A\n",
      " 70%|███████   | 7/10 [00:05<00:01,  2.18it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "                                     \u001b[A\n",
      " 80%|████████  | 8/10 [00:05<00:00,  2.46it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "                                     \u001b[A\n",
      " 90%|█████████ | 9/10 [00:05<00:00,  2.73it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "                                     \u001b[A\n",
      "100%|██████████| 10/10 [00:05<00:00,  2.97it/s]\n",
      "100%|██████████| 10/10 [00:05<00:00,  1.69it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/xuvin/py_repos/wccm/src/catpred_pipeline/catpred\n",
      "Predicting.. This will take a while..\n",
      "calculating protein embed only on cpu\n",
      "Loading training args\n",
      "Loading models\n",
      "Setting molecule featurization parameters to default.\n",
      "Loading data\n",
      "Validating SMILES\n",
      "Test size = 4\n",
      "Creating protein model\n",
      "MoleculeModel(\n",
      "  (softplus): Softplus(beta=1.0, threshold=20.0)\n",
      "  (encoder): MPN(\n",
      "    (encoder): ModuleList(\n",
      "      (0): MPNEncoder(\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (act_func): ReLU()\n",
      "        (W_i): Linear(in_features=147, out_features=300, bias=False)\n",
      "        (W_h): Linear(in_features=300, out_features=300, bias=False)\n",
      "        (W_o): Linear(in_features=433, out_features=300, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (seq_embedder): Embedding(21, 36, padding_idx=20)\n",
      "  (rotary_embedder): RotaryEmbedding()\n",
      "  (multihead_attn): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=36, out_features=36, bias=True)\n",
      "  )\n",
      "  (attentive_pooler): AttentivePooling(\n",
      "    (linear1): Linear(in_features=1316, out_features=1316, bias=True)\n",
      "    (tanh): Tanh()\n",
      "    (linear2): Linear(in_features=1316, out_features=1, bias=True)\n",
      "    (softmax): Softmax(dim=1)\n",
      "  )\n",
      "  (readout): Sequential(\n",
      "    (0): Dropout(p=0.0, inplace=False)\n",
      "    (1): Linear(in_features=1616, out_features=300, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.0, inplace=False)\n",
      "    (4): Linear(in_features=300, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
      "Loading pretrained parameter \"seq_embedder.weight\".\n",
      "Loading pretrained parameter \"rotary_embedder.freqs\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_weight\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_bias\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.weight\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.bias\".\n",
      "Loading pretrained parameter \"readout.1.weight\".\n",
      "Loading pretrained parameter \"readout.1.bias\".\n",
      "Loading pretrained parameter \"readout.4.weight\".\n",
      "Loading pretrained parameter \"readout.4.bias\".\n",
      "Moving model to cuda\n",
      "Creating protein model\n",
      "MoleculeModel(\n",
      "  (softplus): Softplus(beta=1.0, threshold=20.0)\n",
      "  (encoder): MPN(\n",
      "    (encoder): ModuleList(\n",
      "      (0): MPNEncoder(\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (act_func): ReLU()\n",
      "        (W_i): Linear(in_features=147, out_features=300, bias=False)\n",
      "        (W_h): Linear(in_features=300, out_features=300, bias=False)\n",
      "        (W_o): Linear(in_features=433, out_features=300, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (seq_embedder): Embedding(21, 36, padding_idx=20)\n",
      "  (rotary_embedder): RotaryEmbedding()\n",
      "  (multihead_attn): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=36, out_features=36, bias=True)\n",
      "  )\n",
      "  (attentive_pooler): AttentivePooling(\n",
      "    (linear1): Linear(in_features=1316, out_features=1316, bias=True)\n",
      "    (tanh): Tanh()\n",
      "    (linear2): Linear(in_features=1316, out_features=1, bias=True)\n",
      "    (softmax): Softmax(dim=1)\n",
      "  )\n",
      "  (readout): Sequential(\n",
      "    (0): Dropout(p=0.0, inplace=False)\n",
      "    (1): Linear(in_features=1616, out_features=300, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.0, inplace=False)\n",
      "    (4): Linear(in_features=300, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
      "Loading pretrained parameter \"seq_embedder.weight\".\n",
      "Loading pretrained parameter \"rotary_embedder.freqs\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_weight\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_bias\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.weight\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.bias\".\n",
      "Loading pretrained parameter \"readout.1.weight\".\n",
      "Loading pretrained parameter \"readout.1.bias\".\n",
      "Loading pretrained parameter \"readout.4.weight\".\n",
      "Loading pretrained parameter \"readout.4.bias\".\n",
      "Moving model to cuda\n",
      "Creating protein model\n",
      "MoleculeModel(\n",
      "  (softplus): Softplus(beta=1.0, threshold=20.0)\n",
      "  (encoder): MPN(\n",
      "    (encoder): ModuleList(\n",
      "      (0): MPNEncoder(\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (act_func): ReLU()\n",
      "        (W_i): Linear(in_features=147, out_features=300, bias=False)\n",
      "        (W_h): Linear(in_features=300, out_features=300, bias=False)\n",
      "        (W_o): Linear(in_features=433, out_features=300, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (seq_embedder): Embedding(21, 36, padding_idx=20)\n",
      "  (rotary_embedder): RotaryEmbedding()\n",
      "  (multihead_attn): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=36, out_features=36, bias=True)\n",
      "  )\n",
      "  (attentive_pooler): AttentivePooling(\n",
      "    (linear1): Linear(in_features=1316, out_features=1316, bias=True)\n",
      "    (tanh): Tanh()\n",
      "    (linear2): Linear(in_features=1316, out_features=1, bias=True)\n",
      "    (softmax): Softmax(dim=1)\n",
      "  )\n",
      "  (readout): Sequential(\n",
      "    (0): Dropout(p=0.0, inplace=False)\n",
      "    (1): Linear(in_features=1616, out_features=300, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.0, inplace=False)\n",
      "    (4): Linear(in_features=300, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
      "Loading pretrained parameter \"seq_embedder.weight\".\n",
      "Loading pretrained parameter \"rotary_embedder.freqs\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_weight\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_bias\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.weight\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.bias\".\n",
      "Loading pretrained parameter \"readout.1.weight\".\n",
      "Loading pretrained parameter \"readout.1.bias\".\n",
      "Loading pretrained parameter \"readout.4.weight\".\n",
      "Loading pretrained parameter \"readout.4.bias\".\n",
      "Moving model to cuda\n",
      "Creating protein model\n",
      "MoleculeModel(\n",
      "  (softplus): Softplus(beta=1.0, threshold=20.0)\n",
      "  (encoder): MPN(\n",
      "    (encoder): ModuleList(\n",
      "      (0): MPNEncoder(\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (act_func): ReLU()\n",
      "        (W_i): Linear(in_features=147, out_features=300, bias=False)\n",
      "        (W_h): Linear(in_features=300, out_features=300, bias=False)\n",
      "        (W_o): Linear(in_features=433, out_features=300, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (seq_embedder): Embedding(21, 36, padding_idx=20)\n",
      "  (rotary_embedder): RotaryEmbedding()\n",
      "  (multihead_attn): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=36, out_features=36, bias=True)\n",
      "  )\n",
      "  (attentive_pooler): AttentivePooling(\n",
      "    (linear1): Linear(in_features=1316, out_features=1316, bias=True)\n",
      "    (tanh): Tanh()\n",
      "    (linear2): Linear(in_features=1316, out_features=1, bias=True)\n",
      "    (softmax): Softmax(dim=1)\n",
      "  )\n",
      "  (readout): Sequential(\n",
      "    (0): Dropout(p=0.0, inplace=False)\n",
      "    (1): Linear(in_features=1616, out_features=300, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.0, inplace=False)\n",
      "    (4): Linear(in_features=300, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
      "Loading pretrained parameter \"seq_embedder.weight\".\n",
      "Loading pretrained parameter \"rotary_embedder.freqs\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_weight\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_bias\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.weight\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.bias\".\n",
      "Loading pretrained parameter \"readout.1.weight\".\n",
      "Loading pretrained parameter \"readout.1.bias\".\n",
      "Loading pretrained parameter \"readout.4.weight\".\n",
      "Loading pretrained parameter \"readout.4.bias\".\n",
      "Moving model to cuda\n",
      "Creating protein model\n",
      "MoleculeModel(\n",
      "  (softplus): Softplus(beta=1.0, threshold=20.0)\n",
      "  (encoder): MPN(\n",
      "    (encoder): ModuleList(\n",
      "      (0): MPNEncoder(\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (act_func): ReLU()\n",
      "        (W_i): Linear(in_features=147, out_features=300, bias=False)\n",
      "        (W_h): Linear(in_features=300, out_features=300, bias=False)\n",
      "        (W_o): Linear(in_features=433, out_features=300, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (seq_embedder): Embedding(21, 36, padding_idx=20)\n",
      "  (rotary_embedder): RotaryEmbedding()\n",
      "  (multihead_attn): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=36, out_features=36, bias=True)\n",
      "  )\n",
      "  (attentive_pooler): AttentivePooling(\n",
      "    (linear1): Linear(in_features=1316, out_features=1316, bias=True)\n",
      "    (tanh): Tanh()\n",
      "    (linear2): Linear(in_features=1316, out_features=1, bias=True)\n",
      "    (softmax): Softmax(dim=1)\n",
      "  )\n",
      "  (readout): Sequential(\n",
      "    (0): Dropout(p=0.0, inplace=False)\n",
      "    (1): Linear(in_features=1616, out_features=300, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.0, inplace=False)\n",
      "    (4): Linear(in_features=300, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
      "Loading pretrained parameter \"seq_embedder.weight\".\n",
      "Loading pretrained parameter \"rotary_embedder.freqs\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_weight\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_bias\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.weight\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.bias\".\n",
      "Loading pretrained parameter \"readout.1.weight\".\n",
      "Loading pretrained parameter \"readout.1.bias\".\n",
      "Loading pretrained parameter \"readout.4.weight\".\n",
      "Loading pretrained parameter \"readout.4.bias\".\n",
      "Moving model to cuda\n",
      "Creating protein model\n",
      "MoleculeModel(\n",
      "  (softplus): Softplus(beta=1.0, threshold=20.0)\n",
      "  (encoder): MPN(\n",
      "    (encoder): ModuleList(\n",
      "      (0): MPNEncoder(\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (act_func): ReLU()\n",
      "        (W_i): Linear(in_features=147, out_features=300, bias=False)\n",
      "        (W_h): Linear(in_features=300, out_features=300, bias=False)\n",
      "        (W_o): Linear(in_features=433, out_features=300, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (seq_embedder): Embedding(21, 36, padding_idx=20)\n",
      "  (rotary_embedder): RotaryEmbedding()\n",
      "  (multihead_attn): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=36, out_features=36, bias=True)\n",
      "  )\n",
      "  (attentive_pooler): AttentivePooling(\n",
      "    (linear1): Linear(in_features=1316, out_features=1316, bias=True)\n",
      "    (tanh): Tanh()\n",
      "    (linear2): Linear(in_features=1316, out_features=1, bias=True)\n",
      "    (softmax): Softmax(dim=1)\n",
      "  )\n",
      "  (readout): Sequential(\n",
      "    (0): Dropout(p=0.0, inplace=False)\n",
      "    (1): Linear(in_features=1616, out_features=300, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.0, inplace=False)\n",
      "    (4): Linear(in_features=300, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
      "Loading pretrained parameter \"seq_embedder.weight\".\n",
      "Loading pretrained parameter \"rotary_embedder.freqs\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_weight\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_bias\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.weight\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.bias\".\n",
      "Loading pretrained parameter \"readout.1.weight\".\n",
      "Loading pretrained parameter \"readout.1.bias\".\n",
      "Loading pretrained parameter \"readout.4.weight\".\n",
      "Loading pretrained parameter \"readout.4.bias\".\n",
      "Moving model to cuda\n",
      "Creating protein model\n",
      "MoleculeModel(\n",
      "  (softplus): Softplus(beta=1.0, threshold=20.0)\n",
      "  (encoder): MPN(\n",
      "    (encoder): ModuleList(\n",
      "      (0): MPNEncoder(\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (act_func): ReLU()\n",
      "        (W_i): Linear(in_features=147, out_features=300, bias=False)\n",
      "        (W_h): Linear(in_features=300, out_features=300, bias=False)\n",
      "        (W_o): Linear(in_features=433, out_features=300, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (seq_embedder): Embedding(21, 36, padding_idx=20)\n",
      "  (rotary_embedder): RotaryEmbedding()\n",
      "  (multihead_attn): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=36, out_features=36, bias=True)\n",
      "  )\n",
      "  (attentive_pooler): AttentivePooling(\n",
      "    (linear1): Linear(in_features=1316, out_features=1316, bias=True)\n",
      "    (tanh): Tanh()\n",
      "    (linear2): Linear(in_features=1316, out_features=1, bias=True)\n",
      "    (softmax): Softmax(dim=1)\n",
      "  )\n",
      "  (readout): Sequential(\n",
      "    (0): Dropout(p=0.0, inplace=False)\n",
      "    (1): Linear(in_features=1616, out_features=300, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.0, inplace=False)\n",
      "    (4): Linear(in_features=300, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
      "Loading pretrained parameter \"seq_embedder.weight\".\n",
      "Loading pretrained parameter \"rotary_embedder.freqs\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_weight\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_bias\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.weight\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.bias\".\n",
      "Loading pretrained parameter \"readout.1.weight\".\n",
      "Loading pretrained parameter \"readout.1.bias\".\n",
      "Loading pretrained parameter \"readout.4.weight\".\n",
      "Loading pretrained parameter \"readout.4.bias\".\n",
      "Moving model to cuda\n",
      "Creating protein model\n",
      "MoleculeModel(\n",
      "  (softplus): Softplus(beta=1.0, threshold=20.0)\n",
      "  (encoder): MPN(\n",
      "    (encoder): ModuleList(\n",
      "      (0): MPNEncoder(\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (act_func): ReLU()\n",
      "        (W_i): Linear(in_features=147, out_features=300, bias=False)\n",
      "        (W_h): Linear(in_features=300, out_features=300, bias=False)\n",
      "        (W_o): Linear(in_features=433, out_features=300, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (seq_embedder): Embedding(21, 36, padding_idx=20)\n",
      "  (rotary_embedder): RotaryEmbedding()\n",
      "  (multihead_attn): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=36, out_features=36, bias=True)\n",
      "  )\n",
      "  (attentive_pooler): AttentivePooling(\n",
      "    (linear1): Linear(in_features=1316, out_features=1316, bias=True)\n",
      "    (tanh): Tanh()\n",
      "    (linear2): Linear(in_features=1316, out_features=1, bias=True)\n",
      "    (softmax): Softmax(dim=1)\n",
      "  )\n",
      "  (readout): Sequential(\n",
      "    (0): Dropout(p=0.0, inplace=False)\n",
      "    (1): Linear(in_features=1616, out_features=300, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.0, inplace=False)\n",
      "    (4): Linear(in_features=300, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
      "Loading pretrained parameter \"seq_embedder.weight\".\n",
      "Loading pretrained parameter \"rotary_embedder.freqs\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_weight\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_bias\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.weight\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.bias\".\n",
      "Loading pretrained parameter \"readout.1.weight\".\n",
      "Loading pretrained parameter \"readout.1.bias\".\n",
      "Loading pretrained parameter \"readout.4.weight\".\n",
      "Loading pretrained parameter \"readout.4.bias\".\n",
      "Moving model to cuda\n",
      "Creating protein model\n",
      "MoleculeModel(\n",
      "  (softplus): Softplus(beta=1.0, threshold=20.0)\n",
      "  (encoder): MPN(\n",
      "    (encoder): ModuleList(\n",
      "      (0): MPNEncoder(\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (act_func): ReLU()\n",
      "        (W_i): Linear(in_features=147, out_features=300, bias=False)\n",
      "        (W_h): Linear(in_features=300, out_features=300, bias=False)\n",
      "        (W_o): Linear(in_features=433, out_features=300, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (seq_embedder): Embedding(21, 36, padding_idx=20)\n",
      "  (rotary_embedder): RotaryEmbedding()\n",
      "  (multihead_attn): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=36, out_features=36, bias=True)\n",
      "  )\n",
      "  (attentive_pooler): AttentivePooling(\n",
      "    (linear1): Linear(in_features=1316, out_features=1316, bias=True)\n",
      "    (tanh): Tanh()\n",
      "    (linear2): Linear(in_features=1316, out_features=1, bias=True)\n",
      "    (softmax): Softmax(dim=1)\n",
      "  )\n",
      "  (readout): Sequential(\n",
      "    (0): Dropout(p=0.0, inplace=False)\n",
      "    (1): Linear(in_features=1616, out_features=300, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.0, inplace=False)\n",
      "    (4): Linear(in_features=300, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
      "Loading pretrained parameter \"seq_embedder.weight\".\n",
      "Loading pretrained parameter \"rotary_embedder.freqs\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_weight\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_bias\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.weight\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.bias\".\n",
      "Loading pretrained parameter \"readout.1.weight\".\n",
      "Loading pretrained parameter \"readout.1.bias\".\n",
      "Loading pretrained parameter \"readout.4.weight\".\n",
      "Loading pretrained parameter \"readout.4.bias\".\n",
      "Moving model to cuda\n",
      "Creating protein model\n",
      "MoleculeModel(\n",
      "  (softplus): Softplus(beta=1.0, threshold=20.0)\n",
      "  (encoder): MPN(\n",
      "    (encoder): ModuleList(\n",
      "      (0): MPNEncoder(\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (act_func): ReLU()\n",
      "        (W_i): Linear(in_features=147, out_features=300, bias=False)\n",
      "        (W_h): Linear(in_features=300, out_features=300, bias=False)\n",
      "        (W_o): Linear(in_features=433, out_features=300, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (seq_embedder): Embedding(21, 36, padding_idx=20)\n",
      "  (rotary_embedder): RotaryEmbedding()\n",
      "  (multihead_attn): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=36, out_features=36, bias=True)\n",
      "  )\n",
      "  (attentive_pooler): AttentivePooling(\n",
      "    (linear1): Linear(in_features=1316, out_features=1316, bias=True)\n",
      "    (tanh): Tanh()\n",
      "    (linear2): Linear(in_features=1316, out_features=1, bias=True)\n",
      "    (softmax): Softmax(dim=1)\n",
      "  )\n",
      "  (readout): Sequential(\n",
      "    (0): Dropout(p=0.0, inplace=False)\n",
      "    (1): Linear(in_features=1616, out_features=300, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.0, inplace=False)\n",
      "    (4): Linear(in_features=300, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
      "Loading pretrained parameter \"seq_embedder.weight\".\n",
      "Loading pretrained parameter \"rotary_embedder.freqs\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_weight\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_bias\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.weight\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.bias\".\n",
      "Loading pretrained parameter \"readout.1.weight\".\n",
      "Loading pretrained parameter \"readout.1.bias\".\n",
      "Loading pretrained parameter \"readout.4.weight\".\n",
      "Loading pretrained parameter \"readout.4.bias\".\n",
      "Moving model to cuda\n",
      "Saving predictions to ../pipeline_entry_km_input_output.csv\n",
      "Elapsed time = 0:00:07\n",
      "Prediction completed.\n",
      "Output saved to results/ pipeline_entry_km_input_output.csv\n",
      "\n",
      "output |    |   Unnamed: 0.2 |   Unnamed: 0.1 |   Unnamed: 0 | substrate    | SMILES                                                                                                                    | sequence                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  | pdbpath   |   log10kcat_max |   log10kcat_max_mve_uncal_var |   log10kcat_max_model_0 |   log10kcat_max_model_1 |   log10kcat_max_model_2 |   log10kcat_max_model_3 |   log10kcat_max_model_4 |   log10kcat_max_model_5 |   log10kcat_max_model_6 |   log10kcat_max_model_7 |   log10kcat_max_model_8 |   log10kcat_max_model_9 |   Prediction_(s^(-1)) |   Prediction_log10 |   SD_total |   SD_aleatoric |   SD_epistemic |\n",
      "|---:|---------------:|---------------:|-------------:|:-------------|:--------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:----------|----------------:|------------------------------:|------------------------:|------------------------:|------------------------:|------------------------:|------------------------:|------------------------:|------------------------:|------------------------:|------------------------:|------------------------:|----------------------:|-------------------:|-----------:|---------------:|---------------:|\n",
      "|  0 |              0 |              0 |            0 | Ethanol      | CCO                                                                                                                       | MTKIANKYEVIDNVEKLEKALKRLREAQSVYATYTQEQVDKIFFEAAMAANKMRIPLAKMAVEETGMGVVEDKVIKNHYASEYIYNAYKNTKTCGVIEEDPAFGIKKIAEPLGVIAAVIPTTNPTSTAIFKTLIALKTRNAIIISPHPRAKNSTIEAAKIVLEAAVKAGAPEGIIGWIDVPSLELTNLVMREADVILATGGPGLVKAAYSSGKPAIGVGAGNTPAIIDDSADIVLAVNSIIHSKTFDNGMICASEQSVIVLDGVYKEVKKEFEKRGCYFLNEDETEKVRKTIIINGALNAKIVGQKAHTIANLAGFEVPETTKILIGEVTSVDISEEFAHEKLCPVLAMYRAKDFDDALDKAERLVADGGFGHTSSLYIDTVTQKEKLQKFSERMKTCRILVNTPSSQGGIGDLYNFKLAPSLTLGCGSWGGNSVSDNVGVKHLLNIKTVAERRENMLWFRTPEKIYIKRGCLPVALDELKNVMGKKKAFIVTDNFLYNNGYTKPITDKLDEMGIVHKTFFDVSPDPSLASAKAGAAEMLAFQPDTIIAVGGGSAMDAAKIMWVMYEHPEVDFMDMAMRFMDIRKRVYTFPKMGQKAYFIAIPTSAGTGSEVTPFAVITDEKTGIKYPLADYELLPDMAIVDADMMMNAPKGLTAASGIDALTHALEAYVSMLATDYTDSLALRAIKMIFEYLPRAYENGASDPVAREKMANAATIAGMAFANAFLGVCHSMAHKLGAFYHLPHGVANALMINEVIRFNSSEAPTKMGTFPQYDHPRTLERYAEIADYIGLKGKNNEEKVENLIKAIDELKEKVGIRKTIKDYDIDEKEFLDRLDEMVEQAFDDQCTGTNPRYPLMNEIRQMYLNAYYGGAKK | seq3.seq  |         2.3648  |                      0.438938 |                 1.89137 |                 1.95521 |                 1.94465 |                 2.91008 |                 3.54126 |                 1.37299 |                 2.68588 |                 2.59073 |                 1.90712 |                 2.84868 |               231.632 |            2.3648  |   0.662524 |       0.236251 |       0.61897  |\n",
      "|  1 |              1 |              1 |            1 | NAD+         | NC(=O)c1ccc[n+]([C@@H]2O[C@H](COP(=O)([O-])OP(=O)(O)OC[C@H]3O[C@@H](n4cnc5c(N)ncnc54)[C@H](O)[C@@H]3O)[C@@H](O)[C@H]2O)c1 | MTKIANKYEVIDNVEKLEKALKRLREAQSVYATYTQEQVDKIFFEAAMAANKMRIPLAKMAVEETGMGVVEDKVIKNHYASEYIYNAYKNTKTCGVIEEDPAFGIKKIAEPLGVIAAVIPTTNPTSTAIFKTLIALKTRNAIIISPHPRAKNSTIEAAKIVLEAAVKAGAPEGIIGWIDVPSLELTNLVMREADVILATGGPGLVKAAYSSGKPAIGVGAGNTPAIIDDSADIVLAVNSIIHSKTFDNGMICASEQSVIVLDGVYKEVKKEFEKRGCYFLNEDETEKVRKTIIINGALNAKIVGQKAHTIANLAGFEVPETTKILIGEVTSVDISEEFAHEKLCPVLAMYRAKDFDDALDKAERLVADGGFGHTSSLYIDTVTQKEKLQKFSERMKTCRILVNTPSSQGGIGDLYNFKLAPSLTLGCGSWGGNSVSDNVGVKHLLNIKTVAERRENMLWFRTPEKIYIKRGCLPVALDELKNVMGKKKAFIVTDNFLYNNGYTKPITDKLDEMGIVHKTFFDVSPDPSLASAKAGAAEMLAFQPDTIIAVGGGSAMDAAKIMWVMYEHPEVDFMDMAMRFMDIRKRVYTFPKMGQKAYFIAIPTSAGTGSEVTPFAVITDEKTGIKYPLADYELLPDMAIVDADMMMNAPKGLTAASGIDALTHALEAYVSMLATDYTDSLALRAIKMIFEYLPRAYENGASDPVAREKMANAATIAGMAFANAFLGVCHSMAHKLGAFYHLPHGVANALMINEVIRFNSSEAPTKMGTFPQYDHPRTLERYAEIADYIGLKGKNNEEKVENLIKAIDELKEKVGIRKTIKDYDIDEKEFLDRLDEMVEQAFDDQCTGTNPRYPLMNEIRQMYLNAYYGGAKK | seq3.seq  |         2.33442 |                      0.256028 |                 2.02525 |                 2.53369 |                 2.09414 |                 2.23674 |                 2.52491 |                 2.00514 |                 2.97708 |                 1.94275 |                 2.14095 |                 2.86351 |               215.981 |            2.33442 |   0.505992 |       0.364582 |       0.350867 |\n",
      "|  2 |              2 |              2 |            2 | Acetaldehyde | CC=O                                                                                                                      | MTKIANKYEVIDNVEKLEKALKRLREAQSVYATYTQEQVDKIFFEAAMAANKMRIPLAKMAVEETGMGVVEDKVIKNHYASEYIYNAYKNTKTCGVIEEDPAFGIKKIAEPLGVIAAVIPTTNPTSTAIFKTLIALKTRNAIIISPHPRAKNSTIEAAKIVLEAAVKAGAPEGIIGWIDVPSLELTNLVMREADVILATGGPGLVKAAYSSGKPAIGVGAGNTPAIIDDSADIVLAVNSIIHSKTFDNGMICASEQSVIVLDGVYKEVKKEFEKRGCYFLNEDETEKVRKTIIINGALNAKIVGQKAHTIANLAGFEVPETTKILIGEVTSVDISEEFAHEKLCPVLAMYRAKDFDDALDKAERLVADGGFGHTSSLYIDTVTQKEKLQKFSERMKTCRILVNTPSSQGGIGDLYNFKLAPSLTLGCGSWGGNSVSDNVGVKHLLNIKTVAERRENMLWFRTPEKIYIKRGCLPVALDELKNVMGKKKAFIVTDNFLYNNGYTKPITDKLDEMGIVHKTFFDVSPDPSLASAKAGAAEMLAFQPDTIIAVGGGSAMDAAKIMWVMYEHPEVDFMDMAMRFMDIRKRVYTFPKMGQKAYFIAIPTSAGTGSEVTPFAVITDEKTGIKYPLADYELLPDMAIVDADMMMNAPKGLTAASGIDALTHALEAYVSMLATDYTDSLALRAIKMIFEYLPRAYENGASDPVAREKMANAATIAGMAFANAFLGVCHSMAHKLGAFYHLPHGVANALMINEVIRFNSSEAPTKMGTFPQYDHPRTLERYAEIADYIGLKGKNNEEKVENLIKAIDELKEKVGIRKTIKDYDIDEKEFLDRLDEMVEQAFDDQCTGTNPRYPLMNEIRQMYLNAYYGGAKK | seq3.seq  |         2.50809 |                      0.375855 |                 2.03705 |                 2.75296 |                 1.82475 |                 2.28112 |                 3.61614 |                 1.8981  |                 3.24472 |                 2.64027 |                 2.087   |                 2.69881 |               322.175 |            2.50809 |   0.613071 |       0.241788 |       0.563378 |\n",
      "|  3 |              3 |              3 |            3 | NADH         | NC(=O)C1=CN([C@@H]2O[C@H](COP(=O)(O)OP(=O)(O)OC[C@H]3O[C@@H](n4cnc5c(N)ncnc54)[C@H](O)[C@@H]3O)[C@@H](O)[C@H]2O)C=CC1     | MTKIANKYEVIDNVEKLEKALKRLREAQSVYATYTQEQVDKIFFEAAMAANKMRIPLAKMAVEETGMGVVEDKVIKNHYASEYIYNAYKNTKTCGVIEEDPAFGIKKIAEPLGVIAAVIPTTNPTSTAIFKTLIALKTRNAIIISPHPRAKNSTIEAAKIVLEAAVKAGAPEGIIGWIDVPSLELTNLVMREADVILATGGPGLVKAAYSSGKPAIGVGAGNTPAIIDDSADIVLAVNSIIHSKTFDNGMICASEQSVIVLDGVYKEVKKEFEKRGCYFLNEDETEKVRKTIIINGALNAKIVGQKAHTIANLAGFEVPETTKILIGEVTSVDISEEFAHEKLCPVLAMYRAKDFDDALDKAERLVADGGFGHTSSLYIDTVTQKEKLQKFSERMKTCRILVNTPSSQGGIGDLYNFKLAPSLTLGCGSWGGNSVSDNVGVKHLLNIKTVAERRENMLWFRTPEKIYIKRGCLPVALDELKNVMGKKKAFIVTDNFLYNNGYTKPITDKLDEMGIVHKTFFDVSPDPSLASAKAGAAEMLAFQPDTIIAVGGGSAMDAAKIMWVMYEHPEVDFMDMAMRFMDIRKRVYTFPKMGQKAYFIAIPTSAGTGSEVTPFAVITDEKTGIKYPLADYELLPDMAIVDADMMMNAPKGLTAASGIDALTHALEAYVSMLATDYTDSLALRAIKMIFEYLPRAYENGASDPVAREKMANAATIAGMAFANAFLGVCHSMAHKLGAFYHLPHGVANALMINEVIRFNSSEAPTKMGTFPQYDHPRTLERYAEIADYIGLKGKNNEEKVENLIKAIDELKEKVGIRKTIKDYDIDEKEFLDRLDEMVEQAFDDQCTGTNPRYPLMNEIRQMYLNAYYGGAKK | seq3.seq  |         2.50715 |                      0.140201 |                 2.21552 |                 2.61805 |                 2.40163 |                 2.56587 |                 2.66256 |                 2.07491 |                 2.93619 |                 2.54513 |                 2.3512  |                 2.70044 |               321.477 |            2.50715 |   0.374434 |       0.288426 |       0.23877  |\n",
      "SMILES ['CCO', 'C1=CC(=C[N+](=C1)[C@H]2[C@@H]([C@@H]([C@H](O2)COP(=O)([O-])OP(=O)(O)OC[C@@H]3[C@H]([C@H]([C@@H](O3)N4C=NC5=C(N=CN=C54)N)O)O)O)O)C(=O)N', 'CC=O', 'C1C=CN(C=C1C(=O)N)[C@H]2[C@@H]([C@@H]([C@H](O2)COP(=O)(O)OP(=O)(O)OC[C@@H]3[C@H]([C@H]([C@@H](O3)N4C=NC5=C(N=CN=C54)N)O)O)O)O']\n",
      "smiles_list CCO ['C(=O)[O-]', 'CC(=O)SCCNC(=O)CCNC(=O)[C@@H](C(C)(C)COP(=O)(O)OP(=O)(O)OC[C@@H]1[C@H]([C@H]([C@@H](O1)N2C=NC3=C(N=CN=C32)N)O)OP(=O)(O)O)O', 'CC(=O)C(=O)[O-]', 'CC(C)(COP(=O)(O)OP(=O)(O)OC[C@@H]1[C@H]([C@H]([C@@H](O1)N2C=NC3=C(N=CN=C32)N)O)OP(=O)(O)O)[C@H](C(=O)NCCC(=O)NCCS)O', 'CC(=O)[O-]', 'C1C=CN(C=C1C(=O)N)[C@H]2[C@@H]([C@@H]([C@H](O2)COP(=O)(O)OP(=O)(O)OC[C@@H]3[C@H]([C@H]([C@@H](O3)N4C=NC5=C(N=CN=C54)N)O)O)O)O', 'CC=O', 'C1=CC(=C[N+](=C1)[C@H]2[C@@H]([C@@H]([C@H](O2)COP(=O)([O-])OP(=O)(O)OC[C@@H]3[C@H]([C@H]([C@@H](O3)N4C=NC5=C(N=CN=C54)N)O)O)O)O)C(=O)N', 'CCO']\n",
      "smiles_list C1=CC(=C[N+](=C1)[C@H]2[C@@H]([C@@H]([C@H](O2)COP(=O)([O-])OP(=O)(O)OC[C@@H]3[C@H]([C@H]([C@@H](O3)N4C=NC5=C(N=CN=C54)N)O)O)O)O)C(=O)N ['C(=O)[O-]', 'CC(=O)SCCNC(=O)CCNC(=O)[C@@H](C(C)(C)COP(=O)(O)OP(=O)(O)OC[C@@H]1[C@H]([C@H]([C@@H](O1)N2C=NC3=C(N=CN=C32)N)O)OP(=O)(O)O)O', 'CC(=O)C(=O)[O-]', 'CC(C)(COP(=O)(O)OP(=O)(O)OC[C@@H]1[C@H]([C@H]([C@@H](O1)N2C=NC3=C(N=CN=C32)N)O)OP(=O)(O)O)[C@H](C(=O)NCCC(=O)NCCS)O', 'CC(=O)[O-]', 'C1C=CN(C=C1C(=O)N)[C@H]2[C@@H]([C@@H]([C@H](O2)COP(=O)(O)OP(=O)(O)OC[C@@H]3[C@H]([C@H]([C@@H](O3)N4C=NC5=C(N=CN=C54)N)O)O)O)O', 'CC=O', 'C1=CC(=C[N+](=C1)[C@H]2[C@@H]([C@@H]([C@H](O2)COP(=O)([O-])OP(=O)(O)OC[C@@H]3[C@H]([C@H]([C@@H](O3)N4C=NC5=C(N=CN=C54)N)O)O)O)O)C(=O)N', 'CCO']\n",
      "smiles_list CC=O ['C(=O)[O-]', 'CC(=O)SCCNC(=O)CCNC(=O)[C@@H](C(C)(C)COP(=O)(O)OP(=O)(O)OC[C@@H]1[C@H]([C@H]([C@@H](O1)N2C=NC3=C(N=CN=C32)N)O)OP(=O)(O)O)O', 'CC(=O)C(=O)[O-]', 'CC(C)(COP(=O)(O)OP(=O)(O)OC[C@@H]1[C@H]([C@H]([C@@H](O1)N2C=NC3=C(N=CN=C32)N)O)OP(=O)(O)O)[C@H](C(=O)NCCC(=O)NCCS)O', 'CC(=O)[O-]', 'C1C=CN(C=C1C(=O)N)[C@H]2[C@@H]([C@@H]([C@H](O2)COP(=O)(O)OP(=O)(O)OC[C@@H]3[C@H]([C@H]([C@@H](O3)N4C=NC5=C(N=CN=C54)N)O)O)O)O', 'CC=O', 'C1=CC(=C[N+](=C1)[C@H]2[C@@H]([C@@H]([C@H](O2)COP(=O)([O-])OP(=O)(O)OC[C@@H]3[C@H]([C@H]([C@@H](O3)N4C=NC5=C(N=CN=C54)N)O)O)O)O)C(=O)N', 'CCO']\n",
      "smiles_list C1C=CN(C=C1C(=O)N)[C@H]2[C@@H]([C@@H]([C@H](O2)COP(=O)(O)OP(=O)(O)OC[C@@H]3[C@H]([C@H]([C@@H](O3)N4C=NC5=C(N=CN=C54)N)O)O)O)O ['C(=O)[O-]', 'CC(=O)SCCNC(=O)CCNC(=O)[C@@H](C(C)(C)COP(=O)(O)OP(=O)(O)OC[C@@H]1[C@H]([C@H]([C@@H](O1)N2C=NC3=C(N=CN=C32)N)O)OP(=O)(O)O)O', 'CC(=O)C(=O)[O-]', 'CC(C)(COP(=O)(O)OP(=O)(O)OC[C@@H]1[C@H]([C@H]([C@@H](O1)N2C=NC3=C(N=CN=C32)N)O)OP(=O)(O)O)[C@H](C(=O)NCCC(=O)NCCS)O', 'CC(=O)[O-]', 'C1C=CN(C=C1C(=O)N)[C@H]2[C@@H]([C@@H]([C@H](O2)COP(=O)(O)OP(=O)(O)OC[C@@H]3[C@H]([C@H]([C@@H](O3)N4C=NC5=C(N=CN=C54)N)O)O)O)O', 'CC=O', 'C1=CC(=C[N+](=C1)[C@H]2[C@@H]([C@@H]([C@H](O2)COP(=O)([O-])OP(=O)(O)OC[C@@H]3[C@H]([C@H]([C@@H](O3)N4C=NC5=C(N=CN=C54)N)O)O)O)O)C(=O)N', 'CCO']\n",
      "item {'substrates': ['Acetaldehyde', 'CoA', 'NAD+', 'Acetyl-CoA', 'NADH'], 'SMILES': ['CC=O', 'CC(C)(COP(=O)(O)OP(=O)(O)OC[C@@H]1[C@H]([C@H]([C@@H](O1)N2C=NC3=C(N=CN=C32)N)O)OP(=O)(O)O)[C@H](C(=O)NCCC(=O)NCCS)O', 'C1=CC(=C[N+](=C1)[C@H]2[C@@H]([C@@H]([C@H](O2)COP(=O)([O-])OP(=O)(O)OC[C@@H]3[C@H]([C@H]([C@@H](O3)N4C=NC5=C(N=CN=C54)N)O)O)O)O)C(=O)N', 'CC(=O)SCCNC(=O)CCNC(=O)[C@@H](C(C)(C)COP(=O)(O)OP(=O)(O)OC[C@@H]1[C@H]([C@H]([C@@H](O1)N2C=NC3=C(N=CN=C32)N)O)OP(=O)(O)O)O', 'C1C=CN(C=C1C(=O)N)[C@H]2[C@@H]([C@@H]([C@H](O2)COP(=O)(O)OP(=O)(O)OC[C@@H]3[C@H]([C@H]([C@@H](O3)N4C=NC5=C(N=CN=C54)N)O)O)O)O'], 'sequences': ['MSKRKVAIIGSGNIGTDLMIKILRHGQHLEMAVMVGIDPQSDGLARARRMGVATTHEGVIGLMNMPEFADIDIVFDATSAGAHVKNDAALREAKPDIRLIDLTPAAIGPYCVPVVNLEANVDQLNVNMVTCGGQATIPMVAAVSRVARVHYAEIIASIASKSAGPGTRANIDEFTETTSRAIEVVGGAAKGKAIIVLNPAEPPLMMRDTVYVLSDEASQDDIEASINEMAEAVQAYVPGYRLKQRVQFEVIPQDKPVNLPGVGQFSGLKTAVWLEVEGAAHYLPAYAGNLDIMTSSALATAEKMAQSLARKAGEAA', 'MSKRKVAIIGSGNIGTDLMIKILRHGQHLEMAVMVGIDPQSDGLARARRMGVATTHEGVIGLMNMPEFADIDIVFDATSAGAHVKNDAALREAKPDIRLIDLTPAAIGPYCVPVVNLEANVDQLNVNMVTCGGQATIPMVAAVSRVARVHYAEIIASIASKSAGPGTRANIDEFTETTSRAIEVVGGAAKGKAIIVLNPAEPPLMMRDTVYVLSDEASQDDIEASINEMAEAVQAYVPGYRLKQRVQFEVIPQDKPVNLPGVGQFSGLKTAVWLEVEGAAHYLPAYAGNLDIMTSSALATAEKMAQSLARKAGEAA', 'MSKRKVAIIGSGNIGTDLMIKILRHGQHLEMAVMVGIDPQSDGLARARRMGVATTHEGVIGLMNMPEFADIDIVFDATSAGAHVKNDAALREAKPDIRLIDLTPAAIGPYCVPVVNLEANVDQLNVNMVTCGGQATIPMVAAVSRVARVHYAEIIASIASKSAGPGTRANIDEFTETTSRAIEVVGGAAKGKAIIVLNPAEPPLMMRDTVYVLSDEASQDDIEASINEMAEAVQAYVPGYRLKQRVQFEVIPQDKPVNLPGVGQFSGLKTAVWLEVEGAAHYLPAYAGNLDIMTSSALATAEKMAQSLARKAGEAA', 'MSKRKVAIIGSGNIGTDLMIKILRHGQHLEMAVMVGIDPQSDGLARARRMGVATTHEGVIGLMNMPEFADIDIVFDATSAGAHVKNDAALREAKPDIRLIDLTPAAIGPYCVPVVNLEANVDQLNVNMVTCGGQATIPMVAAVSRVARVHYAEIIASIASKSAGPGTRANIDEFTETTSRAIEVVGGAAKGKAIIVLNPAEPPLMMRDTVYVLSDEASQDDIEASINEMAEAVQAYVPGYRLKQRVQFEVIPQDKPVNLPGVGQFSGLKTAVWLEVEGAAHYLPAYAGNLDIMTSSALATAEKMAQSLARKAGEAA', 'MSKRKVAIIGSGNIGTDLMIKILRHGQHLEMAVMVGIDPQSDGLARARRMGVATTHEGVIGLMNMPEFADIDIVFDATSAGAHVKNDAALREAKPDIRLIDLTPAAIGPYCVPVVNLEANVDQLNVNMVTCGGQATIPMVAAVSRVARVHYAEIIASIASKSAGPGTRANIDEFTETTSRAIEVVGGAAKGKAIIVLNPAEPPLMMRDTVYVLSDEASQDDIEASINEMAEAVQAYVPGYRLKQRVQFEVIPQDKPVNLPGVGQFSGLKTAVWLEVEGAAHYLPAYAGNLDIMTSSALATAEKMAQSLARKAGEAA'], 'pdb_paths': ['seq4.seq', 'seq4.seq', 'seq4.seq', 'seq4.seq', 'seq4.seq'], 'subs': ['CC=O', 'CC(C)(COP(=O)(O)OP(=O)(O)OC[C@@H]1[C@H]([C@H]([C@@H](O1)N2C=NC3=C(N=CN=C32)N)O)OP(=O)(O)O)[C@H](C(=O)NCCC(=O)NCCS)O', 'C1=CC(=C[N+](=C1)[C@H]2[C@@H]([C@@H]([C@H](O2)COP(=O)([O-])OP(=O)(O)OC[C@@H]3[C@H]([C@H]([C@@H](O3)N4C=NC5=C(N=CN=C54)N)O)O)O)O)C(=O)N'], 'stoich': [1, 1, 1, 1, 1], 'EC': '1.2.1.10'}\n",
      "starting R00228\n",
      "input |    | substrate    | SMILES                                                                                                                                 | sequence                                                                                                                                                                                                                                                                                                                     | pdbpath   |\n",
      "|---:|:-------------|:---------------------------------------------------------------------------------------------------------------------------------------|:-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:----------|\n",
      "|  0 | Acetaldehyde | CC=O                                                                                                                                   | MSKRKVAIIGSGNIGTDLMIKILRHGQHLEMAVMVGIDPQSDGLARARRMGVATTHEGVIGLMNMPEFADIDIVFDATSAGAHVKNDAALREAKPDIRLIDLTPAAIGPYCVPVVNLEANVDQLNVNMVTCGGQATIPMVAAVSRVARVHYAEIIASIASKSAGPGTRANIDEFTETTSRAIEVVGGAAKGKAIIVLNPAEPPLMMRDTVYVLSDEASQDDIEASINEMAEAVQAYVPGYRLKQRVQFEVIPQDKPVNLPGVGQFSGLKTAVWLEVEGAAHYLPAYAGNLDIMTSSALATAEKMAQSLARKAGEAA | seq4.seq  |\n",
      "|  1 | CoA          | CC(C)(COP(=O)(O)OP(=O)(O)OC[C@@H]1[C@H]([C@H]([C@@H](O1)N2C=NC3=C(N=CN=C32)N)O)OP(=O)(O)O)[C@H](C(=O)NCCC(=O)NCCS)O                    | MSKRKVAIIGSGNIGTDLMIKILRHGQHLEMAVMVGIDPQSDGLARARRMGVATTHEGVIGLMNMPEFADIDIVFDATSAGAHVKNDAALREAKPDIRLIDLTPAAIGPYCVPVVNLEANVDQLNVNMVTCGGQATIPMVAAVSRVARVHYAEIIASIASKSAGPGTRANIDEFTETTSRAIEVVGGAAKGKAIIVLNPAEPPLMMRDTVYVLSDEASQDDIEASINEMAEAVQAYVPGYRLKQRVQFEVIPQDKPVNLPGVGQFSGLKTAVWLEVEGAAHYLPAYAGNLDIMTSSALATAEKMAQSLARKAGEAA | seq4.seq  |\n",
      "|  2 | NAD+         | C1=CC(=C[N+](=C1)[C@H]2[C@@H]([C@@H]([C@H](O2)COP(=O)([O-])OP(=O)(O)OC[C@@H]3[C@H]([C@H]([C@@H](O3)N4C=NC5=C(N=CN=C54)N)O)O)O)O)C(=O)N | MSKRKVAIIGSGNIGTDLMIKILRHGQHLEMAVMVGIDPQSDGLARARRMGVATTHEGVIGLMNMPEFADIDIVFDATSAGAHVKNDAALREAKPDIRLIDLTPAAIGPYCVPVVNLEANVDQLNVNMVTCGGQATIPMVAAVSRVARVHYAEIIASIASKSAGPGTRANIDEFTETTSRAIEVVGGAAKGKAIIVLNPAEPPLMMRDTVYVLSDEASQDDIEASINEMAEAVQAYVPGYRLKQRVQFEVIPQDKPVNLPGVGQFSGLKTAVWLEVEGAAHYLPAYAGNLDIMTSSALATAEKMAQSLARKAGEAA | seq4.seq  |\n",
      "|  3 | Acetyl-CoA   | CC(=O)SCCNC(=O)CCNC(=O)[C@@H](C(C)(C)COP(=O)(O)OP(=O)(O)OC[C@@H]1[C@H]([C@H]([C@@H](O1)N2C=NC3=C(N=CN=C32)N)O)OP(=O)(O)O)O             | MSKRKVAIIGSGNIGTDLMIKILRHGQHLEMAVMVGIDPQSDGLARARRMGVATTHEGVIGLMNMPEFADIDIVFDATSAGAHVKNDAALREAKPDIRLIDLTPAAIGPYCVPVVNLEANVDQLNVNMVTCGGQATIPMVAAVSRVARVHYAEIIASIASKSAGPGTRANIDEFTETTSRAIEVVGGAAKGKAIIVLNPAEPPLMMRDTVYVLSDEASQDDIEASINEMAEAVQAYVPGYRLKQRVQFEVIPQDKPVNLPGVGQFSGLKTAVWLEVEGAAHYLPAYAGNLDIMTSSALATAEKMAQSLARKAGEAA | seq4.seq  |\n",
      "|  4 | NADH         | C1C=CN(C=C1C(=O)N)[C@H]2[C@@H]([C@@H]([C@H](O2)COP(=O)(O)OP(=O)(O)OC[C@@H]3[C@H]([C@H]([C@@H](O3)N4C=NC5=C(N=CN=C54)N)O)O)O)O          | MSKRKVAIIGSGNIGTDLMIKILRHGQHLEMAVMVGIDPQSDGLARARRMGVATTHEGVIGLMNMPEFADIDIVFDATSAGAHVKNDAALREAKPDIRLIDLTPAAIGPYCVPVVNLEANVDQLNVNMVTCGGQATIPMVAAVSRVARVHYAEIIASIASKSAGPGTRANIDEFTETTSRAIEVVGGAAKGKAIIVLNPAEPPLMMRDTVYVLSDEASQDDIEASINEMAEAVQAYVPGYRLKQRVQFEVIPQDKPVNLPGVGQFSGLKTAVWLEVEGAAHYLPAYAGNLDIMTSSALATAEKMAQSLARKAGEAA | seq4.seq  |\n",
      "input2 ['CC=O', 'CC(C)(COP(=O)(O)OP(=O)(O)OC[C@@H]1[C@H]([C@H]([C@@H](O1)N2C=NC3=C(N=CN=C32)N)O)OP(=O)(O)O)[C@H](C(=O)NCCC(=O)NCCS)O', 'C1=CC(=C[N+](=C1)[C@H]2[C@@H]([C@@H]([C@H](O2)COP(=O)([O-])OP(=O)(O)OC[C@@H]3[C@H]([C@H]([C@@H](O3)N4C=NC5=C(N=CN=C54)N)O)O)O)O)C(=O)N', 'CC(=O)SCCNC(=O)CCNC(=O)[C@@H](C(C)(C)COP(=O)(O)OP(=O)(O)OC[C@@H]1[C@H]([C@H]([C@@H](O1)N2C=NC3=C(N=CN=C32)N)O)OP(=O)(O)O)O', 'C1C=CN(C=C1C(=O)N)[C@H]2[C@@H]([C@@H]([C@H](O2)COP(=O)(O)OP(=O)(O)OC[C@@H]3[C@H]([C@H]([C@@H](O3)N4C=NC5=C(N=CN=C54)N)O)O)O)O']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xuvin/.conda/envs/catpred/lib/python3.9/site-packages/rotary_embedding_torch/rotary_embedding_torch.py:35: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  @autocast(enabled = False)\n",
      "/home/xuvin/.conda/envs/catpred/lib/python3.9/site-packages/rotary_embedding_torch/rotary_embedding_torch.py:268: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  @autocast(enabled = False)\n",
      "\n",
      "0it [00:00, ?it/s]/home/xuvin/py_repos/wccm/src/catpred_pipeline/catpred/catpred/data/cache_utils.py:95: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(str(entry_path))\n",
      "\n",
      "4it [00:00, 59.88it/s]\n",
      "\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 19373.23it/s]\n",
      "\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 1065.02it/s]\n",
      "\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]/home/xuvin/py_repos/wccm/src/catpred_pipeline/catpred/catpred/utils.py:113: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(path, map_location=lambda storage, loc: storage)\n",
      "/home/xuvin/py_repos/wccm/src/catpred_pipeline/catpred/catpred/utils.py:446: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(path, map_location=lambda storage, loc: storage)\n",
      "\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.26s/it]\u001b[A\n",
      "\n",
      "                                             \u001b[A\n",
      " 10%|█         | 1/10 [00:04<00:38,  4.26s/it]/home/xuvin/py_repos/wccm/src/catpred_pipeline/catpred/catpred/utils.py:113: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(path, map_location=lambda storage, loc: storage)\n",
      "/home/xuvin/py_repos/wccm/src/catpred_pipeline/catpred/catpred/utils.py:446: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(path, map_location=lambda storage, loc: storage)\n",
      "\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "                                     \u001b[A\n",
      " 20%|██        | 2/10 [00:04<00:15,  1.92s/it]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "                                     \u001b[A\n",
      " 30%|███       | 3/10 [00:04<00:08,  1.24s/it]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "                                     \u001b[A\n",
      " 40%|████      | 4/10 [00:05<00:05,  1.17it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "                                     \u001b[A\n",
      " 50%|█████     | 5/10 [00:05<00:03,  1.56it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "                                     \u001b[A\n",
      " 60%|██████    | 6/10 [00:05<00:02,  1.94it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "                                     \u001b[A\n",
      " 70%|███████   | 7/10 [00:06<00:01,  1.97it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "                                     \u001b[A\n",
      " 80%|████████  | 8/10 [00:06<00:00,  2.31it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "                                     \u001b[A\n",
      " 90%|█████████ | 9/10 [00:06<00:00,  2.63it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "                                     \u001b[A\n",
      "100%|██████████| 10/10 [00:07<00:00,  2.96it/s]\n",
      "100%|██████████| 10/10 [00:07<00:00,  1.42it/s]\n",
      "\n",
      "/home/xuvin/.conda/envs/catpred/lib/python3.9/site-packages/rotary_embedding_torch/rotary_embedding_torch.py:35: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  @autocast(enabled = False)\n",
      "/home/xuvin/.conda/envs/catpred/lib/python3.9/site-packages/rotary_embedding_torch/rotary_embedding_torch.py:268: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  @autocast(enabled = False)\n",
      "\n",
      "0it [00:00, ?it/s]/home/xuvin/py_repos/wccm/src/catpred_pipeline/catpred/catpred/data/cache_utils.py:95: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(str(entry_path))\n",
      "\n",
      "5it [00:00, 67.16it/s]\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 13074.51it/s]\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 1278.13it/s]\n",
      "\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]/home/xuvin/py_repos/wccm/src/catpred_pipeline/catpred/catpred/utils.py:113: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(path, map_location=lambda storage, loc: storage)\n",
      "/home/xuvin/py_repos/wccm/src/catpred_pipeline/catpred/catpred/utils.py:446: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(path, map_location=lambda storage, loc: storage)\n",
      "\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.39s/it]\u001b[A\n",
      "\n",
      "                                             \u001b[A\n",
      " 10%|█         | 1/10 [00:02<00:24,  2.71s/it]/home/xuvin/py_repos/wccm/src/catpred_pipeline/catpred/catpred/utils.py:113: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(path, map_location=lambda storage, loc: storage)\n",
      "/home/xuvin/py_repos/wccm/src/catpred_pipeline/catpred/catpred/utils.py:446: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(path, map_location=lambda storage, loc: storage)\n",
      "\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "                                     \u001b[A\n",
      " 20%|██        | 2/10 [00:02<00:10,  1.27s/it]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "                                     \u001b[A\n",
      " 30%|███       | 3/10 [00:03<00:05,  1.24it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "                                     \u001b[A\n",
      " 40%|████      | 4/10 [00:03<00:04,  1.47it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "                                     \u001b[A\n",
      " 50%|█████     | 5/10 [00:03<00:02,  1.94it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "                                     \u001b[A\n",
      " 60%|██████    | 6/10 [00:04<00:01,  2.37it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "                                     \u001b[A\n",
      " 70%|███████   | 7/10 [00:04<00:01,  2.75it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "                                     \u001b[A\n",
      " 80%|████████  | 8/10 [00:04<00:00,  2.49it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "                                     \u001b[A\n",
      " 90%|█████████ | 9/10 [00:05<00:00,  2.90it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "                                     \u001b[A\n",
      "100%|██████████| 10/10 [00:05<00:00,  3.04it/s]\n",
      "100%|██████████| 10/10 [00:05<00:00,  1.85it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/xuvin/py_repos/wccm/src/catpred_pipeline/catpred\n",
      "Predicting.. This will take a while..\n",
      "calculating protein embed only on cpu\n",
      "Loading training args\n",
      "Loading models\n",
      "Setting molecule featurization parameters to default.\n",
      "Loading data\n",
      "Validating SMILES\n",
      "Test size = 5\n",
      "Creating protein model\n",
      "MoleculeModel(\n",
      "  (softplus): Softplus(beta=1.0, threshold=20.0)\n",
      "  (encoder): MPN(\n",
      "    (encoder): ModuleList(\n",
      "      (0): MPNEncoder(\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (act_func): ReLU()\n",
      "        (W_i): Linear(in_features=147, out_features=300, bias=False)\n",
      "        (W_h): Linear(in_features=300, out_features=300, bias=False)\n",
      "        (W_o): Linear(in_features=433, out_features=300, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (seq_embedder): Embedding(21, 36, padding_idx=20)\n",
      "  (rotary_embedder): RotaryEmbedding()\n",
      "  (multihead_attn): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=36, out_features=36, bias=True)\n",
      "  )\n",
      "  (attentive_pooler): AttentivePooling(\n",
      "    (linear1): Linear(in_features=1316, out_features=1316, bias=True)\n",
      "    (tanh): Tanh()\n",
      "    (linear2): Linear(in_features=1316, out_features=1, bias=True)\n",
      "    (softmax): Softmax(dim=1)\n",
      "  )\n",
      "  (readout): Sequential(\n",
      "    (0): Dropout(p=0.0, inplace=False)\n",
      "    (1): Linear(in_features=1616, out_features=300, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.0, inplace=False)\n",
      "    (4): Linear(in_features=300, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
      "Loading pretrained parameter \"seq_embedder.weight\".\n",
      "Loading pretrained parameter \"rotary_embedder.freqs\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_weight\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_bias\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.weight\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.bias\".\n",
      "Loading pretrained parameter \"readout.1.weight\".\n",
      "Loading pretrained parameter \"readout.1.bias\".\n",
      "Loading pretrained parameter \"readout.4.weight\".\n",
      "Loading pretrained parameter \"readout.4.bias\".\n",
      "Moving model to cuda\n",
      "Creating protein model\n",
      "MoleculeModel(\n",
      "  (softplus): Softplus(beta=1.0, threshold=20.0)\n",
      "  (encoder): MPN(\n",
      "    (encoder): ModuleList(\n",
      "      (0): MPNEncoder(\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (act_func): ReLU()\n",
      "        (W_i): Linear(in_features=147, out_features=300, bias=False)\n",
      "        (W_h): Linear(in_features=300, out_features=300, bias=False)\n",
      "        (W_o): Linear(in_features=433, out_features=300, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (seq_embedder): Embedding(21, 36, padding_idx=20)\n",
      "  (rotary_embedder): RotaryEmbedding()\n",
      "  (multihead_attn): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=36, out_features=36, bias=True)\n",
      "  )\n",
      "  (attentive_pooler): AttentivePooling(\n",
      "    (linear1): Linear(in_features=1316, out_features=1316, bias=True)\n",
      "    (tanh): Tanh()\n",
      "    (linear2): Linear(in_features=1316, out_features=1, bias=True)\n",
      "    (softmax): Softmax(dim=1)\n",
      "  )\n",
      "  (readout): Sequential(\n",
      "    (0): Dropout(p=0.0, inplace=False)\n",
      "    (1): Linear(in_features=1616, out_features=300, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.0, inplace=False)\n",
      "    (4): Linear(in_features=300, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
      "Loading pretrained parameter \"seq_embedder.weight\".\n",
      "Loading pretrained parameter \"rotary_embedder.freqs\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_weight\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_bias\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.weight\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.bias\".\n",
      "Loading pretrained parameter \"readout.1.weight\".\n",
      "Loading pretrained parameter \"readout.1.bias\".\n",
      "Loading pretrained parameter \"readout.4.weight\".\n",
      "Loading pretrained parameter \"readout.4.bias\".\n",
      "Moving model to cuda\n",
      "Creating protein model\n",
      "MoleculeModel(\n",
      "  (softplus): Softplus(beta=1.0, threshold=20.0)\n",
      "  (encoder): MPN(\n",
      "    (encoder): ModuleList(\n",
      "      (0): MPNEncoder(\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (act_func): ReLU()\n",
      "        (W_i): Linear(in_features=147, out_features=300, bias=False)\n",
      "        (W_h): Linear(in_features=300, out_features=300, bias=False)\n",
      "        (W_o): Linear(in_features=433, out_features=300, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (seq_embedder): Embedding(21, 36, padding_idx=20)\n",
      "  (rotary_embedder): RotaryEmbedding()\n",
      "  (multihead_attn): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=36, out_features=36, bias=True)\n",
      "  )\n",
      "  (attentive_pooler): AttentivePooling(\n",
      "    (linear1): Linear(in_features=1316, out_features=1316, bias=True)\n",
      "    (tanh): Tanh()\n",
      "    (linear2): Linear(in_features=1316, out_features=1, bias=True)\n",
      "    (softmax): Softmax(dim=1)\n",
      "  )\n",
      "  (readout): Sequential(\n",
      "    (0): Dropout(p=0.0, inplace=False)\n",
      "    (1): Linear(in_features=1616, out_features=300, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.0, inplace=False)\n",
      "    (4): Linear(in_features=300, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
      "Loading pretrained parameter \"seq_embedder.weight\".\n",
      "Loading pretrained parameter \"rotary_embedder.freqs\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_weight\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_bias\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.weight\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.bias\".\n",
      "Loading pretrained parameter \"readout.1.weight\".\n",
      "Loading pretrained parameter \"readout.1.bias\".\n",
      "Loading pretrained parameter \"readout.4.weight\".\n",
      "Loading pretrained parameter \"readout.4.bias\".\n",
      "Moving model to cuda\n",
      "Creating protein model\n",
      "MoleculeModel(\n",
      "  (softplus): Softplus(beta=1.0, threshold=20.0)\n",
      "  (encoder): MPN(\n",
      "    (encoder): ModuleList(\n",
      "      (0): MPNEncoder(\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (act_func): ReLU()\n",
      "        (W_i): Linear(in_features=147, out_features=300, bias=False)\n",
      "        (W_h): Linear(in_features=300, out_features=300, bias=False)\n",
      "        (W_o): Linear(in_features=433, out_features=300, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (seq_embedder): Embedding(21, 36, padding_idx=20)\n",
      "  (rotary_embedder): RotaryEmbedding()\n",
      "  (multihead_attn): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=36, out_features=36, bias=True)\n",
      "  )\n",
      "  (attentive_pooler): AttentivePooling(\n",
      "    (linear1): Linear(in_features=1316, out_features=1316, bias=True)\n",
      "    (tanh): Tanh()\n",
      "    (linear2): Linear(in_features=1316, out_features=1, bias=True)\n",
      "    (softmax): Softmax(dim=1)\n",
      "  )\n",
      "  (readout): Sequential(\n",
      "    (0): Dropout(p=0.0, inplace=False)\n",
      "    (1): Linear(in_features=1616, out_features=300, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.0, inplace=False)\n",
      "    (4): Linear(in_features=300, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
      "Loading pretrained parameter \"seq_embedder.weight\".\n",
      "Loading pretrained parameter \"rotary_embedder.freqs\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_weight\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_bias\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.weight\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.bias\".\n",
      "Loading pretrained parameter \"readout.1.weight\".\n",
      "Loading pretrained parameter \"readout.1.bias\".\n",
      "Loading pretrained parameter \"readout.4.weight\".\n",
      "Loading pretrained parameter \"readout.4.bias\".\n",
      "Moving model to cuda\n",
      "Creating protein model\n",
      "MoleculeModel(\n",
      "  (softplus): Softplus(beta=1.0, threshold=20.0)\n",
      "  (encoder): MPN(\n",
      "    (encoder): ModuleList(\n",
      "      (0): MPNEncoder(\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (act_func): ReLU()\n",
      "        (W_i): Linear(in_features=147, out_features=300, bias=False)\n",
      "        (W_h): Linear(in_features=300, out_features=300, bias=False)\n",
      "        (W_o): Linear(in_features=433, out_features=300, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (seq_embedder): Embedding(21, 36, padding_idx=20)\n",
      "  (rotary_embedder): RotaryEmbedding()\n",
      "  (multihead_attn): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=36, out_features=36, bias=True)\n",
      "  )\n",
      "  (attentive_pooler): AttentivePooling(\n",
      "    (linear1): Linear(in_features=1316, out_features=1316, bias=True)\n",
      "    (tanh): Tanh()\n",
      "    (linear2): Linear(in_features=1316, out_features=1, bias=True)\n",
      "    (softmax): Softmax(dim=1)\n",
      "  )\n",
      "  (readout): Sequential(\n",
      "    (0): Dropout(p=0.0, inplace=False)\n",
      "    (1): Linear(in_features=1616, out_features=300, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.0, inplace=False)\n",
      "    (4): Linear(in_features=300, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
      "Loading pretrained parameter \"seq_embedder.weight\".\n",
      "Loading pretrained parameter \"rotary_embedder.freqs\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_weight\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_bias\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.weight\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.bias\".\n",
      "Loading pretrained parameter \"readout.1.weight\".\n",
      "Loading pretrained parameter \"readout.1.bias\".\n",
      "Loading pretrained parameter \"readout.4.weight\".\n",
      "Loading pretrained parameter \"readout.4.bias\".\n",
      "Moving model to cuda\n",
      "Creating protein model\n",
      "MoleculeModel(\n",
      "  (softplus): Softplus(beta=1.0, threshold=20.0)\n",
      "  (encoder): MPN(\n",
      "    (encoder): ModuleList(\n",
      "      (0): MPNEncoder(\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (act_func): ReLU()\n",
      "        (W_i): Linear(in_features=147, out_features=300, bias=False)\n",
      "        (W_h): Linear(in_features=300, out_features=300, bias=False)\n",
      "        (W_o): Linear(in_features=433, out_features=300, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (seq_embedder): Embedding(21, 36, padding_idx=20)\n",
      "  (rotary_embedder): RotaryEmbedding()\n",
      "  (multihead_attn): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=36, out_features=36, bias=True)\n",
      "  )\n",
      "  (attentive_pooler): AttentivePooling(\n",
      "    (linear1): Linear(in_features=1316, out_features=1316, bias=True)\n",
      "    (tanh): Tanh()\n",
      "    (linear2): Linear(in_features=1316, out_features=1, bias=True)\n",
      "    (softmax): Softmax(dim=1)\n",
      "  )\n",
      "  (readout): Sequential(\n",
      "    (0): Dropout(p=0.0, inplace=False)\n",
      "    (1): Linear(in_features=1616, out_features=300, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.0, inplace=False)\n",
      "    (4): Linear(in_features=300, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
      "Loading pretrained parameter \"seq_embedder.weight\".\n",
      "Loading pretrained parameter \"rotary_embedder.freqs\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_weight\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_bias\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.weight\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.bias\".\n",
      "Loading pretrained parameter \"readout.1.weight\".\n",
      "Loading pretrained parameter \"readout.1.bias\".\n",
      "Loading pretrained parameter \"readout.4.weight\".\n",
      "Loading pretrained parameter \"readout.4.bias\".\n",
      "Moving model to cuda\n",
      "Creating protein model\n",
      "MoleculeModel(\n",
      "  (softplus): Softplus(beta=1.0, threshold=20.0)\n",
      "  (encoder): MPN(\n",
      "    (encoder): ModuleList(\n",
      "      (0): MPNEncoder(\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (act_func): ReLU()\n",
      "        (W_i): Linear(in_features=147, out_features=300, bias=False)\n",
      "        (W_h): Linear(in_features=300, out_features=300, bias=False)\n",
      "        (W_o): Linear(in_features=433, out_features=300, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (seq_embedder): Embedding(21, 36, padding_idx=20)\n",
      "  (rotary_embedder): RotaryEmbedding()\n",
      "  (multihead_attn): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=36, out_features=36, bias=True)\n",
      "  )\n",
      "  (attentive_pooler): AttentivePooling(\n",
      "    (linear1): Linear(in_features=1316, out_features=1316, bias=True)\n",
      "    (tanh): Tanh()\n",
      "    (linear2): Linear(in_features=1316, out_features=1, bias=True)\n",
      "    (softmax): Softmax(dim=1)\n",
      "  )\n",
      "  (readout): Sequential(\n",
      "    (0): Dropout(p=0.0, inplace=False)\n",
      "    (1): Linear(in_features=1616, out_features=300, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.0, inplace=False)\n",
      "    (4): Linear(in_features=300, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
      "Loading pretrained parameter \"seq_embedder.weight\".\n",
      "Loading pretrained parameter \"rotary_embedder.freqs\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_weight\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_bias\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.weight\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.bias\".\n",
      "Loading pretrained parameter \"readout.1.weight\".\n",
      "Loading pretrained parameter \"readout.1.bias\".\n",
      "Loading pretrained parameter \"readout.4.weight\".\n",
      "Loading pretrained parameter \"readout.4.bias\".\n",
      "Moving model to cuda\n",
      "Creating protein model\n",
      "MoleculeModel(\n",
      "  (softplus): Softplus(beta=1.0, threshold=20.0)\n",
      "  (encoder): MPN(\n",
      "    (encoder): ModuleList(\n",
      "      (0): MPNEncoder(\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (act_func): ReLU()\n",
      "        (W_i): Linear(in_features=147, out_features=300, bias=False)\n",
      "        (W_h): Linear(in_features=300, out_features=300, bias=False)\n",
      "        (W_o): Linear(in_features=433, out_features=300, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (seq_embedder): Embedding(21, 36, padding_idx=20)\n",
      "  (rotary_embedder): RotaryEmbedding()\n",
      "  (multihead_attn): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=36, out_features=36, bias=True)\n",
      "  )\n",
      "  (attentive_pooler): AttentivePooling(\n",
      "    (linear1): Linear(in_features=1316, out_features=1316, bias=True)\n",
      "    (tanh): Tanh()\n",
      "    (linear2): Linear(in_features=1316, out_features=1, bias=True)\n",
      "    (softmax): Softmax(dim=1)\n",
      "  )\n",
      "  (readout): Sequential(\n",
      "    (0): Dropout(p=0.0, inplace=False)\n",
      "    (1): Linear(in_features=1616, out_features=300, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.0, inplace=False)\n",
      "    (4): Linear(in_features=300, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
      "Loading pretrained parameter \"seq_embedder.weight\".\n",
      "Loading pretrained parameter \"rotary_embedder.freqs\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_weight\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_bias\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.weight\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.bias\".\n",
      "Loading pretrained parameter \"readout.1.weight\".\n",
      "Loading pretrained parameter \"readout.1.bias\".\n",
      "Loading pretrained parameter \"readout.4.weight\".\n",
      "Loading pretrained parameter \"readout.4.bias\".\n",
      "Moving model to cuda\n",
      "Creating protein model\n",
      "MoleculeModel(\n",
      "  (softplus): Softplus(beta=1.0, threshold=20.0)\n",
      "  (encoder): MPN(\n",
      "    (encoder): ModuleList(\n",
      "      (0): MPNEncoder(\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (act_func): ReLU()\n",
      "        (W_i): Linear(in_features=147, out_features=300, bias=False)\n",
      "        (W_h): Linear(in_features=300, out_features=300, bias=False)\n",
      "        (W_o): Linear(in_features=433, out_features=300, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (seq_embedder): Embedding(21, 36, padding_idx=20)\n",
      "  (rotary_embedder): RotaryEmbedding()\n",
      "  (multihead_attn): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=36, out_features=36, bias=True)\n",
      "  )\n",
      "  (attentive_pooler): AttentivePooling(\n",
      "    (linear1): Linear(in_features=1316, out_features=1316, bias=True)\n",
      "    (tanh): Tanh()\n",
      "    (linear2): Linear(in_features=1316, out_features=1, bias=True)\n",
      "    (softmax): Softmax(dim=1)\n",
      "  )\n",
      "  (readout): Sequential(\n",
      "    (0): Dropout(p=0.0, inplace=False)\n",
      "    (1): Linear(in_features=1616, out_features=300, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.0, inplace=False)\n",
      "    (4): Linear(in_features=300, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
      "Loading pretrained parameter \"seq_embedder.weight\".\n",
      "Loading pretrained parameter \"rotary_embedder.freqs\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_weight\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_bias\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.weight\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.bias\".\n",
      "Loading pretrained parameter \"readout.1.weight\".\n",
      "Loading pretrained parameter \"readout.1.bias\".\n",
      "Loading pretrained parameter \"readout.4.weight\".\n",
      "Loading pretrained parameter \"readout.4.bias\".\n",
      "Moving model to cuda\n",
      "Creating protein model\n",
      "MoleculeModel(\n",
      "  (softplus): Softplus(beta=1.0, threshold=20.0)\n",
      "  (encoder): MPN(\n",
      "    (encoder): ModuleList(\n",
      "      (0): MPNEncoder(\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (act_func): ReLU()\n",
      "        (W_i): Linear(in_features=147, out_features=300, bias=False)\n",
      "        (W_h): Linear(in_features=300, out_features=300, bias=False)\n",
      "        (W_o): Linear(in_features=433, out_features=300, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (seq_embedder): Embedding(21, 36, padding_idx=20)\n",
      "  (rotary_embedder): RotaryEmbedding()\n",
      "  (multihead_attn): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=36, out_features=36, bias=True)\n",
      "  )\n",
      "  (attentive_pooler): AttentivePooling(\n",
      "    (linear1): Linear(in_features=1316, out_features=1316, bias=True)\n",
      "    (tanh): Tanh()\n",
      "    (linear2): Linear(in_features=1316, out_features=1, bias=True)\n",
      "    (softmax): Softmax(dim=1)\n",
      "  )\n",
      "  (readout): Sequential(\n",
      "    (0): Dropout(p=0.0, inplace=False)\n",
      "    (1): Linear(in_features=1616, out_features=300, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.0, inplace=False)\n",
      "    (4): Linear(in_features=300, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
      "Loading pretrained parameter \"seq_embedder.weight\".\n",
      "Loading pretrained parameter \"rotary_embedder.freqs\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_weight\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_bias\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.weight\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.bias\".\n",
      "Loading pretrained parameter \"readout.1.weight\".\n",
      "Loading pretrained parameter \"readout.1.bias\".\n",
      "Loading pretrained parameter \"readout.4.weight\".\n",
      "Loading pretrained parameter \"readout.4.bias\".\n",
      "Moving model to cuda\n",
      "Saving predictions to ../pipeline_entry_kcat_input_output.csv\n",
      "Elapsed time = 0:00:06\n",
      "Prediction completed.\n",
      "Output saved to results/ pipeline_entry_kcat_input_output.csv\n",
      "\n",
      "/home/xuvin/py_repos/wccm/src/catpred_pipeline/catpred\n",
      "Predicting.. This will take a while..\n",
      "calculating protein embed only on cpu\n",
      "Loading training args\n",
      "Loading models\n",
      "Setting molecule featurization parameters to default.\n",
      "Loading data\n",
      "Validating SMILES\n",
      "Test size = 5\n",
      "Creating protein model\n",
      "MoleculeModel(\n",
      "  (softplus): Softplus(beta=1.0, threshold=20.0)\n",
      "  (encoder): MPN(\n",
      "    (encoder): ModuleList(\n",
      "      (0): MPNEncoder(\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (act_func): ReLU()\n",
      "        (W_i): Linear(in_features=147, out_features=300, bias=False)\n",
      "        (W_h): Linear(in_features=300, out_features=300, bias=False)\n",
      "        (W_o): Linear(in_features=433, out_features=300, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (seq_embedder): Embedding(21, 36, padding_idx=20)\n",
      "  (rotary_embedder): RotaryEmbedding()\n",
      "  (multihead_attn): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=36, out_features=36, bias=True)\n",
      "  )\n",
      "  (attentive_pooler): AttentivePooling(\n",
      "    (linear1): Linear(in_features=1316, out_features=1316, bias=True)\n",
      "    (tanh): Tanh()\n",
      "    (linear2): Linear(in_features=1316, out_features=1, bias=True)\n",
      "    (softmax): Softmax(dim=1)\n",
      "  )\n",
      "  (readout): Sequential(\n",
      "    (0): Dropout(p=0.0, inplace=False)\n",
      "    (1): Linear(in_features=1616, out_features=300, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.0, inplace=False)\n",
      "    (4): Linear(in_features=300, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
      "Loading pretrained parameter \"seq_embedder.weight\".\n",
      "Loading pretrained parameter \"rotary_embedder.freqs\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_weight\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_bias\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.weight\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.bias\".\n",
      "Loading pretrained parameter \"readout.1.weight\".\n",
      "Loading pretrained parameter \"readout.1.bias\".\n",
      "Loading pretrained parameter \"readout.4.weight\".\n",
      "Loading pretrained parameter \"readout.4.bias\".\n",
      "Moving model to cuda\n",
      "Creating protein model\n",
      "MoleculeModel(\n",
      "  (softplus): Softplus(beta=1.0, threshold=20.0)\n",
      "  (encoder): MPN(\n",
      "    (encoder): ModuleList(\n",
      "      (0): MPNEncoder(\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (act_func): ReLU()\n",
      "        (W_i): Linear(in_features=147, out_features=300, bias=False)\n",
      "        (W_h): Linear(in_features=300, out_features=300, bias=False)\n",
      "        (W_o): Linear(in_features=433, out_features=300, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (seq_embedder): Embedding(21, 36, padding_idx=20)\n",
      "  (rotary_embedder): RotaryEmbedding()\n",
      "  (multihead_attn): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=36, out_features=36, bias=True)\n",
      "  )\n",
      "  (attentive_pooler): AttentivePooling(\n",
      "    (linear1): Linear(in_features=1316, out_features=1316, bias=True)\n",
      "    (tanh): Tanh()\n",
      "    (linear2): Linear(in_features=1316, out_features=1, bias=True)\n",
      "    (softmax): Softmax(dim=1)\n",
      "  )\n",
      "  (readout): Sequential(\n",
      "    (0): Dropout(p=0.0, inplace=False)\n",
      "    (1): Linear(in_features=1616, out_features=300, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.0, inplace=False)\n",
      "    (4): Linear(in_features=300, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
      "Loading pretrained parameter \"seq_embedder.weight\".\n",
      "Loading pretrained parameter \"rotary_embedder.freqs\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_weight\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_bias\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.weight\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.bias\".\n",
      "Loading pretrained parameter \"readout.1.weight\".\n",
      "Loading pretrained parameter \"readout.1.bias\".\n",
      "Loading pretrained parameter \"readout.4.weight\".\n",
      "Loading pretrained parameter \"readout.4.bias\".\n",
      "Moving model to cuda\n",
      "Creating protein model\n",
      "MoleculeModel(\n",
      "  (softplus): Softplus(beta=1.0, threshold=20.0)\n",
      "  (encoder): MPN(\n",
      "    (encoder): ModuleList(\n",
      "      (0): MPNEncoder(\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (act_func): ReLU()\n",
      "        (W_i): Linear(in_features=147, out_features=300, bias=False)\n",
      "        (W_h): Linear(in_features=300, out_features=300, bias=False)\n",
      "        (W_o): Linear(in_features=433, out_features=300, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (seq_embedder): Embedding(21, 36, padding_idx=20)\n",
      "  (rotary_embedder): RotaryEmbedding()\n",
      "  (multihead_attn): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=36, out_features=36, bias=True)\n",
      "  )\n",
      "  (attentive_pooler): AttentivePooling(\n",
      "    (linear1): Linear(in_features=1316, out_features=1316, bias=True)\n",
      "    (tanh): Tanh()\n",
      "    (linear2): Linear(in_features=1316, out_features=1, bias=True)\n",
      "    (softmax): Softmax(dim=1)\n",
      "  )\n",
      "  (readout): Sequential(\n",
      "    (0): Dropout(p=0.0, inplace=False)\n",
      "    (1): Linear(in_features=1616, out_features=300, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.0, inplace=False)\n",
      "    (4): Linear(in_features=300, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
      "Loading pretrained parameter \"seq_embedder.weight\".\n",
      "Loading pretrained parameter \"rotary_embedder.freqs\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_weight\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_bias\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.weight\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.bias\".\n",
      "Loading pretrained parameter \"readout.1.weight\".\n",
      "Loading pretrained parameter \"readout.1.bias\".\n",
      "Loading pretrained parameter \"readout.4.weight\".\n",
      "Loading pretrained parameter \"readout.4.bias\".\n",
      "Moving model to cuda\n",
      "Creating protein model\n",
      "MoleculeModel(\n",
      "  (softplus): Softplus(beta=1.0, threshold=20.0)\n",
      "  (encoder): MPN(\n",
      "    (encoder): ModuleList(\n",
      "      (0): MPNEncoder(\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (act_func): ReLU()\n",
      "        (W_i): Linear(in_features=147, out_features=300, bias=False)\n",
      "        (W_h): Linear(in_features=300, out_features=300, bias=False)\n",
      "        (W_o): Linear(in_features=433, out_features=300, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (seq_embedder): Embedding(21, 36, padding_idx=20)\n",
      "  (rotary_embedder): RotaryEmbedding()\n",
      "  (multihead_attn): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=36, out_features=36, bias=True)\n",
      "  )\n",
      "  (attentive_pooler): AttentivePooling(\n",
      "    (linear1): Linear(in_features=1316, out_features=1316, bias=True)\n",
      "    (tanh): Tanh()\n",
      "    (linear2): Linear(in_features=1316, out_features=1, bias=True)\n",
      "    (softmax): Softmax(dim=1)\n",
      "  )\n",
      "  (readout): Sequential(\n",
      "    (0): Dropout(p=0.0, inplace=False)\n",
      "    (1): Linear(in_features=1616, out_features=300, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.0, inplace=False)\n",
      "    (4): Linear(in_features=300, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
      "Loading pretrained parameter \"seq_embedder.weight\".\n",
      "Loading pretrained parameter \"rotary_embedder.freqs\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_weight\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_bias\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.weight\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.bias\".\n",
      "Loading pretrained parameter \"readout.1.weight\".\n",
      "Loading pretrained parameter \"readout.1.bias\".\n",
      "Loading pretrained parameter \"readout.4.weight\".\n",
      "Loading pretrained parameter \"readout.4.bias\".\n",
      "Moving model to cuda\n",
      "Creating protein model\n",
      "MoleculeModel(\n",
      "  (softplus): Softplus(beta=1.0, threshold=20.0)\n",
      "  (encoder): MPN(\n",
      "    (encoder): ModuleList(\n",
      "      (0): MPNEncoder(\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (act_func): ReLU()\n",
      "        (W_i): Linear(in_features=147, out_features=300, bias=False)\n",
      "        (W_h): Linear(in_features=300, out_features=300, bias=False)\n",
      "        (W_o): Linear(in_features=433, out_features=300, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (seq_embedder): Embedding(21, 36, padding_idx=20)\n",
      "  (rotary_embedder): RotaryEmbedding()\n",
      "  (multihead_attn): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=36, out_features=36, bias=True)\n",
      "  )\n",
      "  (attentive_pooler): AttentivePooling(\n",
      "    (linear1): Linear(in_features=1316, out_features=1316, bias=True)\n",
      "    (tanh): Tanh()\n",
      "    (linear2): Linear(in_features=1316, out_features=1, bias=True)\n",
      "    (softmax): Softmax(dim=1)\n",
      "  )\n",
      "  (readout): Sequential(\n",
      "    (0): Dropout(p=0.0, inplace=False)\n",
      "    (1): Linear(in_features=1616, out_features=300, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.0, inplace=False)\n",
      "    (4): Linear(in_features=300, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
      "Loading pretrained parameter \"seq_embedder.weight\".\n",
      "Loading pretrained parameter \"rotary_embedder.freqs\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_weight\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_bias\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.weight\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.bias\".\n",
      "Loading pretrained parameter \"readout.1.weight\".\n",
      "Loading pretrained parameter \"readout.1.bias\".\n",
      "Loading pretrained parameter \"readout.4.weight\".\n",
      "Loading pretrained parameter \"readout.4.bias\".\n",
      "Moving model to cuda\n",
      "Creating protein model\n",
      "MoleculeModel(\n",
      "  (softplus): Softplus(beta=1.0, threshold=20.0)\n",
      "  (encoder): MPN(\n",
      "    (encoder): ModuleList(\n",
      "      (0): MPNEncoder(\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (act_func): ReLU()\n",
      "        (W_i): Linear(in_features=147, out_features=300, bias=False)\n",
      "        (W_h): Linear(in_features=300, out_features=300, bias=False)\n",
      "        (W_o): Linear(in_features=433, out_features=300, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (seq_embedder): Embedding(21, 36, padding_idx=20)\n",
      "  (rotary_embedder): RotaryEmbedding()\n",
      "  (multihead_attn): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=36, out_features=36, bias=True)\n",
      "  )\n",
      "  (attentive_pooler): AttentivePooling(\n",
      "    (linear1): Linear(in_features=1316, out_features=1316, bias=True)\n",
      "    (tanh): Tanh()\n",
      "    (linear2): Linear(in_features=1316, out_features=1, bias=True)\n",
      "    (softmax): Softmax(dim=1)\n",
      "  )\n",
      "  (readout): Sequential(\n",
      "    (0): Dropout(p=0.0, inplace=False)\n",
      "    (1): Linear(in_features=1616, out_features=300, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.0, inplace=False)\n",
      "    (4): Linear(in_features=300, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
      "Loading pretrained parameter \"seq_embedder.weight\".\n",
      "Loading pretrained parameter \"rotary_embedder.freqs\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_weight\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_bias\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.weight\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.bias\".\n",
      "Loading pretrained parameter \"readout.1.weight\".\n",
      "Loading pretrained parameter \"readout.1.bias\".\n",
      "Loading pretrained parameter \"readout.4.weight\".\n",
      "Loading pretrained parameter \"readout.4.bias\".\n",
      "Moving model to cuda\n",
      "Creating protein model\n",
      "MoleculeModel(\n",
      "  (softplus): Softplus(beta=1.0, threshold=20.0)\n",
      "  (encoder): MPN(\n",
      "    (encoder): ModuleList(\n",
      "      (0): MPNEncoder(\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (act_func): ReLU()\n",
      "        (W_i): Linear(in_features=147, out_features=300, bias=False)\n",
      "        (W_h): Linear(in_features=300, out_features=300, bias=False)\n",
      "        (W_o): Linear(in_features=433, out_features=300, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (seq_embedder): Embedding(21, 36, padding_idx=20)\n",
      "  (rotary_embedder): RotaryEmbedding()\n",
      "  (multihead_attn): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=36, out_features=36, bias=True)\n",
      "  )\n",
      "  (attentive_pooler): AttentivePooling(\n",
      "    (linear1): Linear(in_features=1316, out_features=1316, bias=True)\n",
      "    (tanh): Tanh()\n",
      "    (linear2): Linear(in_features=1316, out_features=1, bias=True)\n",
      "    (softmax): Softmax(dim=1)\n",
      "  )\n",
      "  (readout): Sequential(\n",
      "    (0): Dropout(p=0.0, inplace=False)\n",
      "    (1): Linear(in_features=1616, out_features=300, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.0, inplace=False)\n",
      "    (4): Linear(in_features=300, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
      "Loading pretrained parameter \"seq_embedder.weight\".\n",
      "Loading pretrained parameter \"rotary_embedder.freqs\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_weight\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_bias\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.weight\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.bias\".\n",
      "Loading pretrained parameter \"readout.1.weight\".\n",
      "Loading pretrained parameter \"readout.1.bias\".\n",
      "Loading pretrained parameter \"readout.4.weight\".\n",
      "Loading pretrained parameter \"readout.4.bias\".\n",
      "Moving model to cuda\n",
      "Creating protein model\n",
      "MoleculeModel(\n",
      "  (softplus): Softplus(beta=1.0, threshold=20.0)\n",
      "  (encoder): MPN(\n",
      "    (encoder): ModuleList(\n",
      "      (0): MPNEncoder(\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (act_func): ReLU()\n",
      "        (W_i): Linear(in_features=147, out_features=300, bias=False)\n",
      "        (W_h): Linear(in_features=300, out_features=300, bias=False)\n",
      "        (W_o): Linear(in_features=433, out_features=300, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (seq_embedder): Embedding(21, 36, padding_idx=20)\n",
      "  (rotary_embedder): RotaryEmbedding()\n",
      "  (multihead_attn): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=36, out_features=36, bias=True)\n",
      "  )\n",
      "  (attentive_pooler): AttentivePooling(\n",
      "    (linear1): Linear(in_features=1316, out_features=1316, bias=True)\n",
      "    (tanh): Tanh()\n",
      "    (linear2): Linear(in_features=1316, out_features=1, bias=True)\n",
      "    (softmax): Softmax(dim=1)\n",
      "  )\n",
      "  (readout): Sequential(\n",
      "    (0): Dropout(p=0.0, inplace=False)\n",
      "    (1): Linear(in_features=1616, out_features=300, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.0, inplace=False)\n",
      "    (4): Linear(in_features=300, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
      "Loading pretrained parameter \"seq_embedder.weight\".\n",
      "Loading pretrained parameter \"rotary_embedder.freqs\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_weight\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_bias\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.weight\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.bias\".\n",
      "Loading pretrained parameter \"readout.1.weight\".\n",
      "Loading pretrained parameter \"readout.1.bias\".\n",
      "Loading pretrained parameter \"readout.4.weight\".\n",
      "Loading pretrained parameter \"readout.4.bias\".\n",
      "Moving model to cuda\n",
      "Creating protein model\n",
      "MoleculeModel(\n",
      "  (softplus): Softplus(beta=1.0, threshold=20.0)\n",
      "  (encoder): MPN(\n",
      "    (encoder): ModuleList(\n",
      "      (0): MPNEncoder(\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (act_func): ReLU()\n",
      "        (W_i): Linear(in_features=147, out_features=300, bias=False)\n",
      "        (W_h): Linear(in_features=300, out_features=300, bias=False)\n",
      "        (W_o): Linear(in_features=433, out_features=300, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (seq_embedder): Embedding(21, 36, padding_idx=20)\n",
      "  (rotary_embedder): RotaryEmbedding()\n",
      "  (multihead_attn): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=36, out_features=36, bias=True)\n",
      "  )\n",
      "  (attentive_pooler): AttentivePooling(\n",
      "    (linear1): Linear(in_features=1316, out_features=1316, bias=True)\n",
      "    (tanh): Tanh()\n",
      "    (linear2): Linear(in_features=1316, out_features=1, bias=True)\n",
      "    (softmax): Softmax(dim=1)\n",
      "  )\n",
      "  (readout): Sequential(\n",
      "    (0): Dropout(p=0.0, inplace=False)\n",
      "    (1): Linear(in_features=1616, out_features=300, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.0, inplace=False)\n",
      "    (4): Linear(in_features=300, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
      "Loading pretrained parameter \"seq_embedder.weight\".\n",
      "Loading pretrained parameter \"rotary_embedder.freqs\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_weight\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_bias\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.weight\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.bias\".\n",
      "Loading pretrained parameter \"readout.1.weight\".\n",
      "Loading pretrained parameter \"readout.1.bias\".\n",
      "Loading pretrained parameter \"readout.4.weight\".\n",
      "Loading pretrained parameter \"readout.4.bias\".\n",
      "Moving model to cuda\n",
      "Creating protein model\n",
      "MoleculeModel(\n",
      "  (softplus): Softplus(beta=1.0, threshold=20.0)\n",
      "  (encoder): MPN(\n",
      "    (encoder): ModuleList(\n",
      "      (0): MPNEncoder(\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (act_func): ReLU()\n",
      "        (W_i): Linear(in_features=147, out_features=300, bias=False)\n",
      "        (W_h): Linear(in_features=300, out_features=300, bias=False)\n",
      "        (W_o): Linear(in_features=433, out_features=300, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (seq_embedder): Embedding(21, 36, padding_idx=20)\n",
      "  (rotary_embedder): RotaryEmbedding()\n",
      "  (multihead_attn): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=36, out_features=36, bias=True)\n",
      "  )\n",
      "  (attentive_pooler): AttentivePooling(\n",
      "    (linear1): Linear(in_features=1316, out_features=1316, bias=True)\n",
      "    (tanh): Tanh()\n",
      "    (linear2): Linear(in_features=1316, out_features=1, bias=True)\n",
      "    (softmax): Softmax(dim=1)\n",
      "  )\n",
      "  (readout): Sequential(\n",
      "    (0): Dropout(p=0.0, inplace=False)\n",
      "    (1): Linear(in_features=1616, out_features=300, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.0, inplace=False)\n",
      "    (4): Linear(in_features=300, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
      "Loading pretrained parameter \"seq_embedder.weight\".\n",
      "Loading pretrained parameter \"rotary_embedder.freqs\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_weight\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_bias\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.weight\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.bias\".\n",
      "Loading pretrained parameter \"readout.1.weight\".\n",
      "Loading pretrained parameter \"readout.1.bias\".\n",
      "Loading pretrained parameter \"readout.4.weight\".\n",
      "Loading pretrained parameter \"readout.4.bias\".\n",
      "Moving model to cuda\n",
      "Saving predictions to ../pipeline_entry_km_input_output.csv\n",
      "Elapsed time = 0:00:07\n",
      "Prediction completed.\n",
      "Output saved to results/ pipeline_entry_km_input_output.csv\n",
      "\n",
      "output |    |   Unnamed: 0.2 |   Unnamed: 0.1 |   Unnamed: 0 | substrate    | SMILES                                                                                                                    | sequence                                                                                                                                                                                                                                                                                                                     | pdbpath   |   log10kcat_max |   log10kcat_max_mve_uncal_var |   log10kcat_max_model_0 |   log10kcat_max_model_1 |   log10kcat_max_model_2 |   log10kcat_max_model_3 |   log10kcat_max_model_4 |   log10kcat_max_model_5 |   log10kcat_max_model_6 |   log10kcat_max_model_7 |   log10kcat_max_model_8 |   log10kcat_max_model_9 |   Prediction_(s^(-1)) |   Prediction_log10 |   SD_total |   SD_aleatoric |   SD_epistemic |\n",
      "|---:|---------------:|---------------:|-------------:|:-------------|:--------------------------------------------------------------------------------------------------------------------------|:-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:----------|----------------:|------------------------------:|------------------------:|------------------------:|------------------------:|------------------------:|------------------------:|------------------------:|------------------------:|------------------------:|------------------------:|------------------------:|----------------------:|-------------------:|-----------:|---------------:|---------------:|\n",
      "|  0 |              0 |              0 |            0 | Acetaldehyde | CC=O                                                                                                                      | MSKRKVAIIGSGNIGTDLMIKILRHGQHLEMAVMVGIDPQSDGLARARRMGVATTHEGVIGLMNMPEFADIDIVFDATSAGAHVKNDAALREAKPDIRLIDLTPAAIGPYCVPVVNLEANVDQLNVNMVTCGGQATIPMVAAVSRVARVHYAEIIASIASKSAGPGTRANIDEFTETTSRAIEVVGGAAKGKAIIVLNPAEPPLMMRDTVYVLSDEASQDDIEASINEMAEAVQAYVPGYRLKQRVQFEVIPQDKPVNLPGVGQFSGLKTAVWLEVEGAAHYLPAYAGNLDIMTSSALATAEKMAQSLARKAGEAA | seq4.seq  |        0.953852 |                      0.306957 |                 1.99409 |                0.881633 |                0.673911 |                 1.65527 |                0.623136 |                 1.03717 |                0.693647 |                0.834955 |                0.892004 |               0.252704  |               8.99191 |           0.953852 |   0.554037 |       0.268014 |       0.484897 |\n",
      "|  1 |              1 |              1 |            1 | CoA          | CC(C)(COP(=O)(O)OP(=O)(O)OC[C@H]1O[C@@H](n2cnc3c(N)ncnc32)[C@H](O)[C@@H]1OP(=O)(O)O)[C@@H](O)C(=O)NCCC(=O)NCCS            | MSKRKVAIIGSGNIGTDLMIKILRHGQHLEMAVMVGIDPQSDGLARARRMGVATTHEGVIGLMNMPEFADIDIVFDATSAGAHVKNDAALREAKPDIRLIDLTPAAIGPYCVPVVNLEANVDQLNVNMVTCGGQATIPMVAAVSRVARVHYAEIIASIASKSAGPGTRANIDEFTETTSRAIEVVGGAAKGKAIIVLNPAEPPLMMRDTVYVLSDEASQDDIEASINEMAEAVQAYVPGYRLKQRVQFEVIPQDKPVNLPGVGQFSGLKTAVWLEVEGAAHYLPAYAGNLDIMTSSALATAEKMAQSLARKAGEAA | seq4.seq  |        0.911925 |                      0.403128 |                 1.66736 |                0.974509 |                0.717792 |                 1.59825 |                0.661197 |                 1.43178 |                0.723336 |                0.702034 |                0.749888 |              -0.106898  |               8.16442 |           0.911925 |   0.634923 |       0.38353  |       0.505997 |\n",
      "|  2 |              2 |              2 |            2 | NAD+         | NC(=O)c1ccc[n+]([C@@H]2O[C@H](COP(=O)([O-])OP(=O)(O)OC[C@H]3O[C@@H](n4cnc5c(N)ncnc54)[C@H](O)[C@@H]3O)[C@@H](O)[C@H]2O)c1 | MSKRKVAIIGSGNIGTDLMIKILRHGQHLEMAVMVGIDPQSDGLARARRMGVATTHEGVIGLMNMPEFADIDIVFDATSAGAHVKNDAALREAKPDIRLIDLTPAAIGPYCVPVVNLEANVDQLNVNMVTCGGQATIPMVAAVSRVARVHYAEIIASIASKSAGPGTRANIDEFTETTSRAIEVVGGAAKGKAIIVLNPAEPPLMMRDTVYVLSDEASQDDIEASINEMAEAVQAYVPGYRLKQRVQFEVIPQDKPVNLPGVGQFSGLKTAVWLEVEGAAHYLPAYAGNLDIMTSSALATAEKMAQSLARKAGEAA | seq4.seq  |        0.850396 |                      0.52051  |                 1.48176 |                0.939454 |                0.784251 |                 1.34619 |                0.765639 |                 1.31728 |                1.07541  |                0.625369 |                0.784977 |              -0.616363  |               7.08591 |           0.850396 |   0.721464 |       0.454563 |       0.560252 |\n",
      "|  3 |              3 |              3 |            3 | Acetyl-CoA   | CC(=O)SCCNC(=O)CCNC(=O)[C@H](O)C(C)(C)COP(=O)(O)OP(=O)(O)OC[C@H]1O[C@@H](n2cnc3c(N)ncnc32)[C@H](O)[C@@H]1OP(=O)(O)O       | MSKRKVAIIGSGNIGTDLMIKILRHGQHLEMAVMVGIDPQSDGLARARRMGVATTHEGVIGLMNMPEFADIDIVFDATSAGAHVKNDAALREAKPDIRLIDLTPAAIGPYCVPVVNLEANVDQLNVNMVTCGGQATIPMVAAVSRVARVHYAEIIASIASKSAGPGTRANIDEFTETTSRAIEVVGGAAKGKAIIVLNPAEPPLMMRDTVYVLSDEASQDDIEASINEMAEAVQAYVPGYRLKQRVQFEVIPQDKPVNLPGVGQFSGLKTAVWLEVEGAAHYLPAYAGNLDIMTSSALATAEKMAQSLARKAGEAA | seq4.seq  |        0.903835 |                      0.407602 |                 1.72374 |                0.94023  |                0.77162  |                 1.56591 |                0.669127 |                 1.35808 |                0.729121 |                0.614603 |                0.736795 |              -0.0708768 |               8.01373 |           0.903835 |   0.638437 |       0.399132 |       0.498293 |\n",
      "|  4 |              4 |              4 |            4 | NADH         | NC(=O)C1=CN([C@@H]2O[C@H](COP(=O)(O)OP(=O)(O)OC[C@H]3O[C@@H](n4cnc5c(N)ncnc54)[C@H](O)[C@@H]3O)[C@@H](O)[C@H]2O)C=CC1     | MSKRKVAIIGSGNIGTDLMIKILRHGQHLEMAVMVGIDPQSDGLARARRMGVATTHEGVIGLMNMPEFADIDIVFDATSAGAHVKNDAALREAKPDIRLIDLTPAAIGPYCVPVVNLEANVDQLNVNMVTCGGQATIPMVAAVSRVARVHYAEIIASIASKSAGPGTRANIDEFTETTSRAIEVVGGAAKGKAIIVLNPAEPPLMMRDTVYVLSDEASQDDIEASINEMAEAVQAYVPGYRLKQRVQFEVIPQDKPVNLPGVGQFSGLKTAVWLEVEGAAHYLPAYAGNLDIMTSSALATAEKMAQSLARKAGEAA | seq4.seq  |        1.05684  |                      0.364502 |                 1.60883 |                1.05486  |                0.914574 |                 1.72167 |                0.923742 |                 1.53648 |                0.969291 |                0.897277 |                0.89476  |               0.0468845 |              11.3982  |           1.05684  |   0.60374  |       0.393815 |       0.457615 |\n",
      "SMILES ['CC=O', 'CC(C)(COP(=O)(O)OP(=O)(O)OC[C@@H]1[C@H]([C@H]([C@@H](O1)N2C=NC3=C(N=CN=C32)N)O)OP(=O)(O)O)[C@H](C(=O)NCCC(=O)NCCS)O', 'C1=CC(=C[N+](=C1)[C@H]2[C@@H]([C@@H]([C@H](O2)COP(=O)([O-])OP(=O)(O)OC[C@@H]3[C@H]([C@H]([C@@H](O3)N4C=NC5=C(N=CN=C54)N)O)O)O)O)C(=O)N', 'CC(=O)SCCNC(=O)CCNC(=O)[C@@H](C(C)(C)COP(=O)(O)OP(=O)(O)OC[C@@H]1[C@H]([C@H]([C@@H](O1)N2C=NC3=C(N=CN=C32)N)O)OP(=O)(O)O)O', 'C1C=CN(C=C1C(=O)N)[C@H]2[C@@H]([C@@H]([C@H](O2)COP(=O)(O)OP(=O)(O)OC[C@@H]3[C@H]([C@H]([C@@H](O3)N4C=NC5=C(N=CN=C54)N)O)O)O)O']\n",
      "smiles_list CC=O ['C(=O)[O-]', 'CC(=O)SCCNC(=O)CCNC(=O)[C@@H](C(C)(C)COP(=O)(O)OP(=O)(O)OC[C@@H]1[C@H]([C@H]([C@@H](O1)N2C=NC3=C(N=CN=C32)N)O)OP(=O)(O)O)O', 'CC(=O)C(=O)[O-]', 'CC(C)(COP(=O)(O)OP(=O)(O)OC[C@@H]1[C@H]([C@H]([C@@H](O1)N2C=NC3=C(N=CN=C32)N)O)OP(=O)(O)O)[C@H](C(=O)NCCC(=O)NCCS)O', 'CC(=O)[O-]', 'C1C=CN(C=C1C(=O)N)[C@H]2[C@@H]([C@@H]([C@H](O2)COP(=O)(O)OP(=O)(O)OC[C@@H]3[C@H]([C@H]([C@@H](O3)N4C=NC5=C(N=CN=C54)N)O)O)O)O', 'CC=O', 'C1=CC(=C[N+](=C1)[C@H]2[C@@H]([C@@H]([C@H](O2)COP(=O)([O-])OP(=O)(O)OC[C@@H]3[C@H]([C@H]([C@@H](O3)N4C=NC5=C(N=CN=C54)N)O)O)O)O)C(=O)N', 'CCO']\n",
      "smiles_list CC(C)(COP(=O)(O)OP(=O)(O)OC[C@@H]1[C@H]([C@H]([C@@H](O1)N2C=NC3=C(N=CN=C32)N)O)OP(=O)(O)O)[C@H](C(=O)NCCC(=O)NCCS)O ['C(=O)[O-]', 'CC(=O)SCCNC(=O)CCNC(=O)[C@@H](C(C)(C)COP(=O)(O)OP(=O)(O)OC[C@@H]1[C@H]([C@H]([C@@H](O1)N2C=NC3=C(N=CN=C32)N)O)OP(=O)(O)O)O', 'CC(=O)C(=O)[O-]', 'CC(C)(COP(=O)(O)OP(=O)(O)OC[C@@H]1[C@H]([C@H]([C@@H](O1)N2C=NC3=C(N=CN=C32)N)O)OP(=O)(O)O)[C@H](C(=O)NCCC(=O)NCCS)O', 'CC(=O)[O-]', 'C1C=CN(C=C1C(=O)N)[C@H]2[C@@H]([C@@H]([C@H](O2)COP(=O)(O)OP(=O)(O)OC[C@@H]3[C@H]([C@H]([C@@H](O3)N4C=NC5=C(N=CN=C54)N)O)O)O)O', 'CC=O', 'C1=CC(=C[N+](=C1)[C@H]2[C@@H]([C@@H]([C@H](O2)COP(=O)([O-])OP(=O)(O)OC[C@@H]3[C@H]([C@H]([C@@H](O3)N4C=NC5=C(N=CN=C54)N)O)O)O)O)C(=O)N', 'CCO']\n",
      "smiles_list C1=CC(=C[N+](=C1)[C@H]2[C@@H]([C@@H]([C@H](O2)COP(=O)([O-])OP(=O)(O)OC[C@@H]3[C@H]([C@H]([C@@H](O3)N4C=NC5=C(N=CN=C54)N)O)O)O)O)C(=O)N ['C(=O)[O-]', 'CC(=O)SCCNC(=O)CCNC(=O)[C@@H](C(C)(C)COP(=O)(O)OP(=O)(O)OC[C@@H]1[C@H]([C@H]([C@@H](O1)N2C=NC3=C(N=CN=C32)N)O)OP(=O)(O)O)O', 'CC(=O)C(=O)[O-]', 'CC(C)(COP(=O)(O)OP(=O)(O)OC[C@@H]1[C@H]([C@H]([C@@H](O1)N2C=NC3=C(N=CN=C32)N)O)OP(=O)(O)O)[C@H](C(=O)NCCC(=O)NCCS)O', 'CC(=O)[O-]', 'C1C=CN(C=C1C(=O)N)[C@H]2[C@@H]([C@@H]([C@H](O2)COP(=O)(O)OP(=O)(O)OC[C@@H]3[C@H]([C@H]([C@@H](O3)N4C=NC5=C(N=CN=C54)N)O)O)O)O', 'CC=O', 'C1=CC(=C[N+](=C1)[C@H]2[C@@H]([C@@H]([C@H](O2)COP(=O)([O-])OP(=O)(O)OC[C@@H]3[C@H]([C@H]([C@@H](O3)N4C=NC5=C(N=CN=C54)N)O)O)O)O)C(=O)N', 'CCO']\n",
      "smiles_list CC(=O)SCCNC(=O)CCNC(=O)[C@@H](C(C)(C)COP(=O)(O)OP(=O)(O)OC[C@@H]1[C@H]([C@H]([C@@H](O1)N2C=NC3=C(N=CN=C32)N)O)OP(=O)(O)O)O ['C(=O)[O-]', 'CC(=O)SCCNC(=O)CCNC(=O)[C@@H](C(C)(C)COP(=O)(O)OP(=O)(O)OC[C@@H]1[C@H]([C@H]([C@@H](O1)N2C=NC3=C(N=CN=C32)N)O)OP(=O)(O)O)O', 'CC(=O)C(=O)[O-]', 'CC(C)(COP(=O)(O)OP(=O)(O)OC[C@@H]1[C@H]([C@H]([C@@H](O1)N2C=NC3=C(N=CN=C32)N)O)OP(=O)(O)O)[C@H](C(=O)NCCC(=O)NCCS)O', 'CC(=O)[O-]', 'C1C=CN(C=C1C(=O)N)[C@H]2[C@@H]([C@@H]([C@H](O2)COP(=O)(O)OP(=O)(O)OC[C@@H]3[C@H]([C@H]([C@@H](O3)N4C=NC5=C(N=CN=C54)N)O)O)O)O', 'CC=O', 'C1=CC(=C[N+](=C1)[C@H]2[C@@H]([C@@H]([C@H](O2)COP(=O)([O-])OP(=O)(O)OC[C@@H]3[C@H]([C@H]([C@@H](O3)N4C=NC5=C(N=CN=C54)N)O)O)O)O)C(=O)N', 'CCO']\n",
      "smiles_list C1C=CN(C=C1C(=O)N)[C@H]2[C@@H]([C@@H]([C@H](O2)COP(=O)(O)OP(=O)(O)OC[C@@H]3[C@H]([C@H]([C@@H](O3)N4C=NC5=C(N=CN=C54)N)O)O)O)O ['C(=O)[O-]', 'CC(=O)SCCNC(=O)CCNC(=O)[C@@H](C(C)(C)COP(=O)(O)OP(=O)(O)OC[C@@H]1[C@H]([C@H]([C@@H](O1)N2C=NC3=C(N=CN=C32)N)O)OP(=O)(O)O)O', 'CC(=O)C(=O)[O-]', 'CC(C)(COP(=O)(O)OP(=O)(O)OC[C@@H]1[C@H]([C@H]([C@@H](O1)N2C=NC3=C(N=CN=C32)N)O)OP(=O)(O)O)[C@H](C(=O)NCCC(=O)NCCS)O', 'CC(=O)[O-]', 'C1C=CN(C=C1C(=O)N)[C@H]2[C@@H]([C@@H]([C@H](O2)COP(=O)(O)OP(=O)(O)OC[C@@H]3[C@H]([C@H]([C@@H](O3)N4C=NC5=C(N=CN=C54)N)O)O)O)O', 'CC=O', 'C1=CC(=C[N+](=C1)[C@H]2[C@@H]([C@@H]([C@H](O2)COP(=O)([O-])OP(=O)(O)OC[C@@H]3[C@H]([C@H]([C@@H](O3)N4C=NC5=C(N=CN=C54)N)O)O)O)O)C(=O)N', 'CCO']\n",
      "item {'substrates': ['Pyruvate', 'NADH', '(S)-lactate', 'NAD+'], 'SMILES': ['CC(=O)C(=O)[O-]', 'C1C=CN(C=C1C(=O)N)[C@H]2[C@@H]([C@@H]([C@H](O2)COP(=O)(O)OP(=O)(O)OC[C@@H]3[C@H]([C@H]([C@@H](O3)N4C=NC5=C(N=CN=C54)N)O)O)O)O', 'C[C@@H](C(=O)O)O', 'C1=CC(=C[N+](=C1)[C@H]2[C@@H]([C@@H]([C@H](O2)COP(=O)([O-])OP(=O)(O)OC[C@@H]3[C@H]([C@H]([C@@H](O3)N4C=NC5=C(N=CN=C54)N)O)O)O)O)C(=O)N'], 'sequences': ['MADKQRKKVILVGDGAVGSSYAFALVNQGIAQELGIVDLFKEKTQGDAEDLSHALAFTSPKKIYSADYSDASDADLVVLTSGAPQKPGETRLDLVEKNLRITKDVVTKIVASGFKGIFLVAANPVDILTYATWKFSGFPKNRVVGSGTSLDTARFRQALAEKVDVDARSIHAYIMGEHGDSEFAVWSHANVAGVKLEQWFQENDYLNEAEIVKLFESVRDAAYSIIAKKGATFYGVAVALARITKAILDDEHAVLPVSVFQDGQYGVSDCYLGQPAVVGAEGVVNPIHIPLNDAEMQKMEASGAQLKAIIDEAFAKEEFASAVKN', 'MADKQRKKVILVGDGAVGSSYAFALVNQGIAQELGIVDLFKEKTQGDAEDLSHALAFTSPKKIYSADYSDASDADLVVLTSGAPQKPGETRLDLVEKNLRITKDVVTKIVASGFKGIFLVAANPVDILTYATWKFSGFPKNRVVGSGTSLDTARFRQALAEKVDVDARSIHAYIMGEHGDSEFAVWSHANVAGVKLEQWFQENDYLNEAEIVKLFESVRDAAYSIIAKKGATFYGVAVALARITKAILDDEHAVLPVSVFQDGQYGVSDCYLGQPAVVGAEGVVNPIHIPLNDAEMQKMEASGAQLKAIIDEAFAKEEFASAVKN', 'MADKQRKKVILVGDGAVGSSYAFALVNQGIAQELGIVDLFKEKTQGDAEDLSHALAFTSPKKIYSADYSDASDADLVVLTSGAPQKPGETRLDLVEKNLRITKDVVTKIVASGFKGIFLVAANPVDILTYATWKFSGFPKNRVVGSGTSLDTARFRQALAEKVDVDARSIHAYIMGEHGDSEFAVWSHANVAGVKLEQWFQENDYLNEAEIVKLFESVRDAAYSIIAKKGATFYGVAVALARITKAILDDEHAVLPVSVFQDGQYGVSDCYLGQPAVVGAEGVVNPIHIPLNDAEMQKMEASGAQLKAIIDEAFAKEEFASAVKN', 'MADKQRKKVILVGDGAVGSSYAFALVNQGIAQELGIVDLFKEKTQGDAEDLSHALAFTSPKKIYSADYSDASDADLVVLTSGAPQKPGETRLDLVEKNLRITKDVVTKIVASGFKGIFLVAANPVDILTYATWKFSGFPKNRVVGSGTSLDTARFRQALAEKVDVDARSIHAYIMGEHGDSEFAVWSHANVAGVKLEQWFQENDYLNEAEIVKLFESVRDAAYSIIAKKGATFYGVAVALARITKAILDDEHAVLPVSVFQDGQYGVSDCYLGQPAVVGAEGVVNPIHIPLNDAEMQKMEASGAQLKAIIDEAFAKEEFASAVKN'], 'pdb_paths': ['seq5.seq', 'seq5.seq', 'seq5.seq', 'seq5.seq'], 'subs': ['CC(=O)C(=O)[O-]', 'C1C=CN(C=C1C(=O)N)[C@H]2[C@@H]([C@@H]([C@H](O2)COP(=O)(O)OP(=O)(O)OC[C@@H]3[C@H]([C@H]([C@@H](O3)N4C=NC5=C(N=CN=C54)N)O)O)O)O'], 'stoich': [1, 1, 1, 1], 'EC': '1.1.1.27'}\n",
      "starting R00703\n",
      "input |    | substrate   | SMILES                                                                                                                                 | sequence                                                                                                                                                                                                                                                                                                                              | pdbpath   |\n",
      "|---:|:------------|:---------------------------------------------------------------------------------------------------------------------------------------|:--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:----------|\n",
      "|  0 | Pyruvate    | CC(=O)C(=O)[O-]                                                                                                                        | MADKQRKKVILVGDGAVGSSYAFALVNQGIAQELGIVDLFKEKTQGDAEDLSHALAFTSPKKIYSADYSDASDADLVVLTSGAPQKPGETRLDLVEKNLRITKDVVTKIVASGFKGIFLVAANPVDILTYATWKFSGFPKNRVVGSGTSLDTARFRQALAEKVDVDARSIHAYIMGEHGDSEFAVWSHANVAGVKLEQWFQENDYLNEAEIVKLFESVRDAAYSIIAKKGATFYGVAVALARITKAILDDEHAVLPVSVFQDGQYGVSDCYLGQPAVVGAEGVVNPIHIPLNDAEMQKMEASGAQLKAIIDEAFAKEEFASAVKN | seq5.seq  |\n",
      "|  1 | NADH        | C1C=CN(C=C1C(=O)N)[C@H]2[C@@H]([C@@H]([C@H](O2)COP(=O)(O)OP(=O)(O)OC[C@@H]3[C@H]([C@H]([C@@H](O3)N4C=NC5=C(N=CN=C54)N)O)O)O)O          | MADKQRKKVILVGDGAVGSSYAFALVNQGIAQELGIVDLFKEKTQGDAEDLSHALAFTSPKKIYSADYSDASDADLVVLTSGAPQKPGETRLDLVEKNLRITKDVVTKIVASGFKGIFLVAANPVDILTYATWKFSGFPKNRVVGSGTSLDTARFRQALAEKVDVDARSIHAYIMGEHGDSEFAVWSHANVAGVKLEQWFQENDYLNEAEIVKLFESVRDAAYSIIAKKGATFYGVAVALARITKAILDDEHAVLPVSVFQDGQYGVSDCYLGQPAVVGAEGVVNPIHIPLNDAEMQKMEASGAQLKAIIDEAFAKEEFASAVKN | seq5.seq  |\n",
      "|  2 | (S)-lactate | C[C@@H](C(=O)O)O                                                                                                                       | MADKQRKKVILVGDGAVGSSYAFALVNQGIAQELGIVDLFKEKTQGDAEDLSHALAFTSPKKIYSADYSDASDADLVVLTSGAPQKPGETRLDLVEKNLRITKDVVTKIVASGFKGIFLVAANPVDILTYATWKFSGFPKNRVVGSGTSLDTARFRQALAEKVDVDARSIHAYIMGEHGDSEFAVWSHANVAGVKLEQWFQENDYLNEAEIVKLFESVRDAAYSIIAKKGATFYGVAVALARITKAILDDEHAVLPVSVFQDGQYGVSDCYLGQPAVVGAEGVVNPIHIPLNDAEMQKMEASGAQLKAIIDEAFAKEEFASAVKN | seq5.seq  |\n",
      "|  3 | NAD+        | C1=CC(=C[N+](=C1)[C@H]2[C@@H]([C@@H]([C@H](O2)COP(=O)([O-])OP(=O)(O)OC[C@@H]3[C@H]([C@H]([C@@H](O3)N4C=NC5=C(N=CN=C54)N)O)O)O)O)C(=O)N | MADKQRKKVILVGDGAVGSSYAFALVNQGIAQELGIVDLFKEKTQGDAEDLSHALAFTSPKKIYSADYSDASDADLVVLTSGAPQKPGETRLDLVEKNLRITKDVVTKIVASGFKGIFLVAANPVDILTYATWKFSGFPKNRVVGSGTSLDTARFRQALAEKVDVDARSIHAYIMGEHGDSEFAVWSHANVAGVKLEQWFQENDYLNEAEIVKLFESVRDAAYSIIAKKGATFYGVAVALARITKAILDDEHAVLPVSVFQDGQYGVSDCYLGQPAVVGAEGVVNPIHIPLNDAEMQKMEASGAQLKAIIDEAFAKEEFASAVKN | seq5.seq  |\n",
      "input2 ['CC(=O)C(=O)[O-]', 'C1C=CN(C=C1C(=O)N)[C@H]2[C@@H]([C@@H]([C@H](O2)COP(=O)(O)OP(=O)(O)OC[C@@H]3[C@H]([C@H]([C@@H](O3)N4C=NC5=C(N=CN=C54)N)O)O)O)O', 'C[C@@H](C(=O)O)O', 'C1=CC(=C[N+](=C1)[C@H]2[C@@H]([C@@H]([C@H](O2)COP(=O)([O-])OP(=O)(O)OC[C@@H]3[C@H]([C@H]([C@@H](O3)N4C=NC5=C(N=CN=C54)N)O)O)O)O)C(=O)N']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xuvin/.conda/envs/catpred/lib/python3.9/site-packages/rotary_embedding_torch/rotary_embedding_torch.py:35: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  @autocast(enabled = False)\n",
      "/home/xuvin/.conda/envs/catpred/lib/python3.9/site-packages/rotary_embedding_torch/rotary_embedding_torch.py:268: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  @autocast(enabled = False)\n",
      "\n",
      "0it [00:00, ?it/s]/home/xuvin/py_repos/wccm/src/catpred_pipeline/catpred/catpred/data/cache_utils.py:95: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(str(entry_path))\n",
      "\n",
      "5it [00:00, 68.94it/s]\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 39346.19it/s]\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 1289.21it/s]\n",
      "\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]/home/xuvin/py_repos/wccm/src/catpred_pipeline/catpred/catpred/utils.py:113: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(path, map_location=lambda storage, loc: storage)\n",
      "/home/xuvin/py_repos/wccm/src/catpred_pipeline/catpred/catpred/utils.py:446: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(path, map_location=lambda storage, loc: storage)\n",
      "\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.37s/it]\u001b[A\n",
      "\n",
      "                                             \u001b[A\n",
      " 10%|█         | 1/10 [00:04<00:37,  4.17s/it]/home/xuvin/py_repos/wccm/src/catpred_pipeline/catpred/catpred/utils.py:113: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(path, map_location=lambda storage, loc: storage)\n",
      "/home/xuvin/py_repos/wccm/src/catpred_pipeline/catpred/catpred/utils.py:446: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(path, map_location=lambda storage, loc: storage)\n",
      "\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "                                     \u001b[A\n",
      " 20%|██        | 2/10 [00:04<00:14,  1.87s/it]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "                                     \u001b[A\n",
      " 30%|███       | 3/10 [00:04<00:07,  1.14s/it]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "                                     \u001b[A\n",
      " 40%|████      | 4/10 [00:05<00:05,  1.16it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "                                     \u001b[A\n",
      " 50%|█████     | 5/10 [00:05<00:03,  1.53it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "                                     \u001b[A\n",
      " 60%|██████    | 6/10 [00:05<00:02,  1.88it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "                                     \u001b[A\n",
      " 70%|███████   | 7/10 [00:05<00:01,  2.32it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "                                     \u001b[A\n",
      " 80%|████████  | 8/10 [00:06<00:00,  2.31it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "                                     \u001b[A\n",
      " 90%|█████████ | 9/10 [00:06<00:00,  2.70it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "                                     \u001b[A\n",
      "100%|██████████| 10/10 [00:06<00:00,  3.02it/s]\n",
      "100%|██████████| 10/10 [00:06<00:00,  1.46it/s]\n",
      "\n",
      "/home/xuvin/.conda/envs/catpred/lib/python3.9/site-packages/rotary_embedding_torch/rotary_embedding_torch.py:35: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  @autocast(enabled = False)\n",
      "/home/xuvin/.conda/envs/catpred/lib/python3.9/site-packages/rotary_embedding_torch/rotary_embedding_torch.py:268: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  @autocast(enabled = False)\n",
      "\n",
      "0it [00:00, ?it/s]/home/xuvin/py_repos/wccm/src/catpred_pipeline/catpred/catpred/data/cache_utils.py:95: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(str(entry_path))\n",
      "\n",
      "1it [00:00,  5.65it/s]\n",
      "4it [00:00, 16.33it/s]\n",
      "\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 25890.77it/s]\n",
      "\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 813.60it/s]\n",
      "\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]/home/xuvin/py_repos/wccm/src/catpred_pipeline/catpred/catpred/utils.py:113: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(path, map_location=lambda storage, loc: storage)\n",
      "/home/xuvin/py_repos/wccm/src/catpred_pipeline/catpred/catpred/utils.py:446: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(path, map_location=lambda storage, loc: storage)\n",
      "\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.29s/it]\u001b[A\n",
      "\n",
      "                                             \u001b[A\n",
      " 10%|█         | 1/10 [00:02<00:26,  2.89s/it]/home/xuvin/py_repos/wccm/src/catpred_pipeline/catpred/catpred/utils.py:113: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(path, map_location=lambda storage, loc: storage)\n",
      "/home/xuvin/py_repos/wccm/src/catpred_pipeline/catpred/catpred/utils.py:446: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(path, map_location=lambda storage, loc: storage)\n",
      "\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "                                     \u001b[A\n",
      " 20%|██        | 2/10 [00:03<00:10,  1.37s/it]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "                                     \u001b[A\n",
      " 30%|███       | 3/10 [00:03<00:06,  1.16it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "                                     \u001b[A\n",
      " 40%|████      | 4/10 [00:03<00:04,  1.39it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "                                     \u001b[A\n",
      " 50%|█████     | 5/10 [00:04<00:02,  1.80it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "                                     \u001b[A\n",
      " 60%|██████    | 6/10 [00:04<00:01,  2.26it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "                                     \u001b[A\n",
      " 70%|███████   | 7/10 [00:04<00:01,  2.71it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "                                     \u001b[A\n",
      " 80%|████████  | 8/10 [00:05<00:00,  2.55it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "                                     \u001b[A\n",
      " 90%|█████████ | 9/10 [00:05<00:00,  2.84it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "                                     \u001b[A\n",
      "100%|██████████| 10/10 [00:05<00:00,  3.16it/s]\n",
      "100%|██████████| 10/10 [00:05<00:00,  1.79it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/xuvin/py_repos/wccm/src/catpred_pipeline/catpred\n",
      "Predicting.. This will take a while..\n",
      "calculating protein embed only on cpu\n",
      "Loading training args\n",
      "Loading models\n",
      "Setting molecule featurization parameters to default.\n",
      "Loading data\n",
      "Validating SMILES\n",
      "Test size = 4\n",
      "Creating protein model\n",
      "MoleculeModel(\n",
      "  (softplus): Softplus(beta=1.0, threshold=20.0)\n",
      "  (encoder): MPN(\n",
      "    (encoder): ModuleList(\n",
      "      (0): MPNEncoder(\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (act_func): ReLU()\n",
      "        (W_i): Linear(in_features=147, out_features=300, bias=False)\n",
      "        (W_h): Linear(in_features=300, out_features=300, bias=False)\n",
      "        (W_o): Linear(in_features=433, out_features=300, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (seq_embedder): Embedding(21, 36, padding_idx=20)\n",
      "  (rotary_embedder): RotaryEmbedding()\n",
      "  (multihead_attn): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=36, out_features=36, bias=True)\n",
      "  )\n",
      "  (attentive_pooler): AttentivePooling(\n",
      "    (linear1): Linear(in_features=1316, out_features=1316, bias=True)\n",
      "    (tanh): Tanh()\n",
      "    (linear2): Linear(in_features=1316, out_features=1, bias=True)\n",
      "    (softmax): Softmax(dim=1)\n",
      "  )\n",
      "  (readout): Sequential(\n",
      "    (0): Dropout(p=0.0, inplace=False)\n",
      "    (1): Linear(in_features=1616, out_features=300, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.0, inplace=False)\n",
      "    (4): Linear(in_features=300, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
      "Loading pretrained parameter \"seq_embedder.weight\".\n",
      "Loading pretrained parameter \"rotary_embedder.freqs\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_weight\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_bias\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.weight\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.bias\".\n",
      "Loading pretrained parameter \"readout.1.weight\".\n",
      "Loading pretrained parameter \"readout.1.bias\".\n",
      "Loading pretrained parameter \"readout.4.weight\".\n",
      "Loading pretrained parameter \"readout.4.bias\".\n",
      "Moving model to cuda\n",
      "Creating protein model\n",
      "MoleculeModel(\n",
      "  (softplus): Softplus(beta=1.0, threshold=20.0)\n",
      "  (encoder): MPN(\n",
      "    (encoder): ModuleList(\n",
      "      (0): MPNEncoder(\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (act_func): ReLU()\n",
      "        (W_i): Linear(in_features=147, out_features=300, bias=False)\n",
      "        (W_h): Linear(in_features=300, out_features=300, bias=False)\n",
      "        (W_o): Linear(in_features=433, out_features=300, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (seq_embedder): Embedding(21, 36, padding_idx=20)\n",
      "  (rotary_embedder): RotaryEmbedding()\n",
      "  (multihead_attn): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=36, out_features=36, bias=True)\n",
      "  )\n",
      "  (attentive_pooler): AttentivePooling(\n",
      "    (linear1): Linear(in_features=1316, out_features=1316, bias=True)\n",
      "    (tanh): Tanh()\n",
      "    (linear2): Linear(in_features=1316, out_features=1, bias=True)\n",
      "    (softmax): Softmax(dim=1)\n",
      "  )\n",
      "  (readout): Sequential(\n",
      "    (0): Dropout(p=0.0, inplace=False)\n",
      "    (1): Linear(in_features=1616, out_features=300, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.0, inplace=False)\n",
      "    (4): Linear(in_features=300, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
      "Loading pretrained parameter \"seq_embedder.weight\".\n",
      "Loading pretrained parameter \"rotary_embedder.freqs\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_weight\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_bias\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.weight\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.bias\".\n",
      "Loading pretrained parameter \"readout.1.weight\".\n",
      "Loading pretrained parameter \"readout.1.bias\".\n",
      "Loading pretrained parameter \"readout.4.weight\".\n",
      "Loading pretrained parameter \"readout.4.bias\".\n",
      "Moving model to cuda\n",
      "Creating protein model\n",
      "MoleculeModel(\n",
      "  (softplus): Softplus(beta=1.0, threshold=20.0)\n",
      "  (encoder): MPN(\n",
      "    (encoder): ModuleList(\n",
      "      (0): MPNEncoder(\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (act_func): ReLU()\n",
      "        (W_i): Linear(in_features=147, out_features=300, bias=False)\n",
      "        (W_h): Linear(in_features=300, out_features=300, bias=False)\n",
      "        (W_o): Linear(in_features=433, out_features=300, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (seq_embedder): Embedding(21, 36, padding_idx=20)\n",
      "  (rotary_embedder): RotaryEmbedding()\n",
      "  (multihead_attn): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=36, out_features=36, bias=True)\n",
      "  )\n",
      "  (attentive_pooler): AttentivePooling(\n",
      "    (linear1): Linear(in_features=1316, out_features=1316, bias=True)\n",
      "    (tanh): Tanh()\n",
      "    (linear2): Linear(in_features=1316, out_features=1, bias=True)\n",
      "    (softmax): Softmax(dim=1)\n",
      "  )\n",
      "  (readout): Sequential(\n",
      "    (0): Dropout(p=0.0, inplace=False)\n",
      "    (1): Linear(in_features=1616, out_features=300, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.0, inplace=False)\n",
      "    (4): Linear(in_features=300, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
      "Loading pretrained parameter \"seq_embedder.weight\".\n",
      "Loading pretrained parameter \"rotary_embedder.freqs\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_weight\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_bias\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.weight\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.bias\".\n",
      "Loading pretrained parameter \"readout.1.weight\".\n",
      "Loading pretrained parameter \"readout.1.bias\".\n",
      "Loading pretrained parameter \"readout.4.weight\".\n",
      "Loading pretrained parameter \"readout.4.bias\".\n",
      "Moving model to cuda\n",
      "Creating protein model\n",
      "MoleculeModel(\n",
      "  (softplus): Softplus(beta=1.0, threshold=20.0)\n",
      "  (encoder): MPN(\n",
      "    (encoder): ModuleList(\n",
      "      (0): MPNEncoder(\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (act_func): ReLU()\n",
      "        (W_i): Linear(in_features=147, out_features=300, bias=False)\n",
      "        (W_h): Linear(in_features=300, out_features=300, bias=False)\n",
      "        (W_o): Linear(in_features=433, out_features=300, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (seq_embedder): Embedding(21, 36, padding_idx=20)\n",
      "  (rotary_embedder): RotaryEmbedding()\n",
      "  (multihead_attn): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=36, out_features=36, bias=True)\n",
      "  )\n",
      "  (attentive_pooler): AttentivePooling(\n",
      "    (linear1): Linear(in_features=1316, out_features=1316, bias=True)\n",
      "    (tanh): Tanh()\n",
      "    (linear2): Linear(in_features=1316, out_features=1, bias=True)\n",
      "    (softmax): Softmax(dim=1)\n",
      "  )\n",
      "  (readout): Sequential(\n",
      "    (0): Dropout(p=0.0, inplace=False)\n",
      "    (1): Linear(in_features=1616, out_features=300, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.0, inplace=False)\n",
      "    (4): Linear(in_features=300, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
      "Loading pretrained parameter \"seq_embedder.weight\".\n",
      "Loading pretrained parameter \"rotary_embedder.freqs\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_weight\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_bias\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.weight\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.bias\".\n",
      "Loading pretrained parameter \"readout.1.weight\".\n",
      "Loading pretrained parameter \"readout.1.bias\".\n",
      "Loading pretrained parameter \"readout.4.weight\".\n",
      "Loading pretrained parameter \"readout.4.bias\".\n",
      "Moving model to cuda\n",
      "Creating protein model\n",
      "MoleculeModel(\n",
      "  (softplus): Softplus(beta=1.0, threshold=20.0)\n",
      "  (encoder): MPN(\n",
      "    (encoder): ModuleList(\n",
      "      (0): MPNEncoder(\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (act_func): ReLU()\n",
      "        (W_i): Linear(in_features=147, out_features=300, bias=False)\n",
      "        (W_h): Linear(in_features=300, out_features=300, bias=False)\n",
      "        (W_o): Linear(in_features=433, out_features=300, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (seq_embedder): Embedding(21, 36, padding_idx=20)\n",
      "  (rotary_embedder): RotaryEmbedding()\n",
      "  (multihead_attn): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=36, out_features=36, bias=True)\n",
      "  )\n",
      "  (attentive_pooler): AttentivePooling(\n",
      "    (linear1): Linear(in_features=1316, out_features=1316, bias=True)\n",
      "    (tanh): Tanh()\n",
      "    (linear2): Linear(in_features=1316, out_features=1, bias=True)\n",
      "    (softmax): Softmax(dim=1)\n",
      "  )\n",
      "  (readout): Sequential(\n",
      "    (0): Dropout(p=0.0, inplace=False)\n",
      "    (1): Linear(in_features=1616, out_features=300, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.0, inplace=False)\n",
      "    (4): Linear(in_features=300, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
      "Loading pretrained parameter \"seq_embedder.weight\".\n",
      "Loading pretrained parameter \"rotary_embedder.freqs\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_weight\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_bias\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.weight\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.bias\".\n",
      "Loading pretrained parameter \"readout.1.weight\".\n",
      "Loading pretrained parameter \"readout.1.bias\".\n",
      "Loading pretrained parameter \"readout.4.weight\".\n",
      "Loading pretrained parameter \"readout.4.bias\".\n",
      "Moving model to cuda\n",
      "Creating protein model\n",
      "MoleculeModel(\n",
      "  (softplus): Softplus(beta=1.0, threshold=20.0)\n",
      "  (encoder): MPN(\n",
      "    (encoder): ModuleList(\n",
      "      (0): MPNEncoder(\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (act_func): ReLU()\n",
      "        (W_i): Linear(in_features=147, out_features=300, bias=False)\n",
      "        (W_h): Linear(in_features=300, out_features=300, bias=False)\n",
      "        (W_o): Linear(in_features=433, out_features=300, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (seq_embedder): Embedding(21, 36, padding_idx=20)\n",
      "  (rotary_embedder): RotaryEmbedding()\n",
      "  (multihead_attn): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=36, out_features=36, bias=True)\n",
      "  )\n",
      "  (attentive_pooler): AttentivePooling(\n",
      "    (linear1): Linear(in_features=1316, out_features=1316, bias=True)\n",
      "    (tanh): Tanh()\n",
      "    (linear2): Linear(in_features=1316, out_features=1, bias=True)\n",
      "    (softmax): Softmax(dim=1)\n",
      "  )\n",
      "  (readout): Sequential(\n",
      "    (0): Dropout(p=0.0, inplace=False)\n",
      "    (1): Linear(in_features=1616, out_features=300, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.0, inplace=False)\n",
      "    (4): Linear(in_features=300, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
      "Loading pretrained parameter \"seq_embedder.weight\".\n",
      "Loading pretrained parameter \"rotary_embedder.freqs\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_weight\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_bias\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.weight\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.bias\".\n",
      "Loading pretrained parameter \"readout.1.weight\".\n",
      "Loading pretrained parameter \"readout.1.bias\".\n",
      "Loading pretrained parameter \"readout.4.weight\".\n",
      "Loading pretrained parameter \"readout.4.bias\".\n",
      "Moving model to cuda\n",
      "Creating protein model\n",
      "MoleculeModel(\n",
      "  (softplus): Softplus(beta=1.0, threshold=20.0)\n",
      "  (encoder): MPN(\n",
      "    (encoder): ModuleList(\n",
      "      (0): MPNEncoder(\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (act_func): ReLU()\n",
      "        (W_i): Linear(in_features=147, out_features=300, bias=False)\n",
      "        (W_h): Linear(in_features=300, out_features=300, bias=False)\n",
      "        (W_o): Linear(in_features=433, out_features=300, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (seq_embedder): Embedding(21, 36, padding_idx=20)\n",
      "  (rotary_embedder): RotaryEmbedding()\n",
      "  (multihead_attn): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=36, out_features=36, bias=True)\n",
      "  )\n",
      "  (attentive_pooler): AttentivePooling(\n",
      "    (linear1): Linear(in_features=1316, out_features=1316, bias=True)\n",
      "    (tanh): Tanh()\n",
      "    (linear2): Linear(in_features=1316, out_features=1, bias=True)\n",
      "    (softmax): Softmax(dim=1)\n",
      "  )\n",
      "  (readout): Sequential(\n",
      "    (0): Dropout(p=0.0, inplace=False)\n",
      "    (1): Linear(in_features=1616, out_features=300, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.0, inplace=False)\n",
      "    (4): Linear(in_features=300, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
      "Loading pretrained parameter \"seq_embedder.weight\".\n",
      "Loading pretrained parameter \"rotary_embedder.freqs\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_weight\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_bias\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.weight\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.bias\".\n",
      "Loading pretrained parameter \"readout.1.weight\".\n",
      "Loading pretrained parameter \"readout.1.bias\".\n",
      "Loading pretrained parameter \"readout.4.weight\".\n",
      "Loading pretrained parameter \"readout.4.bias\".\n",
      "Moving model to cuda\n",
      "Creating protein model\n",
      "MoleculeModel(\n",
      "  (softplus): Softplus(beta=1.0, threshold=20.0)\n",
      "  (encoder): MPN(\n",
      "    (encoder): ModuleList(\n",
      "      (0): MPNEncoder(\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (act_func): ReLU()\n",
      "        (W_i): Linear(in_features=147, out_features=300, bias=False)\n",
      "        (W_h): Linear(in_features=300, out_features=300, bias=False)\n",
      "        (W_o): Linear(in_features=433, out_features=300, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (seq_embedder): Embedding(21, 36, padding_idx=20)\n",
      "  (rotary_embedder): RotaryEmbedding()\n",
      "  (multihead_attn): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=36, out_features=36, bias=True)\n",
      "  )\n",
      "  (attentive_pooler): AttentivePooling(\n",
      "    (linear1): Linear(in_features=1316, out_features=1316, bias=True)\n",
      "    (tanh): Tanh()\n",
      "    (linear2): Linear(in_features=1316, out_features=1, bias=True)\n",
      "    (softmax): Softmax(dim=1)\n",
      "  )\n",
      "  (readout): Sequential(\n",
      "    (0): Dropout(p=0.0, inplace=False)\n",
      "    (1): Linear(in_features=1616, out_features=300, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.0, inplace=False)\n",
      "    (4): Linear(in_features=300, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
      "Loading pretrained parameter \"seq_embedder.weight\".\n",
      "Loading pretrained parameter \"rotary_embedder.freqs\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_weight\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_bias\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.weight\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.bias\".\n",
      "Loading pretrained parameter \"readout.1.weight\".\n",
      "Loading pretrained parameter \"readout.1.bias\".\n",
      "Loading pretrained parameter \"readout.4.weight\".\n",
      "Loading pretrained parameter \"readout.4.bias\".\n",
      "Moving model to cuda\n",
      "Creating protein model\n",
      "MoleculeModel(\n",
      "  (softplus): Softplus(beta=1.0, threshold=20.0)\n",
      "  (encoder): MPN(\n",
      "    (encoder): ModuleList(\n",
      "      (0): MPNEncoder(\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (act_func): ReLU()\n",
      "        (W_i): Linear(in_features=147, out_features=300, bias=False)\n",
      "        (W_h): Linear(in_features=300, out_features=300, bias=False)\n",
      "        (W_o): Linear(in_features=433, out_features=300, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (seq_embedder): Embedding(21, 36, padding_idx=20)\n",
      "  (rotary_embedder): RotaryEmbedding()\n",
      "  (multihead_attn): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=36, out_features=36, bias=True)\n",
      "  )\n",
      "  (attentive_pooler): AttentivePooling(\n",
      "    (linear1): Linear(in_features=1316, out_features=1316, bias=True)\n",
      "    (tanh): Tanh()\n",
      "    (linear2): Linear(in_features=1316, out_features=1, bias=True)\n",
      "    (softmax): Softmax(dim=1)\n",
      "  )\n",
      "  (readout): Sequential(\n",
      "    (0): Dropout(p=0.0, inplace=False)\n",
      "    (1): Linear(in_features=1616, out_features=300, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.0, inplace=False)\n",
      "    (4): Linear(in_features=300, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
      "Loading pretrained parameter \"seq_embedder.weight\".\n",
      "Loading pretrained parameter \"rotary_embedder.freqs\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_weight\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_bias\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.weight\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.bias\".\n",
      "Loading pretrained parameter \"readout.1.weight\".\n",
      "Loading pretrained parameter \"readout.1.bias\".\n",
      "Loading pretrained parameter \"readout.4.weight\".\n",
      "Loading pretrained parameter \"readout.4.bias\".\n",
      "Moving model to cuda\n",
      "Creating protein model\n",
      "MoleculeModel(\n",
      "  (softplus): Softplus(beta=1.0, threshold=20.0)\n",
      "  (encoder): MPN(\n",
      "    (encoder): ModuleList(\n",
      "      (0): MPNEncoder(\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (act_func): ReLU()\n",
      "        (W_i): Linear(in_features=147, out_features=300, bias=False)\n",
      "        (W_h): Linear(in_features=300, out_features=300, bias=False)\n",
      "        (W_o): Linear(in_features=433, out_features=300, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (seq_embedder): Embedding(21, 36, padding_idx=20)\n",
      "  (rotary_embedder): RotaryEmbedding()\n",
      "  (multihead_attn): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=36, out_features=36, bias=True)\n",
      "  )\n",
      "  (attentive_pooler): AttentivePooling(\n",
      "    (linear1): Linear(in_features=1316, out_features=1316, bias=True)\n",
      "    (tanh): Tanh()\n",
      "    (linear2): Linear(in_features=1316, out_features=1, bias=True)\n",
      "    (softmax): Softmax(dim=1)\n",
      "  )\n",
      "  (readout): Sequential(\n",
      "    (0): Dropout(p=0.0, inplace=False)\n",
      "    (1): Linear(in_features=1616, out_features=300, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.0, inplace=False)\n",
      "    (4): Linear(in_features=300, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
      "Loading pretrained parameter \"seq_embedder.weight\".\n",
      "Loading pretrained parameter \"rotary_embedder.freqs\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_weight\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_bias\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.weight\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.bias\".\n",
      "Loading pretrained parameter \"readout.1.weight\".\n",
      "Loading pretrained parameter \"readout.1.bias\".\n",
      "Loading pretrained parameter \"readout.4.weight\".\n",
      "Loading pretrained parameter \"readout.4.bias\".\n",
      "Moving model to cuda\n",
      "Saving predictions to ../pipeline_entry_kcat_input_output.csv\n",
      "Elapsed time = 0:00:06\n",
      "Prediction completed.\n",
      "Output saved to results/ pipeline_entry_kcat_input_output.csv\n",
      "\n",
      "/home/xuvin/py_repos/wccm/src/catpred_pipeline/catpred\n",
      "Predicting.. This will take a while..\n",
      "calculating protein embed only on cpu\n",
      "Loading training args\n",
      "Loading models\n",
      "Setting molecule featurization parameters to default.\n",
      "Loading data\n",
      "Validating SMILES\n",
      "Test size = 4\n",
      "Creating protein model\n",
      "MoleculeModel(\n",
      "  (softplus): Softplus(beta=1.0, threshold=20.0)\n",
      "  (encoder): MPN(\n",
      "    (encoder): ModuleList(\n",
      "      (0): MPNEncoder(\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (act_func): ReLU()\n",
      "        (W_i): Linear(in_features=147, out_features=300, bias=False)\n",
      "        (W_h): Linear(in_features=300, out_features=300, bias=False)\n",
      "        (W_o): Linear(in_features=433, out_features=300, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (seq_embedder): Embedding(21, 36, padding_idx=20)\n",
      "  (rotary_embedder): RotaryEmbedding()\n",
      "  (multihead_attn): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=36, out_features=36, bias=True)\n",
      "  )\n",
      "  (attentive_pooler): AttentivePooling(\n",
      "    (linear1): Linear(in_features=1316, out_features=1316, bias=True)\n",
      "    (tanh): Tanh()\n",
      "    (linear2): Linear(in_features=1316, out_features=1, bias=True)\n",
      "    (softmax): Softmax(dim=1)\n",
      "  )\n",
      "  (readout): Sequential(\n",
      "    (0): Dropout(p=0.0, inplace=False)\n",
      "    (1): Linear(in_features=1616, out_features=300, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.0, inplace=False)\n",
      "    (4): Linear(in_features=300, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
      "Loading pretrained parameter \"seq_embedder.weight\".\n",
      "Loading pretrained parameter \"rotary_embedder.freqs\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_weight\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_bias\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.weight\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.bias\".\n",
      "Loading pretrained parameter \"readout.1.weight\".\n",
      "Loading pretrained parameter \"readout.1.bias\".\n",
      "Loading pretrained parameter \"readout.4.weight\".\n",
      "Loading pretrained parameter \"readout.4.bias\".\n",
      "Moving model to cuda\n",
      "Creating protein model\n",
      "MoleculeModel(\n",
      "  (softplus): Softplus(beta=1.0, threshold=20.0)\n",
      "  (encoder): MPN(\n",
      "    (encoder): ModuleList(\n",
      "      (0): MPNEncoder(\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (act_func): ReLU()\n",
      "        (W_i): Linear(in_features=147, out_features=300, bias=False)\n",
      "        (W_h): Linear(in_features=300, out_features=300, bias=False)\n",
      "        (W_o): Linear(in_features=433, out_features=300, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (seq_embedder): Embedding(21, 36, padding_idx=20)\n",
      "  (rotary_embedder): RotaryEmbedding()\n",
      "  (multihead_attn): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=36, out_features=36, bias=True)\n",
      "  )\n",
      "  (attentive_pooler): AttentivePooling(\n",
      "    (linear1): Linear(in_features=1316, out_features=1316, bias=True)\n",
      "    (tanh): Tanh()\n",
      "    (linear2): Linear(in_features=1316, out_features=1, bias=True)\n",
      "    (softmax): Softmax(dim=1)\n",
      "  )\n",
      "  (readout): Sequential(\n",
      "    (0): Dropout(p=0.0, inplace=False)\n",
      "    (1): Linear(in_features=1616, out_features=300, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.0, inplace=False)\n",
      "    (4): Linear(in_features=300, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
      "Loading pretrained parameter \"seq_embedder.weight\".\n",
      "Loading pretrained parameter \"rotary_embedder.freqs\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_weight\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_bias\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.weight\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.bias\".\n",
      "Loading pretrained parameter \"readout.1.weight\".\n",
      "Loading pretrained parameter \"readout.1.bias\".\n",
      "Loading pretrained parameter \"readout.4.weight\".\n",
      "Loading pretrained parameter \"readout.4.bias\".\n",
      "Moving model to cuda\n",
      "Creating protein model\n",
      "MoleculeModel(\n",
      "  (softplus): Softplus(beta=1.0, threshold=20.0)\n",
      "  (encoder): MPN(\n",
      "    (encoder): ModuleList(\n",
      "      (0): MPNEncoder(\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (act_func): ReLU()\n",
      "        (W_i): Linear(in_features=147, out_features=300, bias=False)\n",
      "        (W_h): Linear(in_features=300, out_features=300, bias=False)\n",
      "        (W_o): Linear(in_features=433, out_features=300, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (seq_embedder): Embedding(21, 36, padding_idx=20)\n",
      "  (rotary_embedder): RotaryEmbedding()\n",
      "  (multihead_attn): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=36, out_features=36, bias=True)\n",
      "  )\n",
      "  (attentive_pooler): AttentivePooling(\n",
      "    (linear1): Linear(in_features=1316, out_features=1316, bias=True)\n",
      "    (tanh): Tanh()\n",
      "    (linear2): Linear(in_features=1316, out_features=1, bias=True)\n",
      "    (softmax): Softmax(dim=1)\n",
      "  )\n",
      "  (readout): Sequential(\n",
      "    (0): Dropout(p=0.0, inplace=False)\n",
      "    (1): Linear(in_features=1616, out_features=300, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.0, inplace=False)\n",
      "    (4): Linear(in_features=300, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
      "Loading pretrained parameter \"seq_embedder.weight\".\n",
      "Loading pretrained parameter \"rotary_embedder.freqs\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_weight\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_bias\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.weight\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.bias\".\n",
      "Loading pretrained parameter \"readout.1.weight\".\n",
      "Loading pretrained parameter \"readout.1.bias\".\n",
      "Loading pretrained parameter \"readout.4.weight\".\n",
      "Loading pretrained parameter \"readout.4.bias\".\n",
      "Moving model to cuda\n",
      "Creating protein model\n",
      "MoleculeModel(\n",
      "  (softplus): Softplus(beta=1.0, threshold=20.0)\n",
      "  (encoder): MPN(\n",
      "    (encoder): ModuleList(\n",
      "      (0): MPNEncoder(\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (act_func): ReLU()\n",
      "        (W_i): Linear(in_features=147, out_features=300, bias=False)\n",
      "        (W_h): Linear(in_features=300, out_features=300, bias=False)\n",
      "        (W_o): Linear(in_features=433, out_features=300, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (seq_embedder): Embedding(21, 36, padding_idx=20)\n",
      "  (rotary_embedder): RotaryEmbedding()\n",
      "  (multihead_attn): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=36, out_features=36, bias=True)\n",
      "  )\n",
      "  (attentive_pooler): AttentivePooling(\n",
      "    (linear1): Linear(in_features=1316, out_features=1316, bias=True)\n",
      "    (tanh): Tanh()\n",
      "    (linear2): Linear(in_features=1316, out_features=1, bias=True)\n",
      "    (softmax): Softmax(dim=1)\n",
      "  )\n",
      "  (readout): Sequential(\n",
      "    (0): Dropout(p=0.0, inplace=False)\n",
      "    (1): Linear(in_features=1616, out_features=300, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.0, inplace=False)\n",
      "    (4): Linear(in_features=300, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
      "Loading pretrained parameter \"seq_embedder.weight\".\n",
      "Loading pretrained parameter \"rotary_embedder.freqs\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_weight\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_bias\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.weight\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.bias\".\n",
      "Loading pretrained parameter \"readout.1.weight\".\n",
      "Loading pretrained parameter \"readout.1.bias\".\n",
      "Loading pretrained parameter \"readout.4.weight\".\n",
      "Loading pretrained parameter \"readout.4.bias\".\n",
      "Moving model to cuda\n",
      "Creating protein model\n",
      "MoleculeModel(\n",
      "  (softplus): Softplus(beta=1.0, threshold=20.0)\n",
      "  (encoder): MPN(\n",
      "    (encoder): ModuleList(\n",
      "      (0): MPNEncoder(\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (act_func): ReLU()\n",
      "        (W_i): Linear(in_features=147, out_features=300, bias=False)\n",
      "        (W_h): Linear(in_features=300, out_features=300, bias=False)\n",
      "        (W_o): Linear(in_features=433, out_features=300, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (seq_embedder): Embedding(21, 36, padding_idx=20)\n",
      "  (rotary_embedder): RotaryEmbedding()\n",
      "  (multihead_attn): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=36, out_features=36, bias=True)\n",
      "  )\n",
      "  (attentive_pooler): AttentivePooling(\n",
      "    (linear1): Linear(in_features=1316, out_features=1316, bias=True)\n",
      "    (tanh): Tanh()\n",
      "    (linear2): Linear(in_features=1316, out_features=1, bias=True)\n",
      "    (softmax): Softmax(dim=1)\n",
      "  )\n",
      "  (readout): Sequential(\n",
      "    (0): Dropout(p=0.0, inplace=False)\n",
      "    (1): Linear(in_features=1616, out_features=300, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.0, inplace=False)\n",
      "    (4): Linear(in_features=300, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
      "Loading pretrained parameter \"seq_embedder.weight\".\n",
      "Loading pretrained parameter \"rotary_embedder.freqs\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_weight\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_bias\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.weight\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.bias\".\n",
      "Loading pretrained parameter \"readout.1.weight\".\n",
      "Loading pretrained parameter \"readout.1.bias\".\n",
      "Loading pretrained parameter \"readout.4.weight\".\n",
      "Loading pretrained parameter \"readout.4.bias\".\n",
      "Moving model to cuda\n",
      "Creating protein model\n",
      "MoleculeModel(\n",
      "  (softplus): Softplus(beta=1.0, threshold=20.0)\n",
      "  (encoder): MPN(\n",
      "    (encoder): ModuleList(\n",
      "      (0): MPNEncoder(\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (act_func): ReLU()\n",
      "        (W_i): Linear(in_features=147, out_features=300, bias=False)\n",
      "        (W_h): Linear(in_features=300, out_features=300, bias=False)\n",
      "        (W_o): Linear(in_features=433, out_features=300, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (seq_embedder): Embedding(21, 36, padding_idx=20)\n",
      "  (rotary_embedder): RotaryEmbedding()\n",
      "  (multihead_attn): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=36, out_features=36, bias=True)\n",
      "  )\n",
      "  (attentive_pooler): AttentivePooling(\n",
      "    (linear1): Linear(in_features=1316, out_features=1316, bias=True)\n",
      "    (tanh): Tanh()\n",
      "    (linear2): Linear(in_features=1316, out_features=1, bias=True)\n",
      "    (softmax): Softmax(dim=1)\n",
      "  )\n",
      "  (readout): Sequential(\n",
      "    (0): Dropout(p=0.0, inplace=False)\n",
      "    (1): Linear(in_features=1616, out_features=300, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.0, inplace=False)\n",
      "    (4): Linear(in_features=300, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
      "Loading pretrained parameter \"seq_embedder.weight\".\n",
      "Loading pretrained parameter \"rotary_embedder.freqs\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_weight\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_bias\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.weight\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.bias\".\n",
      "Loading pretrained parameter \"readout.1.weight\".\n",
      "Loading pretrained parameter \"readout.1.bias\".\n",
      "Loading pretrained parameter \"readout.4.weight\".\n",
      "Loading pretrained parameter \"readout.4.bias\".\n",
      "Moving model to cuda\n",
      "Creating protein model\n",
      "MoleculeModel(\n",
      "  (softplus): Softplus(beta=1.0, threshold=20.0)\n",
      "  (encoder): MPN(\n",
      "    (encoder): ModuleList(\n",
      "      (0): MPNEncoder(\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (act_func): ReLU()\n",
      "        (W_i): Linear(in_features=147, out_features=300, bias=False)\n",
      "        (W_h): Linear(in_features=300, out_features=300, bias=False)\n",
      "        (W_o): Linear(in_features=433, out_features=300, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (seq_embedder): Embedding(21, 36, padding_idx=20)\n",
      "  (rotary_embedder): RotaryEmbedding()\n",
      "  (multihead_attn): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=36, out_features=36, bias=True)\n",
      "  )\n",
      "  (attentive_pooler): AttentivePooling(\n",
      "    (linear1): Linear(in_features=1316, out_features=1316, bias=True)\n",
      "    (tanh): Tanh()\n",
      "    (linear2): Linear(in_features=1316, out_features=1, bias=True)\n",
      "    (softmax): Softmax(dim=1)\n",
      "  )\n",
      "  (readout): Sequential(\n",
      "    (0): Dropout(p=0.0, inplace=False)\n",
      "    (1): Linear(in_features=1616, out_features=300, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.0, inplace=False)\n",
      "    (4): Linear(in_features=300, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
      "Loading pretrained parameter \"seq_embedder.weight\".\n",
      "Loading pretrained parameter \"rotary_embedder.freqs\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_weight\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_bias\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.weight\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.bias\".\n",
      "Loading pretrained parameter \"readout.1.weight\".\n",
      "Loading pretrained parameter \"readout.1.bias\".\n",
      "Loading pretrained parameter \"readout.4.weight\".\n",
      "Loading pretrained parameter \"readout.4.bias\".\n",
      "Moving model to cuda\n",
      "Creating protein model\n",
      "MoleculeModel(\n",
      "  (softplus): Softplus(beta=1.0, threshold=20.0)\n",
      "  (encoder): MPN(\n",
      "    (encoder): ModuleList(\n",
      "      (0): MPNEncoder(\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (act_func): ReLU()\n",
      "        (W_i): Linear(in_features=147, out_features=300, bias=False)\n",
      "        (W_h): Linear(in_features=300, out_features=300, bias=False)\n",
      "        (W_o): Linear(in_features=433, out_features=300, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (seq_embedder): Embedding(21, 36, padding_idx=20)\n",
      "  (rotary_embedder): RotaryEmbedding()\n",
      "  (multihead_attn): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=36, out_features=36, bias=True)\n",
      "  )\n",
      "  (attentive_pooler): AttentivePooling(\n",
      "    (linear1): Linear(in_features=1316, out_features=1316, bias=True)\n",
      "    (tanh): Tanh()\n",
      "    (linear2): Linear(in_features=1316, out_features=1, bias=True)\n",
      "    (softmax): Softmax(dim=1)\n",
      "  )\n",
      "  (readout): Sequential(\n",
      "    (0): Dropout(p=0.0, inplace=False)\n",
      "    (1): Linear(in_features=1616, out_features=300, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.0, inplace=False)\n",
      "    (4): Linear(in_features=300, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
      "Loading pretrained parameter \"seq_embedder.weight\".\n",
      "Loading pretrained parameter \"rotary_embedder.freqs\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_weight\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_bias\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.weight\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.bias\".\n",
      "Loading pretrained parameter \"readout.1.weight\".\n",
      "Loading pretrained parameter \"readout.1.bias\".\n",
      "Loading pretrained parameter \"readout.4.weight\".\n",
      "Loading pretrained parameter \"readout.4.bias\".\n",
      "Moving model to cuda\n",
      "Creating protein model\n",
      "MoleculeModel(\n",
      "  (softplus): Softplus(beta=1.0, threshold=20.0)\n",
      "  (encoder): MPN(\n",
      "    (encoder): ModuleList(\n",
      "      (0): MPNEncoder(\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (act_func): ReLU()\n",
      "        (W_i): Linear(in_features=147, out_features=300, bias=False)\n",
      "        (W_h): Linear(in_features=300, out_features=300, bias=False)\n",
      "        (W_o): Linear(in_features=433, out_features=300, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (seq_embedder): Embedding(21, 36, padding_idx=20)\n",
      "  (rotary_embedder): RotaryEmbedding()\n",
      "  (multihead_attn): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=36, out_features=36, bias=True)\n",
      "  )\n",
      "  (attentive_pooler): AttentivePooling(\n",
      "    (linear1): Linear(in_features=1316, out_features=1316, bias=True)\n",
      "    (tanh): Tanh()\n",
      "    (linear2): Linear(in_features=1316, out_features=1, bias=True)\n",
      "    (softmax): Softmax(dim=1)\n",
      "  )\n",
      "  (readout): Sequential(\n",
      "    (0): Dropout(p=0.0, inplace=False)\n",
      "    (1): Linear(in_features=1616, out_features=300, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.0, inplace=False)\n",
      "    (4): Linear(in_features=300, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
      "Loading pretrained parameter \"seq_embedder.weight\".\n",
      "Loading pretrained parameter \"rotary_embedder.freqs\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_weight\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_bias\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.weight\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.bias\".\n",
      "Loading pretrained parameter \"readout.1.weight\".\n",
      "Loading pretrained parameter \"readout.1.bias\".\n",
      "Loading pretrained parameter \"readout.4.weight\".\n",
      "Loading pretrained parameter \"readout.4.bias\".\n",
      "Moving model to cuda\n",
      "Creating protein model\n",
      "MoleculeModel(\n",
      "  (softplus): Softplus(beta=1.0, threshold=20.0)\n",
      "  (encoder): MPN(\n",
      "    (encoder): ModuleList(\n",
      "      (0): MPNEncoder(\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (act_func): ReLU()\n",
      "        (W_i): Linear(in_features=147, out_features=300, bias=False)\n",
      "        (W_h): Linear(in_features=300, out_features=300, bias=False)\n",
      "        (W_o): Linear(in_features=433, out_features=300, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (seq_embedder): Embedding(21, 36, padding_idx=20)\n",
      "  (rotary_embedder): RotaryEmbedding()\n",
      "  (multihead_attn): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=36, out_features=36, bias=True)\n",
      "  )\n",
      "  (attentive_pooler): AttentivePooling(\n",
      "    (linear1): Linear(in_features=1316, out_features=1316, bias=True)\n",
      "    (tanh): Tanh()\n",
      "    (linear2): Linear(in_features=1316, out_features=1, bias=True)\n",
      "    (softmax): Softmax(dim=1)\n",
      "  )\n",
      "  (readout): Sequential(\n",
      "    (0): Dropout(p=0.0, inplace=False)\n",
      "    (1): Linear(in_features=1616, out_features=300, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.0, inplace=False)\n",
      "    (4): Linear(in_features=300, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
      "Loading pretrained parameter \"seq_embedder.weight\".\n",
      "Loading pretrained parameter \"rotary_embedder.freqs\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_weight\".\n",
      "Loading pretrained parameter \"multihead_attn.in_proj_bias\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.weight\".\n",
      "Loading pretrained parameter \"multihead_attn.out_proj.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear1.bias\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.weight\".\n",
      "Loading pretrained parameter \"attentive_pooler.linear2.bias\".\n",
      "Loading pretrained parameter \"readout.1.weight\".\n",
      "Loading pretrained parameter \"readout.1.bias\".\n",
      "Loading pretrained parameter \"readout.4.weight\".\n",
      "Loading pretrained parameter \"readout.4.bias\".\n",
      "Moving model to cuda\n",
      "Saving predictions to ../pipeline_entry_km_input_output.csv\n",
      "Elapsed time = 0:00:07\n",
      "Prediction completed.\n",
      "Output saved to results/ pipeline_entry_km_input_output.csv\n",
      "\n",
      "output |    |   Unnamed: 0.2 |   Unnamed: 0.1 |   Unnamed: 0 | substrate   | SMILES                                                                                                                    | sequence                                                                                                                                                                                                                                                                                                                              | pdbpath   |   log10kcat_max |   log10kcat_max_mve_uncal_var |   log10kcat_max_model_0 |   log10kcat_max_model_1 |   log10kcat_max_model_2 |   log10kcat_max_model_3 |   log10kcat_max_model_4 |   log10kcat_max_model_5 |   log10kcat_max_model_6 |   log10kcat_max_model_7 |   log10kcat_max_model_8 |   log10kcat_max_model_9 |   Prediction_(s^(-1)) |   Prediction_log10 |   SD_total |   SD_aleatoric |   SD_epistemic |\n",
      "|---:|---------------:|---------------:|-------------:|:------------|:--------------------------------------------------------------------------------------------------------------------------|:--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:----------|----------------:|------------------------------:|------------------------:|------------------------:|------------------------:|------------------------:|------------------------:|------------------------:|------------------------:|------------------------:|------------------------:|------------------------:|----------------------:|-------------------:|-----------:|---------------:|---------------:|\n",
      "|  0 |              0 |              0 |            0 | Pyruvate    | CC(=O)C(=O)[O-]                                                                                                           | MADKQRKKVILVGDGAVGSSYAFALVNQGIAQELGIVDLFKEKTQGDAEDLSHALAFTSPKKIYSADYSDASDADLVVLTSGAPQKPGETRLDLVEKNLRITKDVVTKIVASGFKGIFLVAANPVDILTYATWKFSGFPKNRVVGSGTSLDTARFRQALAEKVDVDARSIHAYIMGEHGDSEFAVWSHANVAGVKLEQWFQENDYLNEAEIVKLFESVRDAAYSIIAKKGATFYGVAVALARITKAILDDEHAVLPVSVFQDGQYGVSDCYLGQPAVVGAEGVVNPIHIPLNDAEMQKMEASGAQLKAIIDEAFAKEEFASAVKN | seq5.seq  |         1.53357 |                      0.769294 |                 1.92397 |                 2.86434 |                 1.54522 |                0.837428 |               0.0855011 |                 1.96099 |                 1.11847 |                 1.04514 |                 1.61541 |                 2.33925 |               34.1643 |            1.53357 |   0.877094 |       0.440289 |       0.758577 |\n",
      "|  1 |              1 |              1 |            1 | NADH        | NC(=O)C1=CN([C@@H]2O[C@H](COP(=O)(O)OP(=O)(O)OC[C@H]3O[C@@H](n4cnc5c(N)ncnc54)[C@H](O)[C@@H]3O)[C@@H](O)[C@H]2O)C=CC1     | MADKQRKKVILVGDGAVGSSYAFALVNQGIAQELGIVDLFKEKTQGDAEDLSHALAFTSPKKIYSADYSDASDADLVVLTSGAPQKPGETRLDLVEKNLRITKDVVTKIVASGFKGIFLVAANPVDILTYATWKFSGFPKNRVVGSGTSLDTARFRQALAEKVDVDARSIHAYIMGEHGDSEFAVWSHANVAGVKLEQWFQENDYLNEAEIVKLFESVRDAAYSIIAKKGATFYGVAVALARITKAILDDEHAVLPVSVFQDGQYGVSDCYLGQPAVVGAEGVVNPIHIPLNDAEMQKMEASGAQLKAIIDEAFAKEEFASAVKN | seq5.seq  |         3.1304  |                      0.908524 |                 3.75294 |                 3.17237 |                 3.02244 |                2.55032  |               2.63297   |                 2.70273 |                 1.93841 |                 3.51514 |                 3.60903 |                 4.40763 |             1350.2    |            3.1304  |   0.953165 |       0.669938 |       0.678017 |\n",
      "|  2 |              2 |              2 |            2 | (S)-lactate | C[C@H](O)C(=O)O                                                                                                           | MADKQRKKVILVGDGAVGSSYAFALVNQGIAQELGIVDLFKEKTQGDAEDLSHALAFTSPKKIYSADYSDASDADLVVLTSGAPQKPGETRLDLVEKNLRITKDVVTKIVASGFKGIFLVAANPVDILTYATWKFSGFPKNRVVGSGTSLDTARFRQALAEKVDVDARSIHAYIMGEHGDSEFAVWSHANVAGVKLEQWFQENDYLNEAEIVKLFESVRDAAYSIIAKKGATFYGVAVALARITKAILDDEHAVLPVSVFQDGQYGVSDCYLGQPAVVGAEGVVNPIHIPLNDAEMQKMEASGAQLKAIIDEAFAKEEFASAVKN | seq5.seq  |         3.15645 |                      0.945875 |                 3.87675 |                 3.32619 |                 2.8107  |                2.45695  |               2.69048   |                 2.8747  |                 1.85362 |                 3.68513 |                 3.4476  |                 4.54242 |             1433.69   |            3.15645 |   0.972561 |       0.634289 |       0.737261 |\n",
      "|  3 |              3 |              3 |            3 | NAD+        | NC(=O)c1ccc[n+]([C@@H]2O[C@H](COP(=O)([O-])OP(=O)(O)OC[C@H]3O[C@@H](n4cnc5c(N)ncnc54)[C@H](O)[C@@H]3O)[C@@H](O)[C@H]2O)c1 | MADKQRKKVILVGDGAVGSSYAFALVNQGIAQELGIVDLFKEKTQGDAEDLSHALAFTSPKKIYSADYSDASDADLVVLTSGAPQKPGETRLDLVEKNLRITKDVVTKIVASGFKGIFLVAANPVDILTYATWKFSGFPKNRVVGSGTSLDTARFRQALAEKVDVDARSIHAYIMGEHGDSEFAVWSHANVAGVKLEQWFQENDYLNEAEIVKLFESVRDAAYSIIAKKGATFYGVAVALARITKAILDDEHAVLPVSVFQDGQYGVSDCYLGQPAVVGAEGVVNPIHIPLNDAEMQKMEASGAQLKAIIDEAFAKEEFASAVKN | seq5.seq  |         2.85178 |                      1.12348  |                 3.47414 |                 3.08037 |                 2.73237 |                2.12597  |               2.05139   |                 2.3959  |                 1.85689 |                 3.27121 |                 3.31255 |                 4.217   |              710.852  |            2.85178 |   1.05994  |       0.785103 |       0.712106 |\n",
      "SMILES ['CC(=O)C(=O)[O-]', 'C1C=CN(C=C1C(=O)N)[C@H]2[C@@H]([C@@H]([C@H](O2)COP(=O)(O)OP(=O)(O)OC[C@@H]3[C@H]([C@H]([C@@H](O3)N4C=NC5=C(N=CN=C54)N)O)O)O)O', 'C[C@@H](C(=O)O)O', 'C1=CC(=C[N+](=C1)[C@H]2[C@@H]([C@@H]([C@H](O2)COP(=O)([O-])OP(=O)(O)OC[C@@H]3[C@H]([C@H]([C@@H](O3)N4C=NC5=C(N=CN=C54)N)O)O)O)O)C(=O)N']\n",
      "smiles_list CC(=O)C(=O)[O-] ['C(=O)[O-]', 'CC(=O)SCCNC(=O)CCNC(=O)[C@@H](C(C)(C)COP(=O)(O)OP(=O)(O)OC[C@@H]1[C@H]([C@H]([C@@H](O1)N2C=NC3=C(N=CN=C32)N)O)OP(=O)(O)O)O', 'CC(=O)C(=O)[O-]', 'CC(C)(COP(=O)(O)OP(=O)(O)OC[C@@H]1[C@H]([C@H]([C@@H](O1)N2C=NC3=C(N=CN=C32)N)O)OP(=O)(O)O)[C@H](C(=O)NCCC(=O)NCCS)O', 'CC(=O)[O-]', 'C1C=CN(C=C1C(=O)N)[C@H]2[C@@H]([C@@H]([C@H](O2)COP(=O)(O)OP(=O)(O)OC[C@@H]3[C@H]([C@H]([C@@H](O3)N4C=NC5=C(N=CN=C54)N)O)O)O)O', 'CC=O', 'C1=CC(=C[N+](=C1)[C@H]2[C@@H]([C@@H]([C@H](O2)COP(=O)([O-])OP(=O)(O)OC[C@@H]3[C@H]([C@H]([C@@H](O3)N4C=NC5=C(N=CN=C54)N)O)O)O)O)C(=O)N', 'CCO']\n",
      "smiles_list C1C=CN(C=C1C(=O)N)[C@H]2[C@@H]([C@@H]([C@H](O2)COP(=O)(O)OP(=O)(O)OC[C@@H]3[C@H]([C@H]([C@@H](O3)N4C=NC5=C(N=CN=C54)N)O)O)O)O ['C(=O)[O-]', 'CC(=O)SCCNC(=O)CCNC(=O)[C@@H](C(C)(C)COP(=O)(O)OP(=O)(O)OC[C@@H]1[C@H]([C@H]([C@@H](O1)N2C=NC3=C(N=CN=C32)N)O)OP(=O)(O)O)O', 'CC(=O)C(=O)[O-]', 'CC(C)(COP(=O)(O)OP(=O)(O)OC[C@@H]1[C@H]([C@H]([C@@H](O1)N2C=NC3=C(N=CN=C32)N)O)OP(=O)(O)O)[C@H](C(=O)NCCC(=O)NCCS)O', 'CC(=O)[O-]', 'C1C=CN(C=C1C(=O)N)[C@H]2[C@@H]([C@@H]([C@H](O2)COP(=O)(O)OP(=O)(O)OC[C@@H]3[C@H]([C@H]([C@@H](O3)N4C=NC5=C(N=CN=C54)N)O)O)O)O', 'CC=O', 'C1=CC(=C[N+](=C1)[C@H]2[C@@H]([C@@H]([C@H](O2)COP(=O)([O-])OP(=O)(O)OC[C@@H]3[C@H]([C@H]([C@@H](O3)N4C=NC5=C(N=CN=C54)N)O)O)O)O)C(=O)N', 'CCO']\n",
      "smiles_list C[C@@H](C(=O)O)O ['C(=O)[O-]', 'CC(=O)SCCNC(=O)CCNC(=O)[C@@H](C(C)(C)COP(=O)(O)OP(=O)(O)OC[C@@H]1[C@H]([C@H]([C@@H](O1)N2C=NC3=C(N=CN=C32)N)O)OP(=O)(O)O)O', 'CC(=O)C(=O)[O-]', 'CC(C)(COP(=O)(O)OP(=O)(O)OC[C@@H]1[C@H]([C@H]([C@@H](O1)N2C=NC3=C(N=CN=C32)N)O)OP(=O)(O)O)[C@H](C(=O)NCCC(=O)NCCS)O', 'CC(=O)[O-]', 'C1C=CN(C=C1C(=O)N)[C@H]2[C@@H]([C@@H]([C@H](O2)COP(=O)(O)OP(=O)(O)OC[C@@H]3[C@H]([C@H]([C@@H](O3)N4C=NC5=C(N=CN=C54)N)O)O)O)O', 'CC=O', 'C1=CC(=C[N+](=C1)[C@H]2[C@@H]([C@@H]([C@H](O2)COP(=O)([O-])OP(=O)(O)OC[C@@H]3[C@H]([C@H]([C@@H](O3)N4C=NC5=C(N=CN=C54)N)O)O)O)O)C(=O)N', 'CCO', 'C[C@@H](C(=O)O)O']\n",
      "smiles_list C1=CC(=C[N+](=C1)[C@H]2[C@@H]([C@@H]([C@H](O2)COP(=O)([O-])OP(=O)(O)OC[C@@H]3[C@H]([C@H]([C@@H](O3)N4C=NC5=C(N=CN=C54)N)O)O)O)O)C(=O)N ['C(=O)[O-]', 'CC(=O)SCCNC(=O)CCNC(=O)[C@@H](C(C)(C)COP(=O)(O)OP(=O)(O)OC[C@@H]1[C@H]([C@H]([C@@H](O1)N2C=NC3=C(N=CN=C32)N)O)OP(=O)(O)O)O', 'CC(=O)C(=O)[O-]', 'CC(C)(COP(=O)(O)OP(=O)(O)OC[C@@H]1[C@H]([C@H]([C@@H](O1)N2C=NC3=C(N=CN=C32)N)O)OP(=O)(O)O)[C@H](C(=O)NCCC(=O)NCCS)O', 'CC(=O)[O-]', 'C1C=CN(C=C1C(=O)N)[C@H]2[C@@H]([C@@H]([C@H](O2)COP(=O)(O)OP(=O)(O)OC[C@@H]3[C@H]([C@H]([C@@H](O3)N4C=NC5=C(N=CN=C54)N)O)O)O)O', 'CC=O', 'C1=CC(=C[N+](=C1)[C@H]2[C@@H]([C@@H]([C@H](O2)COP(=O)([O-])OP(=O)(O)OC[C@@H]3[C@H]([C@H]([C@@H](O3)N4C=NC5=C(N=CN=C54)N)O)O)O)O)C(=O)N', 'CCO', 'C[C@@H](C(=O)O)O']\n",
      "kin_param_bundle {'R00212': {'subs': {'C(=O)[O-]': {'meta_num': 0, 'meta_name': 'Formate', 'k_cat': 10.055570987342165, 'K_m': 13.844884834853536, 'stoich': 1, 'enz_num': 0}, 'CC(=O)SCCNC(=O)CCNC(=O)[C@@H](C(C)(C)COP(=O)(O)OP(=O)(O)OC[C@@H]1[C@H]([C@H]([C@@H](O1)N2C=NC3=C(N=CN=C32)N)O)OP(=O)(O)O)O': {'meta_num': 1, 'meta_name': 'Acetyl-CoA', 'k_cat': 8.002823257765913, 'K_m': 0.10558599605791594, 'stoich': 1, 'enz_num': 0}}, 'prods': {'CC(=O)C(=O)[O-]': {'meta_num': 2, 'meta_name': 'Pyruvate', 'k_cat': 0.699861431034104, 'K_m': 3.7030182513562626, 'stoich': 1, 'enz_num': 0}, 'CC(C)(COP(=O)(O)OP(=O)(O)OC[C@@H]1[C@H]([C@H]([C@@H](O1)N2C=NC3=C(N=CN=C32)N)O)OP(=O)(O)O)[C@H](C(=O)NCCC(=O)NCCS)O': {'meta_num': 3, 'meta_name': 'CoA', 'k_cat': 8.146511911230082, 'K_m': 0.08583361139178317, 'stoich': 1, 'enz_num': 0}}, 'enz': {'enz_EC': '2.3.1.54', 'enz_num': 0}}, 'R00711': {'subs': {'CC(=O)[O-]': {'meta_num': 4, 'meta_name': 'Acetate', 'k_cat': 91.13363242250604, 'K_m': 1.4673414432132363, 'stoich': 1, 'enz_num': 1}, 'C1C=CN(C=C1C(=O)N)[C@H]2[C@@H]([C@@H]([C@H](O2)COP(=O)(O)OP(=O)(O)OC[C@@H]3[C@H]([C@H]([C@@H](O3)N4C=NC5=C(N=CN=C54)N)O)O)O)O': {'meta_num': 5, 'meta_name': 'NADH', 'k_cat': 22.386048857294554, 'K_m': 0.08149449754277381, 'stoich': 1, 'enz_num': 1}}, 'prods': {'CC=O': {'meta_num': 6, 'meta_name': 'Acetaldehyde', 'k_cat': 27.851824424527813, 'K_m': 3.6298932782879043, 'stoich': 1, 'enz_num': 1}, 'C1=CC(=C[N+](=C1)[C@H]2[C@@H]([C@@H]([C@H](O2)COP(=O)([O-])OP(=O)(O)OC[C@@H]3[C@H]([C@H]([C@@H](O3)N4C=NC5=C(N=CN=C54)N)O)O)O)O)C(=O)N': {'meta_num': 7, 'meta_name': 'NAD+', 'k_cat': 20.34650868513414, 'K_m': 0.35080092962267617, 'stoich': 1, 'enz_num': 1}}, 'enz': {'enz_EC': '1.2.1.3', 'enz_num': 1}}, 'R00746': {'subs': {'CCO': {'meta_num': 8, 'meta_name': 'Ethanol', 'k_cat': 231.63150746705503, 'K_m': 3.6951917513171404, 'stoich': 1, 'enz_num': 2}, 'C1=CC(=C[N+](=C1)[C@H]2[C@@H]([C@@H]([C@H](O2)COP(=O)([O-])OP(=O)(O)OC[C@@H]3[C@H]([C@H]([C@@H](O3)N4C=NC5=C(N=CN=C54)N)O)O)O)O)C(=O)N': {'meta_num': 7, 'meta_name': 'NAD+', 'k_cat': 215.98127199500323, 'K_m': 0.28327345172869894, 'stoich': 1, 'enz_num': 2}}, 'prods': {'CC=O': {'meta_num': 6, 'meta_name': 'Acetaldehyde', 'k_cat': 322.1749664211754, 'K_m': 1.6453886389787897, 'stoich': 1, 'enz_num': 2}, 'C1C=CN(C=C1C(=O)N)[C@H]2[C@@H]([C@@H]([C@H](O2)COP(=O)(O)OP(=O)(O)OC[C@@H]3[C@H]([C@H]([C@@H](O3)N4C=NC5=C(N=CN=C54)N)O)O)O)O': {'meta_num': 5, 'meta_name': 'NADH', 'k_cat': 321.4767660701479, 'K_m': 0.0882936817212929, 'stoich': 1, 'enz_num': 2}}, 'enz': {'enz_EC': '1.1.1.71', 'enz_num': 2}}, 'R00228': {'subs': {'CC=O': {'meta_num': 6, 'meta_name': 'Acetaldehyde', 'k_cat': 8.991911966615412, 'K_m': 11.332360232182543, 'stoich': 1, 'enz_num': 3}, 'CC(C)(COP(=O)(O)OP(=O)(O)OC[C@@H]1[C@H]([C@H]([C@@H](O1)N2C=NC3=C(N=CN=C32)N)O)OP(=O)(O)O)[C@H](C(=O)NCCC(=O)NCCS)O': {'meta_num': 3, 'meta_name': 'CoA', 'k_cat': 8.164417036225661, 'K_m': 0.08566024557747152, 'stoich': 1, 'enz_num': 3}, 'C1=CC(=C[N+](=C1)[C@H]2[C@@H]([C@@H]([C@H](O2)COP(=O)([O-])OP(=O)(O)OC[C@@H]3[C@H]([C@H]([C@@H](O3)N4C=NC5=C(N=CN=C54)N)O)O)O)O)C(=O)N': {'meta_num': 7, 'meta_name': 'NAD+', 'k_cat': 7.085911624231392, 'K_m': 0.46081919932984494, 'stoich': 1, 'enz_num': 3}}, 'prods': {'CC(=O)SCCNC(=O)CCNC(=O)[C@@H](C(C)(C)COP(=O)(O)OP(=O)(O)OC[C@@H]1[C@H]([C@H]([C@@H](O1)N2C=NC3=C(N=CN=C32)N)O)OP(=O)(O)O)O': {'meta_num': 1, 'meta_name': 'Acetyl-CoA', 'k_cat': 8.013727221288514, 'K_m': 0.10531545857093781, 'stoich': 1, 'enz_num': 3}, 'C1C=CN(C=C1C(=O)N)[C@H]2[C@@H]([C@@H]([C@H](O2)COP(=O)(O)OP(=O)(O)OC[C@@H]3[C@H]([C@H]([C@@H](O3)N4C=NC5=C(N=CN=C54)N)O)O)O)O': {'meta_num': 5, 'meta_name': 'NADH', 'k_cat': 11.398230023431234, 'K_m': 0.12316464787568344, 'stoich': 1, 'enz_num': 3}}, 'enz': {'enz_EC': '1.2.1.10', 'enz_num': 3}}, 'R00703': {'subs': {'CC(=O)C(=O)[O-]': {'meta_num': 2, 'meta_name': 'Pyruvate', 'k_cat': 34.16428449182754, 'K_m': 1.5892967640480495, 'stoich': 1, 'enz_num': 4}, 'C1C=CN(C=C1C(=O)N)[C@H]2[C@@H]([C@@H]([C@H](O2)COP(=O)(O)OP(=O)(O)OC[C@@H]3[C@H]([C@H]([C@@H](O3)N4C=NC5=C(N=CN=C54)N)O)O)O)O': {'meta_num': 5, 'meta_name': 'NADH', 'k_cat': 1350.2027262437875, 'K_m': 0.03617133426616177, 'stoich': 1, 'enz_num': 4}}, 'prods': {'C[C@@H](C(=O)O)O': {'meta_num': 9, 'meta_name': '(S)-lactate', 'k_cat': 1433.6886729705807, 'K_m': 71.5960563192227, 'stoich': 1, 'enz_num': 4}, 'C1=CC(=C[N+](=C1)[C@H]2[C@@H]([C@@H]([C@H](O2)COP(=O)([O-])OP(=O)(O)OC[C@@H]3[C@H]([C@H]([C@@H](O3)N4C=NC5=C(N=CN=C54)N)O)O)O)O)C(=O)N': {'meta_num': 7, 'meta_name': 'NAD+', 'k_cat': 710.8523616339278, 'K_m': 0.22553799351110207, 'stoich': 1, 'enz_num': 4}}, 'enz': {'enz_EC': '1.1.1.27', 'enz_num': 4}}, 'meta_ct': 10, 'enz_ct': 5}\n",
      "enz_ECs ['1.1.1.27', '1.2.1.10', '2.3.1.54', '1.1.1.71', '1.2.1.3']\n",
      "meta_smies ['C(=O)[O-]', 'CC(=O)SCCNC(=O)CCNC(=O)[C@@H](C(C)(C)COP(=O)(O)OP(=O)(O)OC[C@@H]1[C@H]([C@H]([C@@H](O1)N2C=NC3=C(N=CN=C32)N)O)OP(=O)(O)O)O', 'CC(=O)C(=O)[O-]', 'CC(C)(COP(=O)(O)OP(=O)(O)OC[C@@H]1[C@H]([C@H]([C@@H](O1)N2C=NC3=C(N=CN=C32)N)O)OP(=O)(O)O)[C@H](C(=O)NCCC(=O)NCCS)O', 'CC(=O)[O-]', 'C1C=CN(C=C1C(=O)N)[C@H]2[C@@H]([C@@H]([C@H](O2)COP(=O)(O)OP(=O)(O)OC[C@@H]3[C@H]([C@H]([C@@H](O3)N4C=NC5=C(N=CN=C54)N)O)O)O)O', 'CC=O', 'C1=CC(=C[N+](=C1)[C@H]2[C@@H]([C@@H]([C@H](O2)COP(=O)([O-])OP(=O)(O)OC[C@@H]3[C@H]([C@H]([C@@H](O3)N4C=NC5=C(N=CN=C54)N)O)O)O)O)C(=O)N', 'CCO', 'C[C@@H](C(=O)O)O']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xuvin/.conda/envs/catpred/lib/python3.9/site-packages/rotary_embedding_torch/rotary_embedding_torch.py:35: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  @autocast(enabled = False)\n",
      "/home/xuvin/.conda/envs/catpred/lib/python3.9/site-packages/rotary_embedding_torch/rotary_embedding_torch.py:268: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  @autocast(enabled = False)\n",
      "\n",
      "0it [00:00, ?it/s]/home/xuvin/py_repos/wccm/src/catpred_pipeline/catpred/catpred/data/cache_utils.py:95: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(str(entry_path))\n",
      "\n",
      "4it [00:00, 43.97it/s]\n",
      "\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 32140.26it/s]\n",
      "\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 1792.05it/s]\n",
      "\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]/home/xuvin/py_repos/wccm/src/catpred_pipeline/catpred/catpred/utils.py:113: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(path, map_location=lambda storage, loc: storage)\n",
      "/home/xuvin/py_repos/wccm/src/catpred_pipeline/catpred/catpred/utils.py:446: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(path, map_location=lambda storage, loc: storage)\n",
      "\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.20s/it]\u001b[A\n",
      "\n",
      "                                             \u001b[A\n",
      " 10%|█         | 1/10 [00:04<00:36,  4.11s/it]/home/xuvin/py_repos/wccm/src/catpred_pipeline/catpred/catpred/utils.py:113: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(path, map_location=lambda storage, loc: storage)\n",
      "/home/xuvin/py_repos/wccm/src/catpred_pipeline/catpred/catpred/utils.py:446: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(path, map_location=lambda storage, loc: storage)\n",
      "\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "                                     \u001b[A\n",
      " 20%|██        | 2/10 [00:04<00:14,  1.86s/it]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "                                     \u001b[A\n",
      " 30%|███       | 3/10 [00:04<00:07,  1.12s/it]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "                                     \u001b[A\n",
      " 40%|████      | 4/10 [00:05<00:05,  1.15it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "                                     \u001b[A\n",
      " 50%|█████     | 5/10 [00:05<00:03,  1.57it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "                                     \u001b[A\n",
      " 60%|██████    | 6/10 [00:05<00:02,  1.96it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "                                     \u001b[A\n",
      " 70%|███████   | 7/10 [00:05<00:01,  2.34it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "                                     \u001b[A\n",
      " 80%|████████  | 8/10 [00:06<00:00,  2.31it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "                                     \u001b[A\n",
      " 90%|█████████ | 9/10 [00:06<00:00,  2.68it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "                                     \u001b[A\n",
      "100%|██████████| 10/10 [00:06<00:00,  3.02it/s]\n",
      "100%|██████████| 10/10 [00:06<00:00,  1.47it/s]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "### STARTHERE: check units of k_cat, K_m\n",
    "\n",
    "# reactions\n",
    "bundle = {}\n",
    "\n",
    "# R00212_2.3.1.54: Formate + acetyl-CoA => Pyruvate + CoA\n",
    "substrates = ['Formate','Acetyl-CoA','Pyruvate','CoA']\n",
    "SMILES = [\n",
    "    'C(=O)[O-]',\n",
    "    'CC(=O)SCCNC(=O)CCNC(=O)[C@@H](C(C)(C)COP(=O)(O)OP(=O)(O)OC[C@@H]1[C@H]([C@H]([C@@H](O1)N2C=NC3=C(N=CN=C32)N)O)OP(=O)(O)O)O',\n",
    "    'CC(=O)C(=O)[O-]',\n",
    "    'CC(C)(COP(=O)(O)OP(=O)(O)OC[C@@H]1[C@H]([C@H]([C@@H](O1)N2C=NC3=C(N=CN=C32)N)O)OP(=O)(O)O)[C@H](C(=O)NCCC(=O)NCCS)O'\n",
    "]\n",
    "sequences = [\n",
    "    'MSELNEKLATAWEGFTKGDWQNEVNVRDFIQKNYTPYEGDESFLAGATEATTTLWDKVMEGVKLENRTHAPVDFDTAVASTITSHDAGYINKQLEKIVGLQTEAPLKRALIPFGGIKMIEGSCKAYNRELDPMIKKIFTEYRKTHNQGVFDVYTPDILRCRKSGVLTGLPDAYGRGRIIGDYRRVALYGIDYLMKDKLAQFTSLQADLENGVNLEQTIRLREEIAEQHRALGQMKEMAAKYGYDISGPATNAQEAIQWTYFGYLAAVKSQNGAAMSFGRTSTFLDVYIERDLKAGKITEQEAQEMVDHLVMKLRMVRFLRTPEYDELFSGDPIWATESIGGMGLDGRTLVTKNSFRFLNTLYTMGPSPEPNMTILWSEKLPLNFKKFAAKVSIDTSSLQYENDDLMRPDFNNDDYAIACCVSPMIVGKQMQFFGARANLAKTMLYAINGGVDEKLKMQVGPKSEPIKGDVLNYDEVMERMDHFMDWLAKQYITALNIIHYMHDKYSYEASLMALHDRDVIRTMACGIAGLSVAADSLSAIKYAKVKPIRDEDGLAIDFEIEGEYPQFGNNDPRVDDLAVDLVERFMKKIQKLHTYRDAIPTQSVLTITSNVVYGKKTGNTPDGRRAGAPFGPGANPMHGRDQKGAVASLTSVAKLPFAYAKDGISYTFSIVPNALGKDDEVRKTNLAGLMDGYFHHEASIEGGQHLNVNVMNREMLLDAMENPEKYPQLTIRVSGYAVRFNSLTKEQQQDVITRTFTQSM',\n",
    "    'MSELNEKLATAWEGFTKGDWQNEVNVRDFIQKNYTPYEGDESFLAGATEATTTLWDKVMEGVKLENRTHAPVDFDTAVASTITSHDAGYINKQLEKIVGLQTEAPLKRALIPFGGIKMIEGSCKAYNRELDPMIKKIFTEYRKTHNQGVFDVYTPDILRCRKSGVLTGLPDAYGRGRIIGDYRRVALYGIDYLMKDKLAQFTSLQADLENGVNLEQTIRLREEIAEQHRALGQMKEMAAKYGYDISGPATNAQEAIQWTYFGYLAAVKSQNGAAMSFGRTSTFLDVYIERDLKAGKITEQEAQEMVDHLVMKLRMVRFLRTPEYDELFSGDPIWATESIGGMGLDGRTLVTKNSFRFLNTLYTMGPSPEPNMTILWSEKLPLNFKKFAAKVSIDTSSLQYENDDLMRPDFNNDDYAIACCVSPMIVGKQMQFFGARANLAKTMLYAINGGVDEKLKMQVGPKSEPIKGDVLNYDEVMERMDHFMDWLAKQYITALNIIHYMHDKYSYEASLMALHDRDVIRTMACGIAGLSVAADSLSAIKYAKVKPIRDEDGLAIDFEIEGEYPQFGNNDPRVDDLAVDLVERFMKKIQKLHTYRDAIPTQSVLTITSNVVYGKKTGNTPDGRRAGAPFGPGANPMHGRDQKGAVASLTSVAKLPFAYAKDGISYTFSIVPNALGKDDEVRKTNLAGLMDGYFHHEASIEGGQHLNVNVMNREMLLDAMENPEKYPQLTIRVSGYAVRFNSLTKEQQQDVITRTFTQSM',\n",
    "    'MSELNEKLATAWEGFTKGDWQNEVNVRDFIQKNYTPYEGDESFLAGATEATTTLWDKVMEGVKLENRTHAPVDFDTAVASTITSHDAGYINKQLEKIVGLQTEAPLKRALIPFGGIKMIEGSCKAYNRELDPMIKKIFTEYRKTHNQGVFDVYTPDILRCRKSGVLTGLPDAYGRGRIIGDYRRVALYGIDYLMKDKLAQFTSLQADLENGVNLEQTIRLREEIAEQHRALGQMKEMAAKYGYDISGPATNAQEAIQWTYFGYLAAVKSQNGAAMSFGRTSTFLDVYIERDLKAGKITEQEAQEMVDHLVMKLRMVRFLRTPEYDELFSGDPIWATESIGGMGLDGRTLVTKNSFRFLNTLYTMGPSPEPNMTILWSEKLPLNFKKFAAKVSIDTSSLQYENDDLMRPDFNNDDYAIACCVSPMIVGKQMQFFGARANLAKTMLYAINGGVDEKLKMQVGPKSEPIKGDVLNYDEVMERMDHFMDWLAKQYITALNIIHYMHDKYSYEASLMALHDRDVIRTMACGIAGLSVAADSLSAIKYAKVKPIRDEDGLAIDFEIEGEYPQFGNNDPRVDDLAVDLVERFMKKIQKLHTYRDAIPTQSVLTITSNVVYGKKTGNTPDGRRAGAPFGPGANPMHGRDQKGAVASLTSVAKLPFAYAKDGISYTFSIVPNALGKDDEVRKTNLAGLMDGYFHHEASIEGGQHLNVNVMNREMLLDAMENPEKYPQLTIRVSGYAVRFNSLTKEQQQDVITRTFTQSM',\n",
    "    'MSELNEKLATAWEGFTKGDWQNEVNVRDFIQKNYTPYEGDESFLAGATEATTTLWDKVMEGVKLENRTHAPVDFDTAVASTITSHDAGYINKQLEKIVGLQTEAPLKRALIPFGGIKMIEGSCKAYNRELDPMIKKIFTEYRKTHNQGVFDVYTPDILRCRKSGVLTGLPDAYGRGRIIGDYRRVALYGIDYLMKDKLAQFTSLQADLENGVNLEQTIRLREEIAEQHRALGQMKEMAAKYGYDISGPATNAQEAIQWTYFGYLAAVKSQNGAAMSFGRTSTFLDVYIERDLKAGKITEQEAQEMVDHLVMKLRMVRFLRTPEYDELFSGDPIWATESIGGMGLDGRTLVTKNSFRFLNTLYTMGPSPEPNMTILWSEKLPLNFKKFAAKVSIDTSSLQYENDDLMRPDFNNDDYAIACCVSPMIVGKQMQFFGARANLAKTMLYAINGGVDEKLKMQVGPKSEPIKGDVLNYDEVMERMDHFMDWLAKQYITALNIIHYMHDKYSYEASLMALHDRDVIRTMACGIAGLSVAADSLSAIKYAKVKPIRDEDGLAIDFEIEGEYPQFGNNDPRVDDLAVDLVERFMKKIQKLHTYRDAIPTQSVLTITSNVVYGKKTGNTPDGRRAGAPFGPGANPMHGRDQKGAVASLTSVAKLPFAYAKDGISYTFSIVPNALGKDDEVRKTNLAGLMDGYFHHEASIEGGQHLNVNVMNREMLLDAMENPEKYPQLTIRVSGYAVRFNSLTKEQQQDVITRTFTQSM'\n",
    "    ]\n",
    "sequences = list(map(lambda x: x.upper(),sequences))\n",
    "pdb_paths = ['seq1.seq','seq1.seq','seq1.seq','seq1.seq']\n",
    "\n",
    "bundle['R00212'] = {\n",
    "    'substrates':substrates,\n",
    "    'SMILES':SMILES,\n",
    "    'sequences':sequences,\n",
    "    'pdb_paths':pdb_paths,\n",
    "    'subs':SMILES[0:2],\n",
    "    'stoich':[1,1,1,1],\n",
    "    'EC':'2.3.1.54'\n",
    "}\n",
    "\n",
    "# R00711_1.2.1.3: acetate + NADH => acetaldehyde + NAD+\n",
    "substrates = ['Acetate','NADH','Acetaldehyde','NAD+']\n",
    "SMILES = [\n",
    "    'CC(=O)[O-]',\n",
    "    'C1C=CN(C=C1C(=O)N)[C@H]2[C@@H]([C@@H]([C@H](O2)COP(=O)(O)OP(=O)(O)OC[C@@H]3[C@H]([C@H]([C@@H](O3)N4C=NC5=C(N=CN=C54)N)O)O)O)O',\n",
    "    'CC=O',\n",
    "    'C1=CC(=C[N+](=C1)[C@H]2[C@@H]([C@@H]([C@H](O2)COP(=O)([O-])OP(=O)(O)OC[C@@H]3[C@H]([C@H]([C@@H](O3)N4C=NC5=C(N=CN=C54)N)O)O)O)O)C(=O)N'\n",
    "    ]\n",
    "sequences = [\n",
    "    'maenhdyereinrlfelqkknvvrlrtssideriaklkklkeyiwenkekiqeavyndlrkppeevllteiypvvseirhviknlkkwtkpkkvrtpislfgaksyyrfeakgvvliispwnypfelsigplitaiaagnavvlkpselsphtsgyikklvadifdesevavvegdavvaqkllemgfnhifftgstkvakavlkkasetlssvtlelggkspviidgkfdieeaakkitwgkylnagqtciapdyvfvkkellgdfvshlkhyikkyyysdgsgrcsnycgiinerhfnrlknvfevtvkegakvcegglfvenecyisptvltdvgrdsyimeeeifgpilpvltyekiddvieyinskpaplvlyvfsrdrkfyrhvinnvisgdclindviahfanprlpfgghnasgigkshgyygfrefshlrsimiqpkrtmlqllyppygefvkkliewstkyf',\n",
    "    'maenhdyereinrlfelqkknvvrlrtssideriaklkklkeyiwenkekiqeavyndlrkppeevllteiypvvseirhviknlkkwtkpkkvrtpislfgaksyyrfeakgvvliispwnypfelsigplitaiaagnavvlkpselsphtsgyikklvadifdesevavvegdavvaqkllemgfnhifftgstkvakavlkkasetlssvtlelggkspviidgkfdieeaakkitwgkylnagqtciapdyvfvkkellgdfvshlkhyikkyyysdgsgrcsnycgiinerhfnrlknvfevtvkegakvcegglfvenecyisptvltdvgrdsyimeeeifgpilpvltyekiddvieyinskpaplvlyvfsrdrkfyrhvinnvisgdclindviahfanprlpfgghnasgigkshgyygfrefshlrsimiqpkrtmlqllyppygefvkkliewstkyf',\n",
    "    'maenhdyereinrlfelqkknvvrlrtssideriaklkklkeyiwenkekiqeavyndlrkppeevllteiypvvseirhviknlkkwtkpkkvrtpislfgaksyyrfeakgvvliispwnypfelsigplitaiaagnavvlkpselsphtsgyikklvadifdesevavvegdavvaqkllemgfnhifftgstkvakavlkkasetlssvtlelggkspviidgkfdieeaakkitwgkylnagqtciapdyvfvkkellgdfvshlkhyikkyyysdgsgrcsnycgiinerhfnrlknvfevtvkegakvcegglfvenecyisptvltdvgrdsyimeeeifgpilpvltyekiddvieyinskpaplvlyvfsrdrkfyrhvinnvisgdclindviahfanprlpfgghnasgigkshgyygfrefshlrsimiqpkrtmlqllyppygefvkkliewstkyf',\n",
    "    'maenhdyereinrlfelqkknvvrlrtssideriaklkklkeyiwenkekiqeavyndlrkppeevllteiypvvseirhviknlkkwtkpkkvrtpislfgaksyyrfeakgvvliispwnypfelsigplitaiaagnavvlkpselsphtsgyikklvadifdesevavvegdavvaqkllemgfnhifftgstkvakavlkkasetlssvtlelggkspviidgkfdieeaakkitwgkylnagqtciapdyvfvkkellgdfvshlkhyikkyyysdgsgrcsnycgiinerhfnrlknvfevtvkegakvcegglfvenecyisptvltdvgrdsyimeeeifgpilpvltyekiddvieyinskpaplvlyvfsrdrkfyrhvinnvisgdclindviahfanprlpfgghnasgigkshgyygfrefshlrsimiqpkrtmlqllyppygefvkkliewstkyf'\n",
    "    ]\n",
    "sequences = list(map(lambda x: x.upper(),sequences))\n",
    "pdb_paths = ['seq2.seq','seq2.seq','seq2.seq','seq2.seq']\n",
    "\n",
    "bundle['R00711'] = {\n",
    "    'substrates':substrates,\n",
    "    'SMILES':SMILES,\n",
    "    'sequences':sequences,\n",
    "    'pdb_paths':pdb_paths,\n",
    "    'subs':SMILES[0:2],\n",
    "    'stoich':[1,1,1,1],\n",
    "    'EC':'1.2.1.3'\n",
    "}\n",
    "\n",
    "# R00746_1.1.1.71: ethanol + NAD+ => acetaldehyde + NADH\n",
    "substrates = ['Ethanol','NAD+','Acetaldehyde','NADH']\n",
    "SMILES = [\n",
    "    'CCO',\n",
    "    'C1=CC(=C[N+](=C1)[C@H]2[C@@H]([C@@H]([C@H](O2)COP(=O)([O-])OP(=O)(O)OC[C@@H]3[C@H]([C@H]([C@@H](O3)N4C=NC5=C(N=CN=C54)N)O)O)O)O)C(=O)N',\n",
    "    'CC=O',\n",
    "    'C1C=CN(C=C1C(=O)N)[C@H]2[C@@H]([C@@H]([C@H](O2)COP(=O)(O)OP(=O)(O)OC[C@@H]3[C@H]([C@H]([C@@H](O3)N4C=NC5=C(N=CN=C54)N)O)O)O)O'\n",
    "]\n",
    "sequences = [\n",
    "    'MTKIANKYEVIDNVEKLEKALKRLREAQSVYATYTQEQVDKIFFEAAMAANKMRIPLAKMAVEETGMGVVEDKVIKNHYASEYIYNAYKNTKTCGVIEEDPAFGIKKIAEPLGVIAAVIPTTNPTSTAIFKTLIALKTRNAIIISPHPRAKNSTIEAAKIVLEAAVKAGAPEGIIGWIDVPSLELTNLVMREADVILATGGPGLVKAAYSSGKPAIGVGAGNTPAIIDDSADIVLAVNSIIHSKTFDNGMICASEQSVIVLDGVYKEVKKEFEKRGCYFLNEDETEKVRKTIIINGALNAKIVGQKAHTIANLAGFEVPETTKILIGEVTSVDISEEFAHEKLCPVLAMYRAKDFDDALDKAERLVADGGFGHTSSLYIDTVTQKEKLQKFSERMKTCRILVNTPSSQGGIGDLYNFKLAPSLTLGCGSWGGNSVSDNVGVKHLLNIKTVAERRENMLWFRTPEKIYIKRGCLPVALDELKNVMGKKKAFIVTDNFLYNNGYTKPITDKLDEMGIVHKTFFDVSPDPSLASAKAGAAEMLAFQPDTIIAVGGGSAMDAAKIMWVMYEHPEVDFMDMAMRFMDIRKRVYTFPKMGQKAYFIAIPTSAGTGSEVTPFAVITDEKTGIKYPLADYELLPDMAIVDADMMMNAPKGLTAASGIDALTHALEAYVSMLATDYTDSLALRAIKMIFEYLPRAYENGASDPVAREKMANAATIAGMAFANAFLGVCHSMAHKLGAFYHLPHGVANALMINEVIRFNSSEAPTKMGTFPQYDHPRTLERYAEIADYIGLKGKNNEEKVENLIKAIDELKEKVGIRKTIKDYDIDEKEFLDRLDEMVEQAFDDQCTGTNPRYPLMNEIRQMYLNAYYGGAKK',\n",
    "    'MTKIANKYEVIDNVEKLEKALKRLREAQSVYATYTQEQVDKIFFEAAMAANKMRIPLAKMAVEETGMGVVEDKVIKNHYASEYIYNAYKNTKTCGVIEEDPAFGIKKIAEPLGVIAAVIPTTNPTSTAIFKTLIALKTRNAIIISPHPRAKNSTIEAAKIVLEAAVKAGAPEGIIGWIDVPSLELTNLVMREADVILATGGPGLVKAAYSSGKPAIGVGAGNTPAIIDDSADIVLAVNSIIHSKTFDNGMICASEQSVIVLDGVYKEVKKEFEKRGCYFLNEDETEKVRKTIIINGALNAKIVGQKAHTIANLAGFEVPETTKILIGEVTSVDISEEFAHEKLCPVLAMYRAKDFDDALDKAERLVADGGFGHTSSLYIDTVTQKEKLQKFSERMKTCRILVNTPSSQGGIGDLYNFKLAPSLTLGCGSWGGNSVSDNVGVKHLLNIKTVAERRENMLWFRTPEKIYIKRGCLPVALDELKNVMGKKKAFIVTDNFLYNNGYTKPITDKLDEMGIVHKTFFDVSPDPSLASAKAGAAEMLAFQPDTIIAVGGGSAMDAAKIMWVMYEHPEVDFMDMAMRFMDIRKRVYTFPKMGQKAYFIAIPTSAGTGSEVTPFAVITDEKTGIKYPLADYELLPDMAIVDADMMMNAPKGLTAASGIDALTHALEAYVSMLATDYTDSLALRAIKMIFEYLPRAYENGASDPVAREKMANAATIAGMAFANAFLGVCHSMAHKLGAFYHLPHGVANALMINEVIRFNSSEAPTKMGTFPQYDHPRTLERYAEIADYIGLKGKNNEEKVENLIKAIDELKEKVGIRKTIKDYDIDEKEFLDRLDEMVEQAFDDQCTGTNPRYPLMNEIRQMYLNAYYGGAKK',\n",
    "    'MTKIANKYEVIDNVEKLEKALKRLREAQSVYATYTQEQVDKIFFEAAMAANKMRIPLAKMAVEETGMGVVEDKVIKNHYASEYIYNAYKNTKTCGVIEEDPAFGIKKIAEPLGVIAAVIPTTNPTSTAIFKTLIALKTRNAIIISPHPRAKNSTIEAAKIVLEAAVKAGAPEGIIGWIDVPSLELTNLVMREADVILATGGPGLVKAAYSSGKPAIGVGAGNTPAIIDDSADIVLAVNSIIHSKTFDNGMICASEQSVIVLDGVYKEVKKEFEKRGCYFLNEDETEKVRKTIIINGALNAKIVGQKAHTIANLAGFEVPETTKILIGEVTSVDISEEFAHEKLCPVLAMYRAKDFDDALDKAERLVADGGFGHTSSLYIDTVTQKEKLQKFSERMKTCRILVNTPSSQGGIGDLYNFKLAPSLTLGCGSWGGNSVSDNVGVKHLLNIKTVAERRENMLWFRTPEKIYIKRGCLPVALDELKNVMGKKKAFIVTDNFLYNNGYTKPITDKLDEMGIVHKTFFDVSPDPSLASAKAGAAEMLAFQPDTIIAVGGGSAMDAAKIMWVMYEHPEVDFMDMAMRFMDIRKRVYTFPKMGQKAYFIAIPTSAGTGSEVTPFAVITDEKTGIKYPLADYELLPDMAIVDADMMMNAPKGLTAASGIDALTHALEAYVSMLATDYTDSLALRAIKMIFEYLPRAYENGASDPVAREKMANAATIAGMAFANAFLGVCHSMAHKLGAFYHLPHGVANALMINEVIRFNSSEAPTKMGTFPQYDHPRTLERYAEIADYIGLKGKNNEEKVENLIKAIDELKEKVGIRKTIKDYDIDEKEFLDRLDEMVEQAFDDQCTGTNPRYPLMNEIRQMYLNAYYGGAKK',\n",
    "    'MTKIANKYEVIDNVEKLEKALKRLREAQSVYATYTQEQVDKIFFEAAMAANKMRIPLAKMAVEETGMGVVEDKVIKNHYASEYIYNAYKNTKTCGVIEEDPAFGIKKIAEPLGVIAAVIPTTNPTSTAIFKTLIALKTRNAIIISPHPRAKNSTIEAAKIVLEAAVKAGAPEGIIGWIDVPSLELTNLVMREADVILATGGPGLVKAAYSSGKPAIGVGAGNTPAIIDDSADIVLAVNSIIHSKTFDNGMICASEQSVIVLDGVYKEVKKEFEKRGCYFLNEDETEKVRKTIIINGALNAKIVGQKAHTIANLAGFEVPETTKILIGEVTSVDISEEFAHEKLCPVLAMYRAKDFDDALDKAERLVADGGFGHTSSLYIDTVTQKEKLQKFSERMKTCRILVNTPSSQGGIGDLYNFKLAPSLTLGCGSWGGNSVSDNVGVKHLLNIKTVAERRENMLWFRTPEKIYIKRGCLPVALDELKNVMGKKKAFIVTDNFLYNNGYTKPITDKLDEMGIVHKTFFDVSPDPSLASAKAGAAEMLAFQPDTIIAVGGGSAMDAAKIMWVMYEHPEVDFMDMAMRFMDIRKRVYTFPKMGQKAYFIAIPTSAGTGSEVTPFAVITDEKTGIKYPLADYELLPDMAIVDADMMMNAPKGLTAASGIDALTHALEAYVSMLATDYTDSLALRAIKMIFEYLPRAYENGASDPVAREKMANAATIAGMAFANAFLGVCHSMAHKLGAFYHLPHGVANALMINEVIRFNSSEAPTKMGTFPQYDHPRTLERYAEIADYIGLKGKNNEEKVENLIKAIDELKEKVGIRKTIKDYDIDEKEFLDRLDEMVEQAFDDQCTGTNPRYPLMNEIRQMYLNAYYGGAKK'\n",
    "]\n",
    "sequences = list(map(lambda x: x.upper(),sequences))\n",
    "pdb_paths = ['seq3.seq','seq3.seq','seq3.seq','seq3.seq']\n",
    "\n",
    "bundle['R00746'] = {\n",
    "    'substrates':substrates,\n",
    "    'SMILES':SMILES,\n",
    "    'sequences':sequences,\n",
    "    'pdb_paths':pdb_paths,\n",
    "    'subs':SMILES[0:2],\n",
    "    'stoich':[1,1,1,1],\n",
    "    'EC':'1.1.1.71'\n",
    "}\n",
    "\n",
    "# R00228_1.2.1.10: acetaldehyde + CoA + NAD+ => acetyl-CoA + NADH + H+\n",
    "substrates = ['Acetaldehyde','CoA','NAD+','Acetyl-CoA','NADH']\n",
    "SMILES = [\n",
    "    'CC=O',\n",
    "    'CC(C)(COP(=O)(O)OP(=O)(O)OC[C@@H]1[C@H]([C@H]([C@@H](O1)N2C=NC3=C(N=CN=C32)N)O)OP(=O)(O)O)[C@H](C(=O)NCCC(=O)NCCS)O',\n",
    "    'C1=CC(=C[N+](=C1)[C@H]2[C@@H]([C@@H]([C@H](O2)COP(=O)([O-])OP(=O)(O)OC[C@@H]3[C@H]([C@H]([C@@H](O3)N4C=NC5=C(N=CN=C54)N)O)O)O)O)C(=O)N',\n",
    "    'CC(=O)SCCNC(=O)CCNC(=O)[C@@H](C(C)(C)COP(=O)(O)OP(=O)(O)OC[C@@H]1[C@H]([C@H]([C@@H](O1)N2C=NC3=C(N=CN=C32)N)O)OP(=O)(O)O)O',\n",
    "    'C1C=CN(C=C1C(=O)N)[C@H]2[C@@H]([C@@H]([C@H](O2)COP(=O)(O)OP(=O)(O)OC[C@@H]3[C@H]([C@H]([C@@H](O3)N4C=NC5=C(N=CN=C54)N)O)O)O)O',\n",
    "]\n",
    "sequences = [\n",
    "    'MSKRKVAIIGSGNIGTDLMIKILRHGQHLEMAVMVGIDPQSDGLARARRMGVATTHEGVIGLMNMPEFADIDIVFDATSAGAHVKNDAALREAKPDIRLIDLTPAAIGPYCVPVVNLEANVDQLNVNMVTCGGQATIPMVAAVSRVARVHYAEIIASIASKSAGPGTRANIDEFTETTSRAIEVVGGAAKGKAIIVLNPAEPPLMMRDTVYVLSDEASQDDIEASINEMAEAVQAYVPGYRLKQRVQFEVIPQDKPVNLPGVGQFSGLKTAVWLEVEGAAHYLPAYAGNLDIMTSSALATAEKMAQSLARKAGEAA',\n",
    "    'MSKRKVAIIGSGNIGTDLMIKILRHGQHLEMAVMVGIDPQSDGLARARRMGVATTHEGVIGLMNMPEFADIDIVFDATSAGAHVKNDAALREAKPDIRLIDLTPAAIGPYCVPVVNLEANVDQLNVNMVTCGGQATIPMVAAVSRVARVHYAEIIASIASKSAGPGTRANIDEFTETTSRAIEVVGGAAKGKAIIVLNPAEPPLMMRDTVYVLSDEASQDDIEASINEMAEAVQAYVPGYRLKQRVQFEVIPQDKPVNLPGVGQFSGLKTAVWLEVEGAAHYLPAYAGNLDIMTSSALATAEKMAQSLARKAGEAA',\n",
    "    'MSKRKVAIIGSGNIGTDLMIKILRHGQHLEMAVMVGIDPQSDGLARARRMGVATTHEGVIGLMNMPEFADIDIVFDATSAGAHVKNDAALREAKPDIRLIDLTPAAIGPYCVPVVNLEANVDQLNVNMVTCGGQATIPMVAAVSRVARVHYAEIIASIASKSAGPGTRANIDEFTETTSRAIEVVGGAAKGKAIIVLNPAEPPLMMRDTVYVLSDEASQDDIEASINEMAEAVQAYVPGYRLKQRVQFEVIPQDKPVNLPGVGQFSGLKTAVWLEVEGAAHYLPAYAGNLDIMTSSALATAEKMAQSLARKAGEAA',\n",
    "    'MSKRKVAIIGSGNIGTDLMIKILRHGQHLEMAVMVGIDPQSDGLARARRMGVATTHEGVIGLMNMPEFADIDIVFDATSAGAHVKNDAALREAKPDIRLIDLTPAAIGPYCVPVVNLEANVDQLNVNMVTCGGQATIPMVAAVSRVARVHYAEIIASIASKSAGPGTRANIDEFTETTSRAIEVVGGAAKGKAIIVLNPAEPPLMMRDTVYVLSDEASQDDIEASINEMAEAVQAYVPGYRLKQRVQFEVIPQDKPVNLPGVGQFSGLKTAVWLEVEGAAHYLPAYAGNLDIMTSSALATAEKMAQSLARKAGEAA',\n",
    "    'MSKRKVAIIGSGNIGTDLMIKILRHGQHLEMAVMVGIDPQSDGLARARRMGVATTHEGVIGLMNMPEFADIDIVFDATSAGAHVKNDAALREAKPDIRLIDLTPAAIGPYCVPVVNLEANVDQLNVNMVTCGGQATIPMVAAVSRVARVHYAEIIASIASKSAGPGTRANIDEFTETTSRAIEVVGGAAKGKAIIVLNPAEPPLMMRDTVYVLSDEASQDDIEASINEMAEAVQAYVPGYRLKQRVQFEVIPQDKPVNLPGVGQFSGLKTAVWLEVEGAAHYLPAYAGNLDIMTSSALATAEKMAQSLARKAGEAA'    \n",
    "]\n",
    "sequences = list(map(lambda x: x.upper(),sequences))\n",
    "pdb_paths = ['seq4.seq','seq4.seq','seq4.seq','seq4.seq','seq4.seq']\n",
    "\n",
    "bundle['R00228'] = {\n",
    "    'substrates':substrates,\n",
    "    'SMILES':SMILES,\n",
    "    'sequences':sequences,\n",
    "    'pdb_paths':pdb_paths,\n",
    "    'subs':SMILES[0:3],\n",
    "    'stoich':[1,1,1,1,1],\n",
    "    'EC':'1.2.1.10'\n",
    "}\n",
    "\n",
    "# R00703_1.1.1.27: pyruvate + NADH + H+ => (S)-lactate + NAD+ \n",
    "substrates = ['Pyruvate','NADH','(S)-lactate','NAD+']\n",
    "SMILES = [\n",
    "    'CC(=O)C(=O)[O-]',\n",
    "    'C1C=CN(C=C1C(=O)N)[C@H]2[C@@H]([C@@H]([C@H](O2)COP(=O)(O)OP(=O)(O)OC[C@@H]3[C@H]([C@H]([C@@H](O3)N4C=NC5=C(N=CN=C54)N)O)O)O)O',\n",
    "    'C[C@@H](C(=O)O)O',\n",
    "    'C1=CC(=C[N+](=C1)[C@H]2[C@@H]([C@@H]([C@H](O2)COP(=O)([O-])OP(=O)(O)OC[C@@H]3[C@H]([C@H]([C@@H](O3)N4C=NC5=C(N=CN=C54)N)O)O)O)O)C(=O)N'\n",
    "]\n",
    "sequences = [\n",
    "    'MADKQRKKVILVGDGAVGSSYAFALVNQGIAQELGIVDLFKEKTQGDAEDLSHALAFTSPKKIYSADYSDASDADLVVLTSGAPQKPGETRLDLVEKNLRITKDVVTKIVASGFKGIFLVAANPVDILTYATWKFSGFPKNRVVGSGTSLDTARFRQALAEKVDVDARSIHAYIMGEHGDSEFAVWSHANVAGVKLEQWFQENDYLNEAEIVKLFESVRDAAYSIIAKKGATFYGVAVALARITKAILDDEHAVLPVSVFQDGQYGVSDCYLGQPAVVGAEGVVNPIHIPLNDAEMQKMEASGAQLKAIIDEAFAKEEFASAVKN',\n",
    "    'MADKQRKKVILVGDGAVGSSYAFALVNQGIAQELGIVDLFKEKTQGDAEDLSHALAFTSPKKIYSADYSDASDADLVVLTSGAPQKPGETRLDLVEKNLRITKDVVTKIVASGFKGIFLVAANPVDILTYATWKFSGFPKNRVVGSGTSLDTARFRQALAEKVDVDARSIHAYIMGEHGDSEFAVWSHANVAGVKLEQWFQENDYLNEAEIVKLFESVRDAAYSIIAKKGATFYGVAVALARITKAILDDEHAVLPVSVFQDGQYGVSDCYLGQPAVVGAEGVVNPIHIPLNDAEMQKMEASGAQLKAIIDEAFAKEEFASAVKN',\n",
    "    'MADKQRKKVILVGDGAVGSSYAFALVNQGIAQELGIVDLFKEKTQGDAEDLSHALAFTSPKKIYSADYSDASDADLVVLTSGAPQKPGETRLDLVEKNLRITKDVVTKIVASGFKGIFLVAANPVDILTYATWKFSGFPKNRVVGSGTSLDTARFRQALAEKVDVDARSIHAYIMGEHGDSEFAVWSHANVAGVKLEQWFQENDYLNEAEIVKLFESVRDAAYSIIAKKGATFYGVAVALARITKAILDDEHAVLPVSVFQDGQYGVSDCYLGQPAVVGAEGVVNPIHIPLNDAEMQKMEASGAQLKAIIDEAFAKEEFASAVKN',\n",
    "    'MADKQRKKVILVGDGAVGSSYAFALVNQGIAQELGIVDLFKEKTQGDAEDLSHALAFTSPKKIYSADYSDASDADLVVLTSGAPQKPGETRLDLVEKNLRITKDVVTKIVASGFKGIFLVAANPVDILTYATWKFSGFPKNRVVGSGTSLDTARFRQALAEKVDVDARSIHAYIMGEHGDSEFAVWSHANVAGVKLEQWFQENDYLNEAEIVKLFESVRDAAYSIIAKKGATFYGVAVALARITKAILDDEHAVLPVSVFQDGQYGVSDCYLGQPAVVGAEGVVNPIHIPLNDAEMQKMEASGAQLKAIIDEAFAKEEFASAVKN'\n",
    "]\n",
    "sequences = list(map(lambda x: x.upper(),sequences))\n",
    "pdb_paths = ['seq5.seq','seq5.seq','seq5.seq','seq5.seq']\n",
    "\n",
    "bundle['R00703'] = {\n",
    "    'substrates':substrates,\n",
    "    'SMILES':SMILES,\n",
    "    'sequences':sequences,\n",
    "    'pdb_paths':pdb_paths,\n",
    "    'subs':SMILES[0:2],\n",
    "    'stoich':[1,1,1,1],\n",
    "    'EC':'1.1.1.27'\n",
    "}\n",
    "\n",
    "kin_param_bundle = {}\n",
    "enz_ECs = []\n",
    "meta_smiles = []\n",
    "# for key in [x for x in list(bundle.keys()) if x == 'R00212']:\n",
    "for key in list(bundle.keys()):\n",
    "    # caluclating kinetic parameters\n",
    "    item = bundle[key]\n",
    "    print('item',item)\n",
    "    input = pd.DataFrame({'substrate':item['substrates'],'SMILES':item['SMILES'],'sequence':item['sequences'],'pdbpath':item['pdb_paths']})\n",
    "\n",
    "    print(f'starting {key}')\n",
    "    print('input',input.to_markdown())\n",
    "    print('input2',item['SMILES'])\n",
    "\n",
    "    input.to_csv('./src/catpred_pipeline/pipeline_entry_kcat.csv')\n",
    "    input.to_csv('./src/catpred_pipeline/pipeline_entry_km.csv')\n",
    "\n",
    "    files_to_remove = [\n",
    "        './src/catpred_pipeline/results/pipeline_entry_kcat_input_output.csv',\n",
    "        './src/catpred_pipeline/pipeline_entry_kcat_input_output.csv',\n",
    "        './src/catpred_pipeline/pipeline_entry_kcat_input.csv',\n",
    "        './src/catpred_pipeline/pipeline_entry_kcat_input.json',\n",
    "        './src/catpred_pipeline/results/pipeline_entry_km_input_output.csv',\n",
    "        './src/catpred_pipeline/pipeline_entry_km_input_output.csv',\n",
    "        './src/catpred_pipeline/pipeline_entry_km_input.csv',\n",
    "        './src/catpred_pipeline/pipeline_entry_km_input.json',\n",
    "    ]\n",
    "\n",
    "    for file in files_to_remove:\n",
    "        if os.path.exists(file):\n",
    "            os.remove(file)\n",
    "        else:\n",
    "            print(f'{file} does not exist')\n",
    "\n",
    "    cmd = 'cd src/catpred_pipeline/catpred; ' \\\n",
    "    'export PROTEIN_EMBED_USE_CPU=1; ' \\\n",
    "    'conda run -n catpred python3 ./demo_run.py --parameter kcat --input_file ../pipeline_entry_kcat.csv --checkpoint_dir ../data/pretrained/production/kcat/ --use_gpu; ' \\\n",
    "    'conda run -n catpred python3 ./demo_run.py --parameter km --input_file ../pipeline_entry_km.csv --checkpoint_dir ../data/pretrained/production/km/ --use_gpu'\n",
    "    subprocess.call(cmd,shell=True,executable='/bin/bash')\n",
    "\n",
    "    # creating kinetic parameter bundles\n",
    "    kcat_output_df = pd.read_csv('./src/catpred_pipeline/results/pipeline_entry_kcat_input_output.csv')\n",
    "    km_output_df = pd.read_csv('./src/catpred_pipeline/results/pipeline_entry_km_input_output.csv')\n",
    "\n",
    "    print('output',kcat_output_df.to_markdown())\n",
    "\n",
    "    kin_param_bundle[key] = {}\n",
    "    kin_param_bundle[key]['subs'] = {}\n",
    "    kin_param_bundle[key]['prods'] = {}\n",
    "\n",
    "    print('SMILES',item['SMILES'])\n",
    "\n",
    "    if item['EC'] in enz_ECs:\n",
    "        enz_num = enz_ECs.index(item['EC'])\n",
    "    else:\n",
    "        enz_num = len(enz_ECs)\n",
    "    kin_param_bundle[key]['enz'] = {\n",
    "        'enz_EC':item['EC'],\n",
    "        'enz_num':enz_num\n",
    "    }\n",
    "\n",
    "    for idx,smi in enumerate(item['SMILES']):\n",
    "        if smi in meta_smiles:\n",
    "            meta_num = meta_smiles.index(smi)\n",
    "        else:\n",
    "            meta_num = len(meta_smiles)\n",
    "            meta_smiles.append(smi)\n",
    "\n",
    "        if smi in item['subs']:\n",
    "            group = 'subs'\n",
    "        else:\n",
    "            group = 'prods'\n",
    "\n",
    "        print('smiles_list',smi,meta_smiles)   \n",
    "\n",
    "        kin_param_bundle[key][group][smi] = {\n",
    "            'meta_num':meta_num,\n",
    "            'meta_name':item['substrates'][idx],\n",
    "            'k_cat':10**float(kcat_output_df.iloc[idx]['log10kcat_max']),\n",
    "            'K_m':10**float(km_output_df.iloc[idx]['log10km_mean']),\n",
    "            'stoich':item['stoich'][idx],\n",
    "            'enz_num':enz_num\n",
    "        }\n",
    "\n",
    "    enz_ECs += [item['EC']]\n",
    "    enz_ECs = list(set(enz_ECs))\n",
    "\n",
    "kin_param_bundle['meta_ct'] = len(meta_smiles)\n",
    "kin_param_bundle['enz_ct'] = len(enz_ECs)\n",
    "\n",
    "print('kin_param_bundle',kin_param_bundle)\n",
    "print('enz_ECs',enz_ECs)\n",
    "print('meta_smies',meta_smiles)\n",
    "\n",
    "# kin_param_bundles\n",
    "### STARTHERE: set enzyme number for each sub\n",
    "# kin_param_bundles = {\n",
    "#     'rxn_name': {\n",
    "#         'subs':{\n",
    "#             'sub_name':{\n",
    "#                 'meta_num': 0,\n",
    "#                 'meta_name': 'hi',\n",
    "#                 'k_cat':1,\n",
    "#                 'K_m':1,\n",
    "#                 'stoich':1,\n",
    "#                 'enz_num':1,\n",
    "#                 'init_conc':1 # mM\n",
    "#             }\n",
    "#         },\n",
    "#         'prods':{\n",
    "#             'prod_name':{\n",
    "#                 'meta_num': 0,\n",
    "#                 'meta_name': 'hi',\n",
    "#                 'k_cat':1,\n",
    "#                 'K_m':1,\n",
    "#                 'stoich':1,\n",
    "#                 'enz_num':1,\n",
    "#                 'init_conc':1 # mM\n",
    "#             }\n",
    "#         },\n",
    "#         'enz':{\n",
    "#             'enz_EC':'test',\n",
    "#             'enz_num':1,\n",
    "#             'init_conc':1 # mM\n",
    "#         }\n",
    "#     },\n",
    "#     'meta_ct':1,\n",
    "#     'enz_ct':1,\n",
    "# }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting Up Kinetic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kin 1\n",
      "{'Formate': 0,\n",
      " 'Acetyl-CoA': 1,\n",
      " 'Pyruvate': 2,\n",
      " 'CoA': 3,\n",
      " '2.3.1.54': 10,\n",
      " 'Acetate': 4,\n",
      " 'NADH': 5,\n",
      " 'Acetaldehyde': 6,\n",
      " 'NAD+': 7,\n",
      " '1.2.1.3': 11,\n",
      " 'Ethanol': 8,\n",
      " '1.1.1.71': 12,\n",
      " '1.2.1.10': 13,\n",
      " '(S)-lactate': 9,\n",
      " '1.1.1.27': 14}\n",
      "kin\n",
      "{'R00212': {'subs': {'C(=O)[O-]': {'meta_num': 0,\n",
      "                                   'meta_name': 'Formate',\n",
      "                                   'k_cat': 10.055570987342165,\n",
      "                                   'K_m': 13.844884834853536,\n",
      "                                   'stoich': 1,\n",
      "                                   'enz_num': 0},\n",
      "                     'CC(=O)SCCNC(=O)CCNC(=O)[C@@H](C(C)(C)COP(=O)(O)OP(=O)(O)OC[C@@H]1[C@H]([C@H]([C@@H](O1)N2C=NC3=C(N=CN=C32)N)O)OP(=O)(O)O)O': {'meta_num': 1,\n",
      "                                                                                                                                                    'meta_name': 'Acetyl-CoA',\n",
      "                                                                                                                                                    'k_cat': 8.002823257765913,\n",
      "                                                                                                                                                    'K_m': 0.10558599605791594,\n",
      "                                                                                                                                                    'stoich': 1,\n",
      "                                                                                                                                                    'enz_num': 0}},\n",
      "            'prods': {'CC(=O)C(=O)[O-]': {'meta_num': 2,\n",
      "                                          'meta_name': 'Pyruvate',\n",
      "                                          'k_cat': 0.699861431034104,\n",
      "                                          'K_m': 3.7030182513562626,\n",
      "                                          'stoich': 1,\n",
      "                                          'enz_num': 0},\n",
      "                      'CC(C)(COP(=O)(O)OP(=O)(O)OC[C@@H]1[C@H]([C@H]([C@@H](O1)N2C=NC3=C(N=CN=C32)N)O)OP(=O)(O)O)[C@H](C(=O)NCCC(=O)NCCS)O': {'meta_num': 3,\n",
      "                                                                                                                                              'meta_name': 'CoA',\n",
      "                                                                                                                                              'k_cat': 8.146511911230082,\n",
      "                                                                                                                                              'K_m': 0.08583361139178317,\n",
      "                                                                                                                                              'stoich': 1,\n",
      "                                                                                                                                              'enz_num': 0}},\n",
      "            'enz': {'enz_EC': '2.3.1.54', 'enz_num': 0}},\n",
      " 'R00711': {'subs': {'CC(=O)[O-]': {'meta_num': 4,\n",
      "                                    'meta_name': 'Acetate',\n",
      "                                    'k_cat': 91.13363242250604,\n",
      "                                    'K_m': 1.4673414432132363,\n",
      "                                    'stoich': 1,\n",
      "                                    'enz_num': 1},\n",
      "                     'C1C=CN(C=C1C(=O)N)[C@H]2[C@@H]([C@@H]([C@H](O2)COP(=O)(O)OP(=O)(O)OC[C@@H]3[C@H]([C@H]([C@@H](O3)N4C=NC5=C(N=CN=C54)N)O)O)O)O': {'meta_num': 5,\n",
      "                                                                                                                                                       'meta_name': 'NADH',\n",
      "                                                                                                                                                       'k_cat': 22.386048857294554,\n",
      "                                                                                                                                                       'K_m': 0.08149449754277381,\n",
      "                                                                                                                                                       'stoich': 1,\n",
      "                                                                                                                                                       'enz_num': 1}},\n",
      "            'prods': {'CC=O': {'meta_num': 6,\n",
      "                               'meta_name': 'Acetaldehyde',\n",
      "                               'k_cat': 27.851824424527813,\n",
      "                               'K_m': 3.6298932782879043,\n",
      "                               'stoich': 1,\n",
      "                               'enz_num': 1},\n",
      "                      'C1=CC(=C[N+](=C1)[C@H]2[C@@H]([C@@H]([C@H](O2)COP(=O)([O-])OP(=O)(O)OC[C@@H]3[C@H]([C@H]([C@@H](O3)N4C=NC5=C(N=CN=C54)N)O)O)O)O)C(=O)N': {'meta_num': 7,\n",
      "                                                                                                                                                                 'meta_name': 'NAD+',\n",
      "                                                                                                                                                                 'k_cat': 20.34650868513414,\n",
      "                                                                                                                                                                 'K_m': 0.35080092962267617,\n",
      "                                                                                                                                                                 'stoich': 1,\n",
      "                                                                                                                                                                 'enz_num': 1}},\n",
      "            'enz': {'enz_EC': '1.2.1.3', 'enz_num': 1}},\n",
      " 'R00746': {'subs': {'CCO': {'meta_num': 8,\n",
      "                             'meta_name': 'Ethanol',\n",
      "                             'k_cat': 231.63150746705503,\n",
      "                             'K_m': 3.6951917513171404,\n",
      "                             'stoich': 1,\n",
      "                             'enz_num': 2},\n",
      "                     'C1=CC(=C[N+](=C1)[C@H]2[C@@H]([C@@H]([C@H](O2)COP(=O)([O-])OP(=O)(O)OC[C@@H]3[C@H]([C@H]([C@@H](O3)N4C=NC5=C(N=CN=C54)N)O)O)O)O)C(=O)N': {'meta_num': 7,\n",
      "                                                                                                                                                                'meta_name': 'NAD+',\n",
      "                                                                                                                                                                'k_cat': 215.98127199500323,\n",
      "                                                                                                                                                                'K_m': 0.28327345172869894,\n",
      "                                                                                                                                                                'stoich': 1,\n",
      "                                                                                                                                                                'enz_num': 2}},\n",
      "            'prods': {'CC=O': {'meta_num': 6,\n",
      "                               'meta_name': 'Acetaldehyde',\n",
      "                               'k_cat': 322.1749664211754,\n",
      "                               'K_m': 1.6453886389787897,\n",
      "                               'stoich': 1,\n",
      "                               'enz_num': 2},\n",
      "                      'C1C=CN(C=C1C(=O)N)[C@H]2[C@@H]([C@@H]([C@H](O2)COP(=O)(O)OP(=O)(O)OC[C@@H]3[C@H]([C@H]([C@@H](O3)N4C=NC5=C(N=CN=C54)N)O)O)O)O': {'meta_num': 5,\n",
      "                                                                                                                                                        'meta_name': 'NADH',\n",
      "                                                                                                                                                        'k_cat': 321.4767660701479,\n",
      "                                                                                                                                                        'K_m': 0.0882936817212929,\n",
      "                                                                                                                                                        'stoich': 1,\n",
      "                                                                                                                                                        'enz_num': 2}},\n",
      "            'enz': {'enz_EC': '1.1.1.71', 'enz_num': 2}},\n",
      " 'R00228': {'subs': {'CC=O': {'meta_num': 6,\n",
      "                              'meta_name': 'Acetaldehyde',\n",
      "                              'k_cat': 8.991911966615412,\n",
      "                              'K_m': 11.332360232182543,\n",
      "                              'stoich': 1,\n",
      "                              'enz_num': 3},\n",
      "                     'CC(C)(COP(=O)(O)OP(=O)(O)OC[C@@H]1[C@H]([C@H]([C@@H](O1)N2C=NC3=C(N=CN=C32)N)O)OP(=O)(O)O)[C@H](C(=O)NCCC(=O)NCCS)O': {'meta_num': 3,\n",
      "                                                                                                                                             'meta_name': 'CoA',\n",
      "                                                                                                                                             'k_cat': 8.164417036225661,\n",
      "                                                                                                                                             'K_m': 0.08566024557747152,\n",
      "                                                                                                                                             'stoich': 1,\n",
      "                                                                                                                                             'enz_num': 3},\n",
      "                     'C1=CC(=C[N+](=C1)[C@H]2[C@@H]([C@@H]([C@H](O2)COP(=O)([O-])OP(=O)(O)OC[C@@H]3[C@H]([C@H]([C@@H](O3)N4C=NC5=C(N=CN=C54)N)O)O)O)O)C(=O)N': {'meta_num': 7,\n",
      "                                                                                                                                                                'meta_name': 'NAD+',\n",
      "                                                                                                                                                                'k_cat': 7.085911624231392,\n",
      "                                                                                                                                                                'K_m': 0.46081919932984494,\n",
      "                                                                                                                                                                'stoich': 1,\n",
      "                                                                                                                                                                'enz_num': 3}},\n",
      "            'prods': {'CC(=O)SCCNC(=O)CCNC(=O)[C@@H](C(C)(C)COP(=O)(O)OP(=O)(O)OC[C@@H]1[C@H]([C@H]([C@@H](O1)N2C=NC3=C(N=CN=C32)N)O)OP(=O)(O)O)O': {'meta_num': 1,\n",
      "                                                                                                                                                     'meta_name': 'Acetyl-CoA',\n",
      "                                                                                                                                                     'k_cat': 8.013727221288514,\n",
      "                                                                                                                                                     'K_m': 0.10531545857093781,\n",
      "                                                                                                                                                     'stoich': 1,\n",
      "                                                                                                                                                     'enz_num': 3},\n",
      "                      'C1C=CN(C=C1C(=O)N)[C@H]2[C@@H]([C@@H]([C@H](O2)COP(=O)(O)OP(=O)(O)OC[C@@H]3[C@H]([C@H]([C@@H](O3)N4C=NC5=C(N=CN=C54)N)O)O)O)O': {'meta_num': 5,\n",
      "                                                                                                                                                        'meta_name': 'NADH',\n",
      "                                                                                                                                                        'k_cat': 11.398230023431234,\n",
      "                                                                                                                                                        'K_m': 0.12316464787568344,\n",
      "                                                                                                                                                        'stoich': 1,\n",
      "                                                                                                                                                        'enz_num': 3}},\n",
      "            'enz': {'enz_EC': '1.2.1.10', 'enz_num': 3}},\n",
      " 'R00703': {'subs': {'CC(=O)C(=O)[O-]': {'meta_num': 2,\n",
      "                                         'meta_name': 'Pyruvate',\n",
      "                                         'k_cat': 34.16428449182754,\n",
      "                                         'K_m': 1.5892967640480495,\n",
      "                                         'stoich': 1,\n",
      "                                         'enz_num': 4},\n",
      "                     'C1C=CN(C=C1C(=O)N)[C@H]2[C@@H]([C@@H]([C@H](O2)COP(=O)(O)OP(=O)(O)OC[C@@H]3[C@H]([C@H]([C@@H](O3)N4C=NC5=C(N=CN=C54)N)O)O)O)O': {'meta_num': 5,\n",
      "                                                                                                                                                       'meta_name': 'NADH',\n",
      "                                                                                                                                                       'k_cat': 1350.2027262437875,\n",
      "                                                                                                                                                       'K_m': 0.03617133426616177,\n",
      "                                                                                                                                                       'stoich': 1,\n",
      "                                                                                                                                                       'enz_num': 4}},\n",
      "            'prods': {'C[C@@H](C(=O)O)O': {'meta_num': 9,\n",
      "                                           'meta_name': '(S)-lactate',\n",
      "                                           'k_cat': 1433.6886729705807,\n",
      "                                           'K_m': 71.5960563192227,\n",
      "                                           'stoich': 1,\n",
      "                                           'enz_num': 4},\n",
      "                      'C1=CC(=C[N+](=C1)[C@H]2[C@@H]([C@@H]([C@H](O2)COP(=O)([O-])OP(=O)(O)OC[C@@H]3[C@H]([C@H]([C@@H](O3)N4C=NC5=C(N=CN=C54)N)O)O)O)O)C(=O)N': {'meta_num': 7,\n",
      "                                                                                                                                                                 'meta_name': 'NAD+',\n",
      "                                                                                                                                                                 'k_cat': 710.8523616339278,\n",
      "                                                                                                                                                                 'K_m': 0.22553799351110207,\n",
      "                                                                                                                                                                 'stoich': 1,\n",
      "                                                                                                                                                                 'enz_num': 4}},\n",
      "            'enz': {'enz_EC': '1.1.1.27', 'enz_num': 4}},\n",
      " 'meta_ct': 10,\n",
      " 'enz_ct': 5}\n",
      "['C(=O)[O-]',\n",
      " 'CC(=O)SCCNC(=O)CCNC(=O)[C@@H](C(C)(C)COP(=O)(O)OP(=O)(O)OC[C@@H]1[C@H]([C@H]([C@@H](O1)N2C=NC3=C(N=CN=C32)N)O)OP(=O)(O)O)O',\n",
      " 'CC(=O)C(=O)[O-]',\n",
      " 'CC(C)(COP(=O)(O)OP(=O)(O)OC[C@@H]1[C@H]([C@H]([C@@H](O1)N2C=NC3=C(N=CN=C32)N)O)OP(=O)(O)O)[C@H](C(=O)NCCC(=O)NCCS)O',\n",
      " 'CC(=O)[O-]',\n",
      " 'C1C=CN(C=C1C(=O)N)[C@H]2[C@@H]([C@@H]([C@H](O2)COP(=O)(O)OP(=O)(O)OC[C@@H]3[C@H]([C@H]([C@@H](O3)N4C=NC5=C(N=CN=C54)N)O)O)O)O',\n",
      " 'CC=O',\n",
      " 'C1=CC(=C[N+](=C1)[C@H]2[C@@H]([C@@H]([C@H](O2)COP(=O)([O-])OP(=O)(O)OC[C@@H]3[C@H]([C@H]([C@@H](O3)N4C=NC5=C(N=CN=C54)N)O)O)O)O)C(=O)N',\n",
      " 'CCO',\n",
      " 'C[C@@H](C(=O)O)O']\n",
      "test ['', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "key 0 {'C(=O)[O-]': {'meta_num': 0, 'meta_name': 'Formate', 'k_cat': 10.055570987342165, 'K_m': 13.844884834853536, 'stoich': 1, 'enz_num': 0}, 'CC(=O)SCCNC(=O)CCNC(=O)[C@@H](C(C)(C)COP(=O)(O)OP(=O)(O)OC[C@@H]1[C@H]([C@H]([C@@H](O1)N2C=NC3=C(N=CN=C32)N)O)OP(=O)(O)O)O': {'meta_num': 1, 'meta_name': 'Acetyl-CoA', 'k_cat': 8.002823257765913, 'K_m': 0.10558599605791594, 'stoich': 1, 'enz_num': 0}} {'CC(=O)C(=O)[O-]': {'meta_num': 2, 'meta_name': 'Pyruvate', 'k_cat': 0.699861431034104, 'K_m': 3.7030182513562626, 'stoich': 1, 'enz_num': 0}, 'CC(C)(COP(=O)(O)OP(=O)(O)OC[C@@H]1[C@H]([C@H]([C@@H](O1)N2C=NC3=C(N=CN=C32)N)O)OP(=O)(O)O)[C@H](C(=O)NCCC(=O)NCCS)O': {'meta_num': 3, 'meta_name': 'CoA', 'k_cat': 8.146511911230082, 'K_m': 0.08583361139178317, 'stoich': 1, 'enz_num': 0}}\n",
      "{'enz_EC': '2.3.1.54', 'enz_num': 0}\n",
      "key 1 {'CC(=O)[O-]': {'meta_num': 4, 'meta_name': 'Acetate', 'k_cat': 91.13363242250604, 'K_m': 1.4673414432132363, 'stoich': 1, 'enz_num': 1}, 'C1C=CN(C=C1C(=O)N)[C@H]2[C@@H]([C@@H]([C@H](O2)COP(=O)(O)OP(=O)(O)OC[C@@H]3[C@H]([C@H]([C@@H](O3)N4C=NC5=C(N=CN=C54)N)O)O)O)O': {'meta_num': 5, 'meta_name': 'NADH', 'k_cat': 22.386048857294554, 'K_m': 0.08149449754277381, 'stoich': 1, 'enz_num': 1}} {'CC=O': {'meta_num': 6, 'meta_name': 'Acetaldehyde', 'k_cat': 27.851824424527813, 'K_m': 3.6298932782879043, 'stoich': 1, 'enz_num': 1}, 'C1=CC(=C[N+](=C1)[C@H]2[C@@H]([C@@H]([C@H](O2)COP(=O)([O-])OP(=O)(O)OC[C@@H]3[C@H]([C@H]([C@@H](O3)N4C=NC5=C(N=CN=C54)N)O)O)O)O)C(=O)N': {'meta_num': 7, 'meta_name': 'NAD+', 'k_cat': 20.34650868513414, 'K_m': 0.35080092962267617, 'stoich': 1, 'enz_num': 1}}\n",
      "{'enz_EC': '1.2.1.3', 'enz_num': 1}\n",
      "key 2 {'CCO': {'meta_num': 8, 'meta_name': 'Ethanol', 'k_cat': 231.63150746705503, 'K_m': 3.6951917513171404, 'stoich': 1, 'enz_num': 2}, 'C1=CC(=C[N+](=C1)[C@H]2[C@@H]([C@@H]([C@H](O2)COP(=O)([O-])OP(=O)(O)OC[C@@H]3[C@H]([C@H]([C@@H](O3)N4C=NC5=C(N=CN=C54)N)O)O)O)O)C(=O)N': {'meta_num': 7, 'meta_name': 'NAD+', 'k_cat': 215.98127199500323, 'K_m': 0.28327345172869894, 'stoich': 1, 'enz_num': 2}} {'CC=O': {'meta_num': 6, 'meta_name': 'Acetaldehyde', 'k_cat': 322.1749664211754, 'K_m': 1.6453886389787897, 'stoich': 1, 'enz_num': 2}, 'C1C=CN(C=C1C(=O)N)[C@H]2[C@@H]([C@@H]([C@H](O2)COP(=O)(O)OP(=O)(O)OC[C@@H]3[C@H]([C@H]([C@@H](O3)N4C=NC5=C(N=CN=C54)N)O)O)O)O': {'meta_num': 5, 'meta_name': 'NADH', 'k_cat': 321.4767660701479, 'K_m': 0.0882936817212929, 'stoich': 1, 'enz_num': 2}}\n",
      "{'enz_EC': '1.1.1.71', 'enz_num': 2}\n",
      "key 3 {'CC=O': {'meta_num': 6, 'meta_name': 'Acetaldehyde', 'k_cat': 8.991911966615412, 'K_m': 11.332360232182543, 'stoich': 1, 'enz_num': 3}, 'CC(C)(COP(=O)(O)OP(=O)(O)OC[C@@H]1[C@H]([C@H]([C@@H](O1)N2C=NC3=C(N=CN=C32)N)O)OP(=O)(O)O)[C@H](C(=O)NCCC(=O)NCCS)O': {'meta_num': 3, 'meta_name': 'CoA', 'k_cat': 8.164417036225661, 'K_m': 0.08566024557747152, 'stoich': 1, 'enz_num': 3}, 'C1=CC(=C[N+](=C1)[C@H]2[C@@H]([C@@H]([C@H](O2)COP(=O)([O-])OP(=O)(O)OC[C@@H]3[C@H]([C@H]([C@@H](O3)N4C=NC5=C(N=CN=C54)N)O)O)O)O)C(=O)N': {'meta_num': 7, 'meta_name': 'NAD+', 'k_cat': 7.085911624231392, 'K_m': 0.46081919932984494, 'stoich': 1, 'enz_num': 3}} {'CC(=O)SCCNC(=O)CCNC(=O)[C@@H](C(C)(C)COP(=O)(O)OP(=O)(O)OC[C@@H]1[C@H]([C@H]([C@@H](O1)N2C=NC3=C(N=CN=C32)N)O)OP(=O)(O)O)O': {'meta_num': 1, 'meta_name': 'Acetyl-CoA', 'k_cat': 8.013727221288514, 'K_m': 0.10531545857093781, 'stoich': 1, 'enz_num': 3}, 'C1C=CN(C=C1C(=O)N)[C@H]2[C@@H]([C@@H]([C@H](O2)COP(=O)(O)OP(=O)(O)OC[C@@H]3[C@H]([C@H]([C@@H](O3)N4C=NC5=C(N=CN=C54)N)O)O)O)O': {'meta_num': 5, 'meta_name': 'NADH', 'k_cat': 11.398230023431234, 'K_m': 0.12316464787568344, 'stoich': 1, 'enz_num': 3}}\n",
      "{'enz_EC': '1.2.1.10', 'enz_num': 3}\n",
      "key 4 {'CC(=O)C(=O)[O-]': {'meta_num': 2, 'meta_name': 'Pyruvate', 'k_cat': 34.16428449182754, 'K_m': 1.5892967640480495, 'stoich': 1, 'enz_num': 4}, 'C1C=CN(C=C1C(=O)N)[C@H]2[C@@H]([C@@H]([C@H](O2)COP(=O)(O)OP(=O)(O)OC[C@@H]3[C@H]([C@H]([C@@H](O3)N4C=NC5=C(N=CN=C54)N)O)O)O)O': {'meta_num': 5, 'meta_name': 'NADH', 'k_cat': 1350.2027262437875, 'K_m': 0.03617133426616177, 'stoich': 1, 'enz_num': 4}} {'C[C@@H](C(=O)O)O': {'meta_num': 9, 'meta_name': '(S)-lactate', 'k_cat': 1433.6886729705807, 'K_m': 71.5960563192227, 'stoich': 1, 'enz_num': 4}, 'C1=CC(=C[N+](=C1)[C@H]2[C@@H]([C@@H]([C@H](O2)COP(=O)([O-])OP(=O)(O)OC[C@@H]3[C@H]([C@H]([C@@H](O3)N4C=NC5=C(N=CN=C54)N)O)O)O)O)C(=O)N': {'meta_num': 7, 'meta_name': 'NAD+', 'k_cat': 710.8523616339278, 'K_m': 0.22553799351110207, 'stoich': 1, 'enz_num': 4}}\n",
      "{'enz_EC': '1.1.1.27', 'enz_num': 4}\n",
      "['1*y[10]*10.055570987342165*(y[0]/(13.844884834853536+y[0]))*(y[1]/(0.10558599605791594+y[1]))',\n",
      " '1*y[13]*8.013727221288514*(y[1]/(0.10531545857093781+y[1]))*(y[5]/(0.12316464787568344+y[5]))',\n",
      " '1*y[14]*34.16428449182754*(y[2]/(1.5892967640480495+y[2]))*(y[5]/(0.03617133426616177+y[5]))',\n",
      " '1*y[13]*8.164417036225661*(y[6]/(11.332360232182543+y[6]))*(y[3]/(0.08566024557747152+y[3]))*(y[7]/(0.46081919932984494+y[7]))',\n",
      " '1*y[11]*91.13363242250604*(y[4]/(1.4673414432132363+y[4]))*(y[5]/(0.08149449754277381+y[5]))',\n",
      " '1*y[14]*1350.2027262437875*(y[2]/(1.5892967640480495+y[2]))*(y[5]/(0.03617133426616177+y[5]))',\n",
      " '1*y[13]*8.991911966615412*(y[6]/(11.332360232182543+y[6]))*(y[3]/(0.08566024557747152+y[3]))*(y[7]/(0.46081919932984494+y[7]))',\n",
      " '1*y[14]*710.8523616339278*(y[9]/(71.5960563192227+y[9]))*(y[7]/(0.22553799351110207+y[7]))',\n",
      " '1*y[12]*231.63150746705503*(y[8]/(3.6951917513171404+y[8]))*(y[7]/(0.28327345172869894+y[7]))',\n",
      " '1*y[14]*1433.6886729705807*(y[9]/(71.5960563192227+y[9]))*(y[7]/(0.22553799351110207+y[7]))',\n",
      " 0,\n",
      " 0,\n",
      " 0,\n",
      " 0,\n",
      " 0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.generate_ode_func.<locals>.func.<locals>.<lambda>(t, y)>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scipy\n",
    "import numpy as np\n",
    "import pprint\n",
    "\n",
    "print('kin',1)\n",
    "# model currently doesn't consider change of enzyme concentrations\n",
    "def generate_ode_func(kin_param_bundle):\n",
    "    meta_ct = kin_param_bundle['meta_ct']\n",
    "    def func(t,y):\n",
    "        dydt_str = ['' for x in range(kin_param_bundle['meta_ct']+kin_param_bundle['enz_ct'])]\n",
    "        print('test',dydt_str)\n",
    "        for idx,key in enumerate([x for x in list(kin_param_bundle.keys()) if 'R' in x]):\n",
    "            subs = kin_param_bundle[key]['subs']\n",
    "            prods = kin_param_bundle[key]['prods']\n",
    "            enz = kin_param_bundle[key]['enz']\n",
    "\n",
    "            subs_monod = ''\n",
    "            print(f'key {idx}',subs,prods)\n",
    "            for idx_2,key_2 in enumerate(list(subs.keys())):\n",
    "                sub = subs[key_2]\n",
    "                meta_num = sub['meta_num']\n",
    "                K_m = sub['K_m']\n",
    "                subs_monod += f'*(y[{meta_num}]/({K_m}+y[{meta_num}]))'\n",
    "            for idx_2,key_2 in enumerate(list(subs.keys())):\n",
    "                sub = subs[key_2]\n",
    "                meta_num = sub['meta_num']\n",
    "                stoich = sub['stoich']\n",
    "                enz_num = sub['enz_num']\n",
    "                k_cat = sub['k_cat']\n",
    "                dydt_str[meta_num] = f'{stoich}*y[{enz_num+meta_ct}]*{k_cat}{subs_monod}'\n",
    "            prods_monod = ''\n",
    "            for idx_2,key_2 in enumerate(list(prods.keys())):\n",
    "                prod = prods[key_2]\n",
    "                meta_num = prod['meta_num']\n",
    "                K_m = prod['K_m']\n",
    "                prods_monod += f'*(y[{meta_num}]/({K_m}+y[{meta_num}]))'\n",
    "            for idx_2,key_2 in enumerate(list(prods.keys())):\n",
    "                prod = prods[key_2]\n",
    "                meta_num = prod['meta_num']\n",
    "                stoich = prod['stoich']\n",
    "                enz_num = prod['enz_num']\n",
    "                k_cat = prod['k_cat']\n",
    "                dydt_str[meta_num] = f'{stoich}*y[{enz_num+meta_ct}]*{k_cat}{prods_monod}'\n",
    "            # for now, defining enzyme concentration rate of change as 0\n",
    "            print(enz)\n",
    "            dydt_str[enz['enz_num']+meta_ct] = 0\n",
    "\n",
    "        pprint.pp(dydt_str)\n",
    "        dydt = lambda t,y: eval(dydt_str)\n",
    "        return dydt\n",
    "    return func\n",
    "\n",
    "func = generate_ode_func(kin_param_bundle)\n",
    "\n",
    "meta_dict = {}\n",
    "meta_ct = kin_param_bundle['meta_ct']\n",
    "for idx,key in enumerate([x for x in list(kin_param_bundle.keys()) if 'R' in x]):\n",
    "    subs = kin_param_bundle[key]['subs']\n",
    "    prods = kin_param_bundle[key]['prods']\n",
    "    enz = kin_param_bundle[key]['enz']\n",
    "\n",
    "    for idx_2,key_2 in enumerate(list(subs.keys())):\n",
    "        sub = subs[key_2]\n",
    "        meta_num = sub['meta_num']\n",
    "        meta_name = sub['meta_name']\n",
    "        meta_dict[meta_name] = meta_num\n",
    "    for idx_2,key_2 in enumerate(list(prods.keys())):\n",
    "        prod = prods[key_2]\n",
    "        meta_num = prod['meta_num']\n",
    "        meta_name = prod['meta_name']\n",
    "        meta_dict[meta_name] = meta_num\n",
    "    meta_dict[enz['enz_EC']] = enz['enz_num']+meta_ct\n",
    "\n",
    "pprint.pp(meta_dict)\n",
    "print('kin')\n",
    "pprint.pp(kin_param_bundle)\n",
    "\n",
    "pprint.pp(meta_smiles)\n",
    "\n",
    "func(1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running Kinetic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_span = (0,60*60*24) # s\n",
    "\n",
    "# 1 - \n",
    "y0_meta = [] # mM\n",
    "y0_enz = [] # mM\n",
    "y0 = y0_meta + y0_enz\n",
    "\n",
    "ode_res = scipy.solve_ivp(\n",
    "    fun=func,\n",
    "    t_span=t_span,\n",
    "    y0=y0,\n",
    "    method='RK45',\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting Kinetic Model Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = ['abc','anadsfdasg']\n",
    "\n",
    "test2 = 'ac'\n",
    "\n",
    "print(test.index(test2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.classes.WholeCellConsortiumModel import WholeCellConsortiumModel\n",
    "\n",
    "wccm = WholeCellConsortiumModel()\n",
    "wccm.generate_whole_network('test',invalid_enz=[],invalid_rxn=[])\n",
    "wccm.set_reaction_mass_balance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.classes.WholeCellConsortiumModel import WholeCellConsortiumModel\n",
    "\n",
    "wccm = WholeCellConsortiumModel({})\n",
    "wccm.generate_whole_network('test',invalid_enz=[],invalid_rxn=[])\n",
    "wccm.match_KEGG_compounds_to_BRENDA()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.classes.WholeCellConsortiumModel import WholeCellConsortiumModel\n",
    "import os\n",
    "\n",
    "os.system('which python')\n",
    "\n",
    "wccm = WholeCellConsortiumModel({})\n",
    "wccm.generate_whole_network('test',invalid_enz=[],invalid_rxn=[])\n",
    "wccm.set_reaction_reversibility()\n",
    "\n",
    "### STARTHERE: how to deal with generic reactions and generic compounds?\n",
    "# how to deal with protonated/non-protonated versions?\n",
    "# KEGG only lists reactions with neutral molecules; BRENDA may list them with non-neutrals, how to handle this? - based on molecular formula:\n",
    "#   if there is a difference of just one hydrogen between each of the potential acid/base states, then consolidate them into a single entity.\n",
    "# need to go through each reaction, find reaction reversibility for each of the enzymes that are dashed\n",
    "# need to incorporate thermodynamic information to also account for reversibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "data = ['C00020','C00044','C00144','C00125','C00126','C00138','C00139','C01326']\n",
    "checkboxes = [widgets.Checkbox(value=False, description=label) for label in data]\n",
    "output = widgets.VBox(children=checkboxes)\n",
    "display(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.classes.WholeCellConsortiumModel import WholeCellConsortiumModel\n",
    "\n",
    "valid_cofactors = {}\n",
    "for idx,x in enumerate(data):\n",
    "    valid_cofactors[x] = checkboxes[idx].value\n",
    "\n",
    "print(valid_cofactors)\n",
    "\n",
    "invalid_enz = ['4.1.2.36']\n",
    "invalid_rxn = ['R00326','R00710','R00711','R01019']\n",
    "\n",
    "wccm = WholeCellConsortiumModel(valid_cofactors=valid_cofactors)\n",
    "wccm.generate_whole_network('test',invalid_enz=invalid_enz,invalid_rxn=invalid_rxn)\n",
    "test_graph_2 = wccm.seek_optimal_network('test',['C00186'],['C00033','C00058','C00469'],2,2)\n",
    "# test_graph_2 = wccm.seek_optimal_network('test',['C00186'],['C00058','C00084'],2,2)\n",
    "wccm.visualize_graph('test')\n",
    "\n",
    "# IMPROVEMENTS:\n",
    "# how can we ensure that we use EtOH+acetate mixture? - calculate overall redox, set EtOH/acetate input ratio before optimization\n",
    "\n",
    "# set EtOH/acetate input ratio based on overall mass/redox balance\n",
    "# restrict valid cofactors\n",
    "# restrict small gas input into reactions\n",
    "# how to get reactions from BRENDA as well?\n",
    "# how to decide on best overall mass/redox balance when there are multiple options? Maybe run an optimization for all of them? - constrain by substrate ratios?\n",
    "# how to determine optimal overall redox balance via optimization with COBRA? Effectively determining best substrate ratios for a given product"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wccm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
